{
  
    
        "post0": {
            "title": "Real World Project for AllCorrect Games",
            "content": "Project Description . The company Allcorrect Games provides video game localization, localization testing, voiceovers, game art, and playable ads. . Allcorrect Group works with major international game publishers and developers. The company has participated in the localization of over 968 games, as well as the testing of over 100 games. Allcorrect Group works in more than 23 language pairs, partners with 6 voice studios, and employs highly competent native speakers to test games on the iOS, Android, and PC platforms. . The company uses players&#39; reviews from different sources (Google PlayMarket, AppStore, etc.) to find games with high localization demand. This information is used to generate business offers from the company to game developers. . During reviews&#39; analysis each of them is manually classified into several different categories. For instance, a review contains localization request or a user complains about translation mistakes, etc. Manual labeling of reviews is tedious and expensive process. . Main assumption of this project is that labeling could be done automatically by machine learning algorithms. . Task . Build an algorithm for classification of user reviews into one of the four categories. The quality of the algorithm should be evaluated using hold-out subset or cross-validation technique. . Data Description . The dataset has a record of 58000 entries. . Each entry has: . id : Unique ID for record entry | mark : Corresponding label/tag category for text review entry | review : Text review | . The following are the unique mark category: . RL : localization request | L+ : good/positive localization | L- : bad/negative localization | YL : localization exists | . Metrics . The following metrics will be used to evaluate the model based on the prediction for the four unique categories: . Accuracy score | Macro F1 score | . Using a macro F1 score, a macro-average will calculate the F1 score independently for each category/class and then take the average (treating all classes equally). . Import Libraries . Create environment . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import re import time from collections import defaultdict from wordcloud import WordCloud import random import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords as nltk_stopwords nltk_data_path = os.path.join(os.path.dirname(nltk.__file__), &#39;nltk_data&#39;) nltk.data.path.append(nltk_data_path) nltk.download(&#39;wordnet&#39;) nltk.download(&#39;averaged_perceptron_tagger&#39;) nltk.download(&#39;punkt&#39;) from nltk.corpus import wordnet from nltk.stem import WordNetLemmatizer from nltk import pos_tag, word_tokenize from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.pipeline import FeatureUnion from sklearn.model_selection import train_test_split, cross_val_score, KFold from sklearn.preprocessing import LabelEncoder from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import LinearSVC from sklearn import decomposition, ensemble from sklearn.metrics import make_scorer, accuracy_score, f1_score, classification_report !pip install catboost !pip install ipywidgets !jupyter nbextension enable --py widgetsnbextension import catboost from catboost import CatBoostClassifier, Pool, metrics, cv from catboost.text_processing import Tokenizer, Dictionary import tensorflow as tf from tensorflow import keras from tensorflow.keras.layers import Embedding import tensorflow_hub as hub from keras.preprocessing import text, sequence from keras import layers, models, optimizers from tensorflow.keras.models import save_model, load_model !pip install bert-for-tf2 from bert import bert_tokenization import warnings warnings.filterwarnings(&quot;ignore&quot;) RANDOM_STATE = 42 . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Package averaged_perceptron_tagger is already up-to- [nltk_data] date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! Collecting catboost Downloading https://files.pythonhosted.org/packages/5a/41/24e14322b9986cf72a8763e0a0a69cc256cf963cf9502c8f0044a62c1ae8/catboost-0.26-cp37-none-manylinux1_x86_64.whl (69.2MB) |████████████████████████████████| 69.2MB 50kB/s Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-0.26 Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0) Requirement already satisfied: traitlets&gt;=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1) Requirement already satisfied: ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34; in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (5.3.5) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets&gt;=4.3.1-&gt;ipywidgets) (0.2.0) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets) (5.3.1) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (2.6.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (0.7.5) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (57.0.0) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (1.0.18) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (0.8.1) Requirement already satisfied: pexpect; sys_platform != &#34;win32&#34; in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (4.8.0) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (4.4.2) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (2.6.0) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets) (4.7.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (2.8.1) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets) (22.1.0) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.10.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (5.6.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.5.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.11.3) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (0.2.5) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (1.15.0) Requirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets) (0.7.0) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (1.4.3) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.8.4) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (3.3.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.0.1) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (0.5.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (20.9) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets) (2.4.7) Enabling notebook extension jupyter-js-widgets/extension... - Validating: OK Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9) Requirement already satisfied: py-params&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2) Requirement already satisfied: params-flow&gt;=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow&gt;=0.8.0-&gt;bert-for-tf2) (4.41.1) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow&gt;=0.8.0-&gt;bert-for-tf2) (1.19.5) . . Exploratory data analysis . Load Data . from google.colab import drive, files drive.mount(&#39;/content/gdrive&#39;) !ls &#39;/content/gdrive/My Drive/AllcorrectGames/&#39; df_reviews = pd.read_excel(&#39;/content/gdrive/My Drive/AllcorrectGames/reviews.xlsx&#39;) . Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&#34;/content/gdrive&#34;, force_remount=True). custom_w2v_100d.txt keras_bert_model reviews.xlsx sample_file.xlsx . def get_information(df): print(&#39;Head:&#39;) print() display(df.head()) print (&#39;-&#39;*100) print(&#39;Info:&#39;) print() display(df.info()) print (&#39;-&#39;*100) print(&#39;Describe:&#39;) print() display(df.describe()) print (&#39;-&#39;*100) print() print(&#39;Columns with nulls:&#39;) display(get_precent_of_na_df(df,4)) print (&#39;-&#39;*100) print(&#39;Shape:&#39;) print(df.shape) print (&#39;-&#39;*100) print(&#39;Duplicated:&#39;) print(&#39; 033[1m&#39; + &#39;We have {} duplicated rows.&#39;.format(df.duplicated().sum()) + &#39; 033[0m&#39;) def get_precent_of_na_df(df,num): df_nulls = pd.DataFrame(df.isna().sum(),columns=[&#39;Missing Values&#39;]) df_nulls[&#39;Percent of Nulls&#39;] = round(df_nulls[&#39;Missing Values&#39;] / df.shape[0],num) *100 return df_nulls . get_information(df_reviews) . Head: . id mark review . 0 6720 | RL | It&#39;s not Turkish, it&#39;s a lie, but I recommend ... | . 1 43313 | RL | You don&#39;t have Korean | . 2 26549 | RL | Very nice, only if it were in Italian it would... | . 3 42306 | RL | The game is nice but when it comes to Turkish ... | . 4 32331 | RL | Amazing work, hope to achieve 100% Chinese tra... | . - Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51800 entries, 0 to 51799 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 id 51800 non-null int64 1 mark 51800 non-null object 2 review 51800 non-null object dtypes: int64(1), object(2) memory usage: 1.2+ MB . None . - Describe: . id . count 51800.000000 | . mean 25918.529459 | . std 14964.874328 | . min 1.000000 | . 25% 12959.750000 | . 50% 25916.500000 | . 75% 38879.250000 | . max 51840.000000 | . - Columns with nulls: . Missing Values Percent of Nulls . id 0 | 0.0 | . mark 0 | 0.0 | . review 0 | 0.0 | . - Shape: (51800, 3) - Duplicated: We have 0 duplicated rows. . . There are 51800 entries with no null values. mark column has the tags for the targeted categories and none are missing for each review. . Preprocessing Data . Target Label Column . display(df_reviews[&#39;mark&#39;].unique()) . array([&#39;RL&#39;, &#39;YL&#39;, &#39;L+&#39;, &#39;L-&#39;, &#39;Rl&#39;, &#39;l-&#39;, &#39;Yl&#39;, &#39;yl&#39;], dtype=object) . Need to clean up the target mark columns to have 4 categories: . RL | YL | L+ | L- | . df_reviews[&#39;mark&#39;] = df_reviews[&#39;mark&#39;].str.upper() display(df_reviews[&#39;mark&#39;].unique()) . array([&#39;RL&#39;, &#39;YL&#39;, &#39;L+&#39;, &#39;L-&#39;], dtype=object) . Normalization of Feature text . def clear_text(text): clean_text = re.sub(r&#39;[^a-zA-z &#39;]&#39;,&#39; &#39;,text) clean_text = &#39; &#39;.join(clean_text.split()) clean_text = clean_text.lower() return(clean_text) . CONTRACTION_MAP = { &quot;ain&#39;t&quot;: &quot;is not&quot;, &quot;aren&#39;t&quot;: &quot;are not&quot;, &quot;can&#39;t&quot;: &quot;cannot&quot;, &quot;can&#39;t&#39;ve&quot;: &quot;cannot have&quot;, &quot;&#39;cause&quot;: &quot;because&quot;, &quot;could&#39;ve&quot;: &quot;could have&quot;, &quot;couldn&#39;t&quot;: &quot;could not&quot;, &quot;couldn&#39;t&#39;ve&quot;: &quot;could not have&quot;, &quot;didn&#39;t&quot;: &quot;did not&quot;, &quot;doesn&#39;t&quot;: &quot;does not&quot;, &quot;don&#39;t&quot;: &quot;do not&quot;, &quot;hadn&#39;t&quot;: &quot;had not&quot;, &quot;hadn&#39;t&#39;ve&quot;: &quot;had not have&quot;, &quot;hasn&#39;t&quot;: &quot;has not&quot;, &quot;haven&#39;t&quot;: &quot;have not&quot;, &quot;he&#39;d&quot;: &quot;he would&quot;, &quot;he&#39;d&#39;ve&quot;: &quot;he would have&quot;, &quot;he&#39;ll&quot;: &quot;he will&quot;, &quot;he&#39;ll&#39;ve&quot;: &quot;he he will have&quot;, &quot;he&#39;s&quot;: &quot;he is&quot;, &quot;how&#39;d&quot;: &quot;how did&quot;, &quot;how&#39;d&#39;y&quot;: &quot;how do you&quot;, &quot;how&#39;ll&quot;: &quot;how will&quot;, &quot;how&#39;s&quot;: &quot;how is&quot;, &quot;I&#39;d&quot;: &quot;I would&quot;, &quot;I&#39;d&#39;ve&quot;: &quot;I would have&quot;, &quot;I&#39;ll&quot;: &quot;I will&quot;, &quot;I&#39;ll&#39;ve&quot;: &quot;I will have&quot;, &quot;I&#39;m&quot;: &quot;I am&quot;, &quot;I&#39;ve&quot;: &quot;I have&quot;, &quot;i&#39;d&quot;: &quot;i would&quot;, &quot;i&#39;d&#39;ve&quot;: &quot;i would have&quot;, &quot;i&#39;ll&quot;: &quot;i will&quot;, &quot;i&#39;ll&#39;ve&quot;: &quot;i will have&quot;, &quot;i&#39;m&quot;: &quot;i am&quot;, &quot;i&#39;ve&quot;: &quot;i have&quot;, &quot;isn&#39;t&quot;: &quot;is not&quot;, &quot;it&#39;d&quot;: &quot;it would&quot;, &quot;it&#39;d&#39;ve&quot;: &quot;it would have&quot;, &quot;it&#39;ll&quot;: &quot;it will&quot;, &quot;it&#39;ll&#39;ve&quot;: &quot;it will have&quot;, &quot;it&#39;s&quot;: &quot;it is&quot;, &quot;let&#39;s&quot;: &quot;let us&quot;, &quot;ma&#39;am&quot;: &quot;madam&quot;, &quot;mayn&#39;t&quot;: &quot;may not&quot;, &quot;might&#39;ve&quot;: &quot;might have&quot;, &quot;mightn&#39;t&quot;: &quot;might not&quot;, &quot;mightn&#39;t&#39;ve&quot;: &quot;might not have&quot;, &quot;must&#39;ve&quot;: &quot;must have&quot;, &quot;mustn&#39;t&quot;: &quot;must not&quot;, &quot;mustn&#39;t&#39;ve&quot;: &quot;must not have&quot;, &quot;needn&#39;t&quot;: &quot;need not&quot;, &quot;needn&#39;t&#39;ve&quot;: &quot;need not have&quot;, &quot;o&#39;clock&quot;: &quot;of the clock&quot;, &quot;oughtn&#39;t&quot;: &quot;ought not&quot;, &quot;oughtn&#39;t&#39;ve&quot;: &quot;ought not have&quot;, &quot;shan&#39;t&quot;: &quot;shall not&quot;, &quot;sha&#39;n&#39;t&quot;: &quot;shall not&quot;, &quot;shan&#39;t&#39;ve&quot;: &quot;shall not have&quot;, &quot;she&#39;d&quot;: &quot;she would&quot;, &quot;she&#39;d&#39;ve&quot;: &quot;she would have&quot;, &quot;she&#39;ll&quot;: &quot;she will&quot;, &quot;she&#39;ll&#39;ve&quot;: &quot;she will have&quot;, &quot;she&#39;s&quot;: &quot;she is&quot;, &quot;should&#39;ve&quot;: &quot;should have&quot;, &quot;shouldn&#39;t&quot;: &quot;should not&quot;, &quot;shouldn&#39;t&#39;ve&quot;: &quot;should not have&quot;, &quot;so&#39;ve&quot;: &quot;so have&quot;, &quot;so&#39;s&quot;: &quot;so as&quot;, &quot;that&#39;d&quot;: &quot;that would&quot;, &quot;that&#39;d&#39;ve&quot;: &quot;that would have&quot;, &quot;that&#39;s&quot;: &quot;that is&quot;, &quot;there&#39;d&quot;: &quot;there would&quot;, &quot;there&#39;d&#39;ve&quot;: &quot;there would have&quot;, &quot;there&#39;s&quot;: &quot;there is&quot;, &quot;they&#39;d&quot;: &quot;they would&quot;, &quot;they&#39;d&#39;ve&quot;: &quot;they would have&quot;, &quot;they&#39;ll&quot;: &quot;they will&quot;, &quot;they&#39;ll&#39;ve&quot;: &quot;they will have&quot;, &quot;they&#39;re&quot;: &quot;they are&quot;, &quot;they&#39;ve&quot;: &quot;they have&quot;, &quot;to&#39;ve&quot;: &quot;to have&quot;, &quot;wasn&#39;t&quot;: &quot;was not&quot;, &quot;we&#39;d&quot;: &quot;we would&quot;, &quot;we&#39;d&#39;ve&quot;: &quot;we would have&quot;, &quot;we&#39;ll&quot;: &quot;we will&quot;, &quot;we&#39;ll&#39;ve&quot;: &quot;we will have&quot;, &quot;we&#39;re&quot;: &quot;we are&quot;, &quot;we&#39;ve&quot;: &quot;we have&quot;, &quot;weren&#39;t&quot;: &quot;were not&quot;, &quot;what&#39;ll&quot;: &quot;what will&quot;, &quot;what&#39;ll&#39;ve&quot;: &quot;what will have&quot;, &quot;what&#39;re&quot;: &quot;what are&quot;, &quot;what&#39;s&quot;: &quot;what is&quot;, &quot;what&#39;ve&quot;: &quot;what have&quot;, &quot;when&#39;s&quot;: &quot;when is&quot;, &quot;when&#39;ve&quot;: &quot;when have&quot;, &quot;where&#39;d&quot;: &quot;where did&quot;, &quot;where&#39;s&quot;: &quot;where is&quot;, &quot;where&#39;ve&quot;: &quot;where have&quot;, &quot;who&#39;ll&quot;: &quot;who will&quot;, &quot;who&#39;ll&#39;ve&quot;: &quot;who will have&quot;, &quot;who&#39;s&quot;: &quot;who is&quot;, &quot;who&#39;ve&quot;: &quot;who have&quot;, &quot;why&#39;s&quot;: &quot;why is&quot;, &quot;why&#39;ve&quot;: &quot;why have&quot;, &quot;will&#39;ve&quot;: &quot;will have&quot;, &quot;won&#39;t&quot;: &quot;will not&quot;, &quot;won&#39;t&#39;ve&quot;: &quot;will not have&quot;, &quot;would&#39;ve&quot;: &quot;would have&quot;, &quot;wouldn&#39;t&quot;: &quot;would not&quot;, &quot;wouldn&#39;t&#39;ve&quot;: &quot;would not have&quot;, &quot;y&#39;all&quot;: &quot;you all&quot;, &quot;y&#39;all&#39;d&quot;: &quot;you all would&quot;, &quot;y&#39;all&#39;d&#39;ve&quot;: &quot;you all would have&quot;, &quot;y&#39;all&#39;re&quot;: &quot;you all are&quot;, &quot;y&#39;all&#39;ve&quot;: &quot;you all have&quot;, &quot;you&#39;d&quot;: &quot;you would&quot;, &quot;you&#39;d&#39;ve&quot;: &quot;you would have&quot;, &quot;you&#39;ll&quot;: &quot;you will&quot;, &quot;you&#39;ll&#39;ve&quot;: &quot;you will have&quot;, &quot;you&#39;re&quot;: &quot;you are&quot;, &quot;you&#39;ve&quot;: &quot;you have&quot; } . . def expand_contractions(text, contraction_mapping=CONTRACTION_MAP): contractions_pattern = re.compile(&#39;({})&#39;.format(&#39;|&#39;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower()) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_text = contractions_pattern.sub(expand_match, text) expanded_text = re.sub(&quot;&#39;&quot;, &quot;&quot;, expanded_text) return expanded_text . df = df_reviews.copy() df[&#39;review_norm&#39;] = df[&#39;review&#39;].apply(clear_text) df[&#39;review_norm&#39;] = df[&#39;review_norm&#39;].apply(expand_contractions) . df[[&#39;review&#39;,&#39;review_norm&#39;]].head() . review review_norm . 0 It&#39;s not Turkish, it&#39;s a lie, but I recommend ... | it is not turkish it is a lie but i recommend ... | . 1 You don&#39;t have Korean | you do not have korean | . 2 Very nice, only if it were in Italian it would... | very nice only if it were in italian it would ... | . 3 The game is nice but when it comes to Turkish ... | the game is nice but when it comes to turkish ... | . 4 Amazing work, hope to achieve 100% Chinese tra... | amazing work hope to achieve chinese translation | . Tokenize with Catboost Tokenizer . # Removing punctuation and lowercases within Catboost Tokenizer # (Feature normalize text already has this completed but Catboost has option in Tokenizer) tokenizer = Tokenizer( lowercasing=True, separator_type=&#39;BySense&#39;, token_types=[&#39;Word&#39;, &#39;Number&#39;] ) df[&#39;tokenized_text&#39;] = [tokenizer.tokenize(text) for text in df[&#39;review_norm&#39;]] df[[&#39;review_norm&#39;, &#39;tokenized_text&#39;]].head() . review_norm tokenized_text . 0 it is not turkish it is a lie but i recommend ... | [it, is, not, turkish, it, is, a, lie, but, i,... | . 1 you do not have korean | [you, do, not, have, korean] | . 2 very nice only if it were in italian it would ... | [very, nice, only, if, it, were, in, italian, ... | . 3 the game is nice but when it comes to turkish ... | [the, game, is, nice, but, when, it, comes, to... | . 4 amazing work hope to achieve chinese translation | [amazing, work, hope, to, achieve, chinese, tr... | . Lemmatization . # Maps POS tag for any word types and returns noun def get_wordnet_pos(word): tag = nltk.pos_tag([word])[0][1][0].upper() tag_dict = {&quot;J&quot;: wordnet.ADJ, &quot;N&quot;: wordnet.NOUN, &quot;V&quot;: wordnet.VERB, &quot;R&quot;: wordnet.ADV} return tag_dict.get(tag, wordnet.NOUN) lemmatizer = WordNetLemmatizer() # Lemmatize text to noun based def lemmatize_tokens_nltk(tokens): lemmatized_text = [] for word in tokens: lemmatized_text.append(str(lemmatizer.lemmatize(word, get_wordnet_pos(word)))) return &#39; &#39;.join(lemmatized_text) df[&#39;text_lemma_nltk&#39;] = df[&#39;tokenized_text&#39;].apply(lambda x: lemmatize_tokens_nltk(x)) . df[[&#39;review_norm&#39;, &#39;text_lemma_nltk&#39;]].head() . review_norm text_lemma_nltk . 0 it is not turkish it is a lie but i recommend ... | it be not turkish it be a lie but i recommend ... | . 1 you do not have korean | you do not have korean | . 2 very nice only if it were in italian it would ... | very nice only if it be in italian it would be... | . 3 the game is nice but when it comes to turkish ... | the game be nice but when it come to turkish l... | . 4 amazing work hope to achieve chinese translation | amaze work hope to achieve chinese translation | . Visualize . Label/Mark distribution . def get_annotation(ax): &quot;&quot;&quot;Get annotation for graphs on top of bar graphs&quot;&quot;&quot; for p in ax.patches: ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.05)) . get_annotation(df[&#39;mark&#39;].value_counts().plot(kind =&#39;bar&#39;, title = &#39;Bar Graph to show count of each mark&#39;, align = &#39;center&#39;, figsize=(10,5))) plt.ylabel(&#39;Count&#39;) plt.xlabel(&#39;mark&#39;) plt.grid(True) plt.show() . Clearly, just as what was seen with the means, there is a larger count of RL maker in comparison to the others. RL has over 40,000 counts. L- has about 5,400 counts, YL has about 2,900 counts, and L+ has about 600 counts. This shows a class imbalance for sure with RL having the highest majority. . Length of Reviews . df[&#39;review_length&#39;] = df[&#39;review_norm&#39;].apply(lambda x: len(str(x).split())) df.describe() . id review_length . count 51800.000000 | 51800.000000 | . mean 25918.529459 | 33.011680 | . std 14964.874328 | 85.466875 | . min 1.000000 | 0.000000 | . 25% 12959.750000 | 8.000000 | . 50% 25916.500000 | 15.000000 | . 75% 38879.250000 | 30.000000 | . max 51840.000000 | 3308.000000 | . There are reviews with word length of 0. . df.loc[df[&#39;review_length&#39;] &lt;= 0] . id mark review review_norm tokenized_text text_lemma_nltk review_length . 968 6284 | RL | ?? | | [] | | 0 | . 2778 45703 | RL | . | | [] | | 0 | . 6538 1037 | RL | ... | | [] | | 0 | . 8779 47782 | YL | 👍👍👍 | | [] | | 0 | . 13292 12229 | RL | 😕😕😕 | | [] | | 0 | . 14029 47768 | RL | .. | | [] | | 0 | . 23782 43514 | RL | 🙏🏽 | | [] | | 0 | . 26259 27730 | RL | 🇩🇰✌️ | | [] | | 0 | . 34109 31822 | RL | ， | | [] | | 0 | . 37024 33813 | RL | По мотивам. Вы должны добавить польский, если ... | | [] | | 0 | . 38710 24083 | RL | 🙏🏻🙏🏻🙏🏻 | | [] | | 0 | . 40152 31653 | RL | 👿 | | [] | | 0 | . 45267 7454 | RL | 👍 | | [] | | 0 | . 48906 24112 | RL | 👍 | | [] | | 0 | . It would be ideal to drop these reviews that have 0 word length. . df = df.loc[(df.review_length != 0)] df.loc[df[&#39;review_length&#39;] &lt;= 0] . id mark review review_norm tokenized_text text_lemma_nltk review_length . plt.figure(figsize=(12,6)) plt.xlim(right=175) plt.grid(True) p1=sns.kdeplot(df[&#39;review_length&#39;], shade=True, color=&#39;b&#39;).set_title(&#39;Kernel Distribution of Number Of Words in a Review&#39;) . plt.figure(figsize=(12,6)) plt.xlim(right=700) plt.grid(True) p1=sns.kdeplot(df[&#39;review&#39;].astype(&#39;str&#39;).apply(len), shade=True, color=&#39;g&#39;).set_title(&#39;Kernel Distribution: Kernel Distribution of Number Of Characters in a Review&#39;) . Review with word length &lt; 0 were removed. Both review lengths of word and character distribution is right-skewed. Most reviews have around 12-13 words in length and around 50 chracters. . Most Frequent Unigram Words (N=1) . stop_words = set(nltk_stopwords.words(&#39;english&#39;)) . df[&#39;temp_list&#39;] = df[&#39;review_norm&#39;].apply(lambda x:str(x).split()) . def remove_stopword(x): return [word for word in x if word not in stop_words] df[&#39;temp_list_stopw&#39;] = df[&#39;temp_list&#39;].apply(lambda x:remove_stopword(x)) . def generate_ngrams(text, n_gram=1): ngrams = zip(*[text[i:] for i in range(n_gram)]) return [&#39; &#39;.join(ngram) for ngram in ngrams] review_unigrams = defaultdict(int) for x in df[&#39;temp_list_stopw&#39;]: for word in generate_ngrams(x): review_unigrams[word] += 1 df_review_unigrams = pd.DataFrame(sorted(review_unigrams.items(), key=lambda x: x[1])[::-1]) . # Bar graph of top N common unigram in reviews N=10 fig, axes = plt.subplots(figsize=(10, 5)) plt.tight_layout() plt.grid(True) sns.barplot(y=df_review_unigrams[0].values[:N], x=df_review_unigrams[1].values[:N], color=&#39;plum&#39;) axes.spines[&#39;right&#39;].set_visible(False) axes.set_xlabel(&#39;&#39;) axes.set_ylabel(&#39;&#39;) axes.tick_params(axis=&#39;x&#39;, labelsize=13) axes.tick_params(axis=&#39;y&#39;, labelsize=13) axes.set_title(f&#39;Top {N} most common unigrams in reviews&#39;, fontsize=15) plt.show() . . # Plot word cloud def col_func(word, font_size, position, orientation, font_path, random_state): colors = [&#39;#b58900&#39;, &#39;#cb4b16&#39;, &#39;#dc322f&#39;, &#39;#d33682&#39;, &#39;#6c71c4&#39;, &#39;#268bd2&#39;, &#39;#2aa198&#39;, &#39;#859900&#39;] return random.choice(colors) fd = { &#39;fontsize&#39;: &#39;32&#39;, &#39;fontweight&#39; : &#39;normal&#39;, &#39;verticalalignment&#39;: &#39;baseline&#39;, &#39;horizontalalignment&#39;: &#39;center&#39;, } wc = WordCloud(width=2000, height=1000, collocations=False, background_color=&quot;white&quot;, color_func=col_func, max_words=200, random_state=RANDOM_STATE) .generate_from_frequencies(review_unigrams) fig, ax = plt.subplots(figsize=(20,10)) ax.imshow(wc, interpolation=&#39;bilinear&#39;) ax.axis(&quot;off&quot;) ax.set_title(&#39;Unigram Words of reviews&#39;, pad=24, fontdict=fd) plt.show() . . In the unigram, it seems the most common words points to good, language, and game. Russian and Turkish is also shows up frequently too as unigrams. . Most Frequent Bigram Words (N=2) . review_bigrams = defaultdict(int) for i in df[&#39;temp_list_stopw&#39;]: for word in generate_ngrams(i, n_gram=2): review_bigrams[word] += 1 df_review_bigrams=pd.DataFrame(sorted(review_bigrams.items(), key=lambda x: x[1])[::-1]) . # Bar graph of top N bigram words N=10 fig, axes = plt.subplots(figsize=(10, 5), dpi=100) plt.tight_layout() plt.grid(True) sns.barplot(y=df_review_bigrams[0].values[:N], x=df_review_bigrams[1].values[:N], color=&#39;aqua&#39;) axes.spines[&#39;right&#39;].set_visible(False) axes.set_xlabel(&#39;&#39;) axes.set_ylabel(&#39;&#39;) axes.tick_params(axis=&#39;x&#39;, labelsize=13) axes.tick_params(axis=&#39;y&#39;, labelsize=13) axes.set_title(f&#39;Top {N} most common Bigrams in reviews&#39;, fontsize=15) plt.show() . . #Word cloud wc = WordCloud(width=2000, height=1000, collocations=False, background_color=&#39;white&#39;, color_func=col_func, max_words=200, random_state=RANDOM_STATE) .generate_from_frequencies(review_bigrams) fig, ax = plt.subplots(figsize=(20,10)) ax.imshow(wc, interpolation=&#39;bilinear&#39;) ax.axis(&#39;off&#39;) ax.set_title(&#39;Bigrams Words of reviews&#39;, pad=24, fontdict=fd) plt.show() . . Bigram gives a bit more context in comparison to unigram. There seems to be more language requests that can be seen. . Split Data . target = df[&#39;mark&#39;] features = df[&#39;text_lemma_nltk&#39;] cols_target = [&#39;L+&#39;, &#39;L-&#39;, &#39;RL&#39;, &#39;YL&#39;] . x_train, x_test, y_train, y_test = train_test_split( features, target, test_size=0.20, random_state=RANDOM_STATE, stratify=target) assert x_train.shape[0] == y_train.shape[0] assert x_test.shape[0] == y_test.shape[0] print(&#39;Train:&#39;, x_train.shape, &#39; Target Train:&#39;, y_train.shape) print(&#39;Test:&#39;, x_test.shape, &#39; Target Test:&#39;, y_test.shape) . Train: (41428,) Target Train: (41428,) Test: (10358,) Target Test: (10358,) . Since cross-validation is used no validation sub-dataset is needed. . encoder = LabelEncoder() y_train = encoder.fit_transform(y_train) y_test = encoder.fit_transform(y_test) . LabelEncoder is applied to the target column to normalize labels/marks. . Feature text Engineering . Feature text data will be transformed into feature vectors. . Count Vectors as features . Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document. . count_vect = CountVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;) count_vect.fit(df[&#39;text_lemma_nltk&#39;]) # Transform the training and validation data using count vectorizer object x_train_count = count_vect.transform(x_train) x_test_count = count_vect.transform(x_test) . TF-IDF Vectors as features . TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears. . $${TF(t)} = frac{ textsf{Number of times term t appears in a document}}{ textsf{Total number of terms in the document}}$$ . $${IDF(t)} = log_e( frac{ textsf{Total number of documents}}{ textsf{Number of documents with term t in it}})$$TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams) . Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents | N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams | Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus | . tfidf_vect = TfidfVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;, max_features=5000) tfidf_vect.fit(df[&#39;text_lemma_nltk&#39;]) x_train_tfidf = tfidf_vect.transform(x_train) x_test_tfidf = tfidf_vect.transform(x_test) # N-gram level tf-idf tfidf_vect_ngram = TfidfVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;, ngram_range=(2,3), max_features=5000) tfidf_vect_ngram.fit(df[&#39;text_lemma_nltk&#39;]) x_train_tfidf_ngram = tfidf_vect_ngram.transform(x_train) x_test_tfidf_ngram = tfidf_vect_ngram.transform(x_test) # Characters level tf-idf tfidf_vect_ngram_chars = TfidfVectorizer(analyzer=&#39;char&#39;, token_pattern=r&#39; w{1,}&#39;, ngram_range=(2,3), max_features=5000) tfidf_vect_ngram_chars.fit(df[&#39;text_lemma_nltk&#39;]) x_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(x_train) x_test_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(x_test) . Hstacking Text / NLP features with text feature vectors . vectorizer = FeatureUnion([ (&#39;count_vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;)), #(&#39;tfidf_vect&#39;, TfidfVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;, max_features=5000)), #(&#39;tfidf_vect_ngram&#39;, TfidfVectorizer(analyzer=&#39;word&#39;, token_pattern=r&#39; w{1,}&#39;, ngram_range=(2,3), max_features=5000)), (&#39;tfidf_vect_ngram_chars&#39;, TfidfVectorizer(analyzer=&#39;char&#39;, token_pattern=r&#39; w{1,}&#39;, ngram_range=(2,3), max_features=5000)) ]) vectorizer.fit(df[&#39;text_lemma_nltk&#39;]) x_train_vect_combo = vectorizer.transform(x_train) x_test_vect_combo = vectorizer.transform(x_test) . Model Building . def train_model(classifier, feature_vector_train, feature_vector_test, target_train, target_test, is_neural_net=False): # fit the training dataset on the classifier %time classifier.fit(feature_vector_train, target_train) # predict the target on test dataset predictions = classifier.predict(feature_vector_test) if is_neural_net: predictions = predictions.argmax(axis=-1) kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True) acc = cross_val_score(classifier, feature_vector_train, target_train, cv=kfold).mean() f1 = cross_val_score(classifier, feature_vector_train, target_train, cv=kfold, scoring=&#39;f1_macro&#39;).mean() print() print(&#39;Cross validation Accuracy score: {}&#39;.format(acc)) print(&#39;Cross validation F1 macro score: {}&#39;.format(f1)) print() print(classification_report(target_test, predictions, target_names=cols_target)) . Base Model: Logistic Regression . logreg = LogisticRegression(multi_class = &#39;multinomial&#39;, solver=&#39;newton-cg&#39;) . print(&#39;LR, Count Vectors: &#39;) train_model(logreg, x_train_count, x_test_count, y_train, y_test) . LR, Count Vectors: CPU times: user 48.8 s, sys: 38.2 s, total: 1min 27s Wall time: 44.4 s Cross validation Accuracy score: 0.8950949394977474 Cross validation F1 macro score: 0.6119392413618302 precision recall f1-score support L+ 0.45 0.32 0.38 133 L- 0.81 0.71 0.76 1096 RL 0.93 0.97 0.95 8537 YL 0.58 0.40 0.47 592 accuracy 0.90 10358 macro avg 0.69 0.60 0.64 10358 weighted avg 0.89 0.90 0.90 10358 . print(&#39;LR, Word level TF-IDF Vectors: &#39;) train_model(logreg, x_train_tfidf, x_test_tfidf, y_train, y_test) . LR, Word level TF-IDF Vectors: CPU times: user 6.82 s, sys: 5.28 s, total: 12.1 s Wall time: 6.21 s Cross validation Accuracy score: 0.9007434284672972 Cross validation F1 macro score: 0.5612290615815929 precision recall f1-score support L+ 0.80 0.06 0.11 133 L- 0.80 0.73 0.76 1096 RL 0.93 0.98 0.95 8537 YL 0.75 0.36 0.48 592 accuracy 0.91 10358 macro avg 0.82 0.53 0.58 10358 weighted avg 0.90 0.91 0.89 10358 . print(&#39;LR, Ngram level TF-IDF Vectors: &#39;) train_model(logreg, x_train_tfidf_ngram, x_test_tfidf_ngram, y_train, y_test) . LR, Ngram level TF-IDF Vectors: CPU times: user 5.85 s, sys: 4.56 s, total: 10.4 s Wall time: 5.37 s Cross validation Accuracy score: 0.8901467293794232 Cross validation F1 macro score: 0.5246856160591294 precision recall f1-score support L+ 0.70 0.05 0.10 133 L- 0.78 0.62 0.69 1096 RL 0.91 0.98 0.94 8537 YL 0.76 0.29 0.42 592 accuracy 0.89 10358 macro avg 0.79 0.49 0.54 10358 weighted avg 0.88 0.89 0.88 10358 . print(&#39;LR, Ngram Character level TF-IDF Vectors: &#39;) train_model(logreg, x_train_tfidf_ngram_chars, x_test_tfidf_ngram_chars, y_train, y_test) . LR, Ngram Character level TF-IDF Vectors: CPU times: user 31.9 s, sys: 22.2 s, total: 54.1 s Wall time: 29.1 s Cross validation Accuracy score: 0.8971709426622274 Cross validation F1 macro score: 0.5497170658507233 precision recall f1-score support L+ 0.64 0.07 0.12 133 L- 0.77 0.72 0.74 1096 RL 0.93 0.98 0.95 8537 YL 0.73 0.35 0.48 592 accuracy 0.90 10358 macro avg 0.77 0.53 0.57 10358 weighted avg 0.89 0.90 0.89 10358 . print(&#39;LR, Count, Ngram Charcter level TF-IDF Vectors: &#39;) train_model(logreg, x_train_vect_combo, x_test_vect_combo, y_train, y_test) . LR, Count, Ngram Charcter level TF-IDF Vectors: CPU times: user 3min 29s, sys: 2min 13s, total: 5min 43s Wall time: 3min 13s Cross validation Accuracy score: 0.9014675356022206 Cross validation F1 macro score: 0.632361706349813 precision recall f1-score support L+ 0.48 0.34 0.40 133 L- 0.82 0.72 0.77 1096 RL 0.94 0.97 0.95 8537 YL 0.65 0.46 0.54 592 accuracy 0.91 10358 macro avg 0.72 0.62 0.67 10358 weighted avg 0.90 0.91 0.90 10358 . The table below shows a summary of this section for base model of Logistic Regression: . Logistic Regression Parameters: multi_class = &#39;multinomial&#39;, solver=&#39;newton-cg&#39; . Count Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.895 | 0.90 | . F1 Score (Macro): | 0.612 | 0.64 | . | . Word level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.901 | 0.90 | . F1 Score (Macro): | 0.561 | 0.58 | . | . Ngram level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.890 | 0.89 | . F1 Score (Macro): | 0.525 | 0.54 | . | . Ngram Charcter level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.897 | 0.90 | . F1 Score (Macro): | 0.550 | 0.57 | . | . Count, Ngram Charcter level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.901 | 0.91 | . F1 Score (Macro): | 0.632 | 0.67 | . Based off the Count vectors&#39; and Ngram Character level TF-IDF vectors&#39; cross-validation scores, it was determined that these two feature vectors could be combined to see if the accuracy and F1 macro scores would increase and indeed these both scores did increase when both feature vectors were combined with Logistic Regression. Moving forward, we may only check Count vectors or the combined feature vector as some models take a long time to find cross-validaton scores. . Over sample minority / Under sample majority . import imblearn from imblearn.over_sampling import SMOTE from imblearn.pipeline import Pipeline, make_pipeline from imblearn.over_sampling import RandomOverSampler from imblearn.under_sampling import RandomUnderSampler . smote_pipeline = make_pipeline(SMOTE(random_state=RANDOM_STATE), LogisticRegression(multi_class = &#39;multinomial&#39;, solver=&#39;newton-cg&#39;)) over_pipeline = make_pipeline(RandomOverSampler(sampling_strategy=&#39;minority&#39;), LogisticRegression(multi_class = &#39;multinomial&#39;, solver=&#39;newton-cg&#39;)) under_pipeline = make_pipeline(RandomUnderSampler(sampling_strategy=&#39;majority&#39;), LogisticRegression(multi_class = &#39;multinomial&#39;, solver=&#39;newton-cg&#39;)) . def get_cv_scores(pipeline, x_train, y_train): acc = %time cross_val_score(pipeline, x_train, y_train, cv=5, n_jobs=-1).mean() f1 = %time cross_val_score(pipeline, x_train, y_train, scoring=&#39;f1_macro&#39;, cv=5, n_jobs=-1).mean() print(&#39;Cross validation score: &#39;, acc) print(&#39;Cross validation F1 macro score: &#39;, f1) . print(&#39;SMOTE - Synthetic Minority Over-sampling Technique (not majority)&#39;) get_cv_scores(smote_pipeline, x_train_count, y_train) . SMOTE - Synthetic Minority Over-sampling Technique (not majority) CPU times: user 5.85 s, sys: 969 ms, total: 6.82 s Wall time: 19min 25s CPU times: user 5.94 s, sys: 797 ms, total: 6.74 s Wall time: 19min 23s Cross validation score: 0.8415805939474295 Cross validation F1 macro score: 0.5601322921363983 . print(&#39;Random Over Sample (minority)&#39;) get_cv_scores(over_pipeline, x_train_count, y_train) . Random Over Sample (minority) CPU times: user 1.51 s, sys: 215 ms, total: 1.72 s Wall time: 4min 49s CPU times: user 1.48 s, sys: 197 ms, total: 1.68 s Wall time: 4min 39s Cross validation score: 0.8839431104460905 Cross validation F1 macro score: 0.5873712618711804 . print(&#39;Random Under Sample (majority)&#39;) get_cv_scores(under_pipeline, x_train_count, y_train) . Random Under Sample (majority) CPU times: user 205 ms, sys: 35.8 ms, total: 241 ms Wall time: 31.6 s CPU times: user 171 ms, sys: 21.7 ms, total: 193 ms Wall time: 24.6 s Cross validation score: 0.5309693980335767 Cross validation F1 macro score: 0.4159653917859837 . In regards to correcting the class imbalance, it seems like the method of over sampling the minority did best. However, with using any of these methods it decreased both accuracy and F1 macro scores. It may not be worth moving forward with these methods if they have been shown to decrease both accuracy and F1 macro with cross-validation. . Logistic Regression with Count Vectors Accuracy F1 Macro . Cross-Validation of 5 fold mean scores | . | . With no sampling | 0.89509 | 0.61194 | . Synthetic Minority Over-sampling Technique (SMOTE): | 0.84158 | 0.56013 | . Random Over Sample (minority): | 0.88394 | 0.58737 | . Random Under Sample (majority): | 0.53097 | 0.41596 | . Random Forest . rf = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=100, min_samples_leaf=2, max_features=300, class_weight=&#39;balanced&#39;) . print(&#39;RF, Count Vectors: &#39;) train_model(rf, x_train_count, x_test_count, y_train, y_test) . RF, Count Vectors: CPU times: user 47.2 s, sys: 100 ms, total: 47.3 s Wall time: 47.1 s Cross validation Accuracy score: 0.8760741686284433 Cross validation F1 macro score: 0.5775907873660777 precision recall f1-score support L+ 0.50 0.11 0.17 133 L- 0.61 0.83 0.71 1096 RL 0.95 0.94 0.94 8537 YL 0.63 0.44 0.52 592 accuracy 0.89 10358 macro avg 0.67 0.58 0.59 10358 weighted avg 0.89 0.89 0.88 10358 . print(&#39;RF, NLTK/TF-IDF, Count, Ngram level TF-IDF Vectors: &#39;) train_model(rf, x_train_vect_combo, x_test_vect_combo, y_train, y_test) . RF, NLTK/TF-IDF, Count, Ngram level TF-IDF Vectors: CPU times: user 2min 29s, sys: 251 ms, total: 2min 29s Wall time: 2min 29s Cross validation Accuracy score: 0.8780292648847748 Cross validation F1 macro score: 0.47457559490202045 precision recall f1-score support L+ 0.40 0.02 0.03 133 L- 0.66 0.68 0.67 1096 RL 0.91 0.97 0.94 8537 YL 0.87 0.13 0.23 592 accuracy 0.88 10358 macro avg 0.71 0.45 0.47 10358 weighted avg 0.87 0.88 0.86 10358 . Random Forest Classifer did not have better results than the base Logistic Regression model. The table below sums up the testing results we had for the Random Forest model. Perhaps better scores could be obtained through hyper parameterization but this can be expensive in regards to computational cost. . Random Forest Classifier . Parameters: random_state= 42, n_estimators=100, max_depth=100, min_samples_leaf=2, max_features=300, class_weight=&#39;balanced&#39; | . | . Count Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.876 | 0.89 | . F1 Score (Macro): | 0.577 | 0.59 | . | . Count, Ngram Charcter level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.878 | 0.88 | . F1 Score (Macro): | 0.475 | 0.47 | . SVM LinearSVC . model_svc = LinearSVC(class_weight=&#39;balanced&#39;, random_state=RANDOM_STATE, max_iter=1000) . print(&#39;SVC, Count Vectors: &#39;) train_model(model_svc, x_train_count, x_test_count, y_train, y_test) . SVC, Count Vectors: CPU times: user 14.9 s, sys: 11.1 ms, total: 14.9 s Wall time: 14.9 s Cross validation Accuracy score: 0.876122297158421 Cross validation F1 macro score: 0.5970908433767836 precision recall f1-score support L+ 0.28 0.27 0.27 133 L- 0.73 0.72 0.73 1096 RL 0.94 0.94 0.94 8537 YL 0.47 0.51 0.49 592 accuracy 0.88 10358 macro avg 0.60 0.61 0.61 10358 weighted avg 0.89 0.88 0.88 10358 . print(&#39;SVC, NLTK/TF-IDF, Count, Ngram character level TF-IDF Vectors: &#39;) train_model(model_svc, x_train_vect_combo, x_test_vect_combo, y_train, y_test) . SVC, NLTK/TF-IDF, Count, Ngram character level TF-IDF Vectors: CPU times: user 1min 20s, sys: 159 ms, total: 1min 20s Wall time: 1min 20s Cross validation Accuracy score: 0.8842087583727837 Cross validation F1 macro score: 0.6116865676987674 precision recall f1-score support L+ 0.37 0.32 0.34 133 L- 0.75 0.73 0.74 1096 RL 0.95 0.95 0.95 8537 YL 0.49 0.54 0.51 592 accuracy 0.89 10358 macro avg 0.64 0.63 0.64 10358 weighted avg 0.89 0.89 0.89 10358 . Similar results to Random Forest model, the Linear SVC model did perform slightly better but not better than the base Logistic Regression model. Same issue we face with Random Forest where we might better scores through hyper parameterization but this can be expensive in regards to computational cost. . Linear SVC . Parameters: random_state= 42, class_weight=&#39;balanced&#39;, max_iter=1000&#39; | . | . Count Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.876 | 0.88 | . F1 Score (Macro): | 0.597 | 0.61 | . | . Count, Ngram Charcter level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.884 | 0.89 | . F1 Score (Macro): | 0.612 | 0.64 | . CatBoost Model . cb = CatBoostClassifier(iterations=1000, task_type=&#39;GPU&#39;, logging_level=&#39;Silent&#39;) . print(&#39;CB, Count Vectors: &#39;) train_model(cb, x_train_count, x_test_count, y_train, y_test) . CB, Count Vectors: . Custom logger is already specified. Specify more than one logger at same time is not thread safe. . CPU times: user 17min 47s, sys: 2.65 s, total: 17min 50s Wall time: 9min 29s Cross validation Accuracy score: 0.8920535572650119 Cross validation F1 macro score: 0.5007487070760853 precision recall f1-score support L+ 0.69 0.07 0.12 133 L- 0.80 0.71 0.75 1096 RL 0.91 0.99 0.95 8537 YL 0.73 0.19 0.30 592 accuracy 0.90 10358 macro avg 0.78 0.49 0.53 10358 weighted avg 0.89 0.90 0.88 10358 . print(&#39;Catboost, NLTK/TF-IDF, Count, Ngram level TF-IDF Vectors: &#39;) train_model(cb, x_train_vect_combo, x_test_vect_combo, y_train, y_test) . Catboost, NLTK/TF-IDF, Count, Ngram level TF-IDF Vectors: CPU times: user 1min 30s, sys: 34.7 s, total: 2min 5s Wall time: 1min 20s Cross validation Accuracy score: 0.899029542963963 Cross validation F1 macro score: 0.5505443023934945 precision recall f1-score support L+ 0.67 0.09 0.16 133 L- 0.79 0.74 0.76 1096 RL 0.92 0.98 0.95 8537 YL 0.76 0.29 0.42 592 accuracy 0.91 10358 macro avg 0.78 0.53 0.57 10358 weighted avg 0.90 0.91 0.89 10358 . With CatBoost Classifer, the metric scores for accuracy and F1 macro were better than both Random Forest and Linear SVC, but the F1 macro score is still a bit lower than base Logistic Regression model. With hyper parameterization, I am sure we can get high metric scores for CatBoost. However, just like the other previous models this can be expensive in regards to computational cost. . CatBoost Classifier . Parameters: iterations=1000, task_type=&#39;GPU&#39;, logging_level=&#39;Silent&#39; | . | . Count Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.892 | 0.90 | . F1 Score (Macro): | 0.500 | 0.53 | . | . Count, Ngram Charcter level TF-IDF Vectors: | Cross-Validation of 5 fold mean scores: | Testing dataset scores: | . Accuracy: | 0.899 | 0.91 | . F1 Score (Macro): | 0.551 | 0.57 | . Keras Deep Learning Model . BERT Embeddings with TensorFlow 2 (bert-for-tf2) . BERT Word Embeddings . !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py . module_url = &#39;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&#39; bert_layer = hub.KerasLayer(module_url, trainable=True) . vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case) # create token-embedding mapping def bert_encode(texts, tokenizer, max_len=512): all_tokens = [] all_masks = [] all_segments = [] for text in texts: # tokenize text text = tokenizer.tokenize(text) # convert text to sequence of tokens and pad them to ensure equal length vectors text = text[:max_len-2] input_sequence = [&quot;[CLS]&quot;] + text + [&quot;[SEP]&quot;] pad_len = max_len - len(input_sequence) tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len pad_masks = [1] * len(input_sequence) + [0] * pad_len segment_ids = [0] * max_len all_tokens.append(tokens) all_masks.append(pad_masks) all_segments.append(segment_ids) return np.array(all_tokens), np.array(all_masks), np.array(all_segments) . Model . def build_model(bert_layer, max_len=512): # Add an Input Layer input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;) input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;) segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;segment_ids&quot;) # Add the BERT layer sequence output pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) clf_output = sequence_output[:, 0, :] # Add the output Layers output_layer1 = tf.keras.layers.Dense(64, activation=&#39;relu&#39;)(clf_output) output_layer1 = tf.keras.layers.Dropout(0.2)(output_layer1) output_layer1 = tf.keras.layers.Dense(32, activation=&#39;relu&#39;)(output_layer1) output_layer1 = tf.keras.layers.Dropout(0.2)(output_layer1) output_layer2 = tf.keras.layers.Dense(4, activation=&#39;softmax&#39;)(output_layer1) # Compile the model model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output_layer2) model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . max_len = 150 train_input = bert_encode(x_train, tokenizer, max_len=max_len) test_input = bert_encode(x_test, tokenizer, max_len=max_len) . num_classes = 4 train_labels = keras.utils.to_categorical(y_train, num_classes) test_labels = keras.utils.to_categorical(y_test, num_classes) . model = build_model(bert_layer, max_len=max_len) model.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_word_ids (InputLayer) [(None, 150)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 150)] 0 __________________________________________________________________________________________________ segment_ids (InputLayer) [(None, 150)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_word_ids[0][0] input_mask[0][0] segment_ids[0][0] __________________________________________________________________________________________________ tf.__operators__.getitem (Slici (None, 768) 0 keras_layer[0][1] __________________________________________________________________________________________________ dense (Dense) (None, 64) 49216 tf.__operators__.getitem[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 64) 0 dense[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 32) 2080 dropout[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 32) 0 dense_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 4) 132 dropout_1[0][0] ================================================================================================== Total params: 109,533,669 Trainable params: 109,533,668 Non-trainable params: 1 __________________________________________________________________________________________________ . checkpoint = tf.keras.callbacks.ModelCheckpoint(&#39;/content/gdrive/MyDrive/AllcorrectGames/model.h5&#39;, monitor=&#39;val_accuracy&#39;, save_best_only=True, verbose=1) earlystopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_accuracy&#39;, patience=5, verbose=1) train_history = model.fit( train_input, train_labels, validation_split=0.2, epochs=3, callbacks=[checkpoint, earlystopping], batch_size=32, verbose=1) . Epoch 1/3 1036/1036 [==============================] - 1118s 1s/step - loss: 0.3876 - accuracy: 0.8751 - val_loss: 0.2556 - val_accuracy: 0.9289 Epoch 00001: val_accuracy improved from -inf to 0.92892, saving model to model.h5 Epoch 2/3 1036/1036 [==============================] - 1104s 1s/step - loss: 0.2100 - accuracy: 0.9343 - val_loss: 0.2026 - val_accuracy: 0.9359 Epoch 00002: val_accuracy improved from 0.92892 to 0.93592, saving model to model.h5 Epoch 3/3 1036/1036 [==============================] - 1104s 1s/step - loss: 0.1538 - accuracy: 0.9530 - val_loss: 0.2093 - val_accuracy: 0.9405 Epoch 00003: val_accuracy improved from 0.93592 to 0.94050, saving model to model.h5 . model.load_weights(&#39;/content/gdrive/MyDrive/AllcorrectGames/model.h5&#39;) test_pred = model.predict(test_input) # For netural_net this gives probabilities (proba) # For neural_net to get predicted classes: test_pred_ = test_pred.argmax(axis=-1) print(classification_report(y_test, test_pred_, target_names=cols_target)) . precision recall f1-score support L+ 0.80 0.64 0.71 133 L- 0.88 0.87 0.87 1096 RL 0.97 0.98 0.97 8537 YL 0.83 0.67 0.74 592 accuracy 0.95 10358 macro avg 0.87 0.79 0.82 10358 weighted avg 0.95 0.95 0.95 10358 . model.save(&#39;/content/gdrive/MyDrive/AllcorrectGames/keras_bert_model&#39;) . WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 945). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AllcorrectGames/keras_bert_model/assets . INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AllcorrectGames/keras_bert_model/assets . The Keras model with bert-for-tf2 performed the best out of all the models tested. Fitting the model did take a bit long though the time shorten when ran with GPU over CPU (about 13 hours per epoch on CPU and about 15 minutes per epooch on GPU). . Keras model with bert-for-tf2 . | Validation scores: | Testing dataset scores: | . Accuracy: | 0.94 | 0.95 | . F1 Score (Macro): | n/a | 0.82 | . It would be interesting to see changing the learning rate or any of the output layers to see if metric scores would increase. In the future, if GPU is available it might be ideal to test hyper tunning this model to see if better metrics are obtainable. . Overall Conclusion . The table below show a summary of all the metrics and models tested in the project. Overall, the best metrics came from the Keras model using BERT Embeddings with TensorFlow 2. . . Text Processing Method Model Accuracy F1 Macro Cross-Validation Accuracy Cross-Validation F1 Macro . Count Vectors | Logistic Regression | 0.90 | 0.64 | 0.90 | 0.61 | . Word level TF-IDF | Logistic Regression | 0.90 | 0.58 | 0.90 | 0.56 | . Ngram level TF-IDF | Logistic Regression | 0.89 | 0.54 | 0.89 | 0.52 | . Ngram Charcter level TF-IDF | Logistic Regression | 0.90 | 0.57 | 0.90 | 0.55 | . Count, Ngram Charcter level TF-IDF | Logistic Regression | 0.91 | 0.67 | 0.90 | 0.63 | . Count Vectors | Logistic Regression SMOTE | n/a | n/a | 0.84 | 0.56 | . Spacy/TF-IDF | Logistic Regression Random Over Sample (minority) | n/a | n/a | 0.88 | 0.59 | . Spacy/TF-IDF | Logistic Regression Random Under Sample (majority) | n/a | n/a | 0.53 | 0.42 | . Count Vectors | Random Forest Classifier | 0.89 | 0.59 | 0.88 | 0.58 | . Count, Ngram Charcter level TF-IDF | Random Forest Classifier | 0.88 | 0.47 | 0.88 | 0.48 | . Count Vectors | Linear SVC | 0.88 | 0.61 | 0.88 | 0.60 | . Count, Ngram Charcter level TF-IDF | Linear SVC | 0.89 | 0.64 | 0.88 | 0.61 | . Count Vectors | CatBoostClassifier | 0.90 | 0.53 | 0.89 | 0.50 | . Count, Ngram Charcter level TF-IDF | CatBoostClassifier | 0.91 | 0.57 | 0.90 | 0.55 | . BERT Embeddings with TensorFlow 2 (bert-for-tf2) | Keras Deep Learning | 0.95 | 0.82 | 0.94 | n/a | . Python Script . Ciick here to see the main.py file . Python script with pretrained model that allows one to classify users&#39; reviews. . Input excel file with two colunns: . Unique ID | Review&#39;s text | . Output is excel file with 6 colunns: . Unique ID | Review&#39;s text | Probability L+ | Probability L- | Probability RL | Probability YL | .",
            "url": "https://cmdang-mochi.github.io/ds-projects/machine%20learning/python/exploratory%20analysis/nlp/sentiment%20analysis/text%20processing/lemmatization/pandas/numpy/mathplotlib/scikit-learn/wordcloud/nltk/catboost/tensorflow/tf2/keras/bert/bert-for-tf2/2021/06/25/allcorrect_games_nlp_text_multiclass_classification.html",
            "relUrl": "/machine%20learning/python/exploratory%20analysis/nlp/sentiment%20analysis/text%20processing/lemmatization/pandas/numpy/mathplotlib/scikit-learn/wordcloud/nltk/catboost/tensorflow/tf2/keras/bert/bert-for-tf2/2021/06/25/allcorrect_games_nlp_text_multiclass_classification.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Gold Recovery Optimization",
            "content": "Project Description . Zyfra develops solutions for the efficient operation of industrial plants. The company needs a machine learning model to predict the recovery rate of gold from gold ore. . Optimize the work of a gold-mining company by predicting amount of gold extracted. . Train a model which can predict the amount of gold extracted from gold ore based on extraction and purification data. . The data is stored in three files: . gold_recovery_train.csv — training dataset | gold_recovery_test.csv — test dataset | gold_recovery_full.csv — source dataset | . Data is indexed with the date and time of acquisition (date feature). Parameters that are next to each other in terms of time are often similar. Some parameters are not available because they were measured and/or calculated much later. That&#39;s why, some of the features that are present in the training set may be absent from the test set. The test set also doesn&#39;t contain targets. . The source dataset contains the training and test sets with all the features. . Technological process . . When the mined ore undergoes primary processing, a crushed mixture is obtained. It is sent for flotation (enrichment) and two-stage purification: . Flotation - A mixture of gold-bearing ore is fed into the flotation plant. After beneficiation, a rough concentrate and &quot;tailings&quot;, i.e. product leftovers with a low concentration of valuable metals, are obtained. The stability of this process is affected by the inconsistent and non-optimal physical and chemical state of the flotation slurry (mixture of solid particles and liquid). . | Purification - The crude concentrate undergoes two purifications. The output is the final concentrate and new tailings. . | Data description . Technological process . Rougher feed — raw material | Rougher additions (or reagent additions) — flotation reagents: Xanthate, Sulphate, Depressant Xanthate — promoter or flotation activator; | Sulphate — sodium sulphide for this particular process; | Depressant — sodium silicate. | . | Rougher process — flotation | Rougher tails — product residues | Float banks — flotation unit | Cleaner process — purification | Rougher Au — rougher gold concentrate | Final Au — final gold concentrate | . Parameters of stages . air amount — volume of air | fluid levels | feed size — feed particle size | feed rate | . Feature naming . [stage].[parameter_type].[parameter_name] Example: rougher.input.feed_ag . Possible values for [stage]: . rougher — flotation | primary_cleaner — primary purification | secondary_cleaner — secondary purification | final — final characteristics | . Possible values for [parameter_type]: . input — raw material parameters | output — product parameters | state — parameters characterizing the current state of the stage | calculation — calculation characteristics | . . Import Libraries . import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy import stats as st from sklearn.metrics import mean_absolute_error, make_scorer from sklearn.model_selection import cross_val_score, GridSearchCV, KFold from sklearn.multioutput import MultiOutputRegressor from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor RANDOM_STATE = 42 . Exploratory data analysis . Load Data . df_train = pd.read_csv(&#39;/datasets/gold_recovery_train.csv&#39;) df_test = pd.read_csv(&#39;/datasets/gold_recovery_test.csv&#39;) df = pd.read_csv(&#39;/datasets/gold_recovery_full.csv&#39;) . #collapse-hide def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . . get_information(df) . Head: . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22627 non-null float64 final.output.concentrate_pb 22629 non-null float64 final.output.concentrate_sol 22331 non-null float64 final.output.concentrate_au 22630 non-null float64 final.output.recovery 20753 non-null float64 final.output.tail_ag 22633 non-null float64 final.output.tail_pb 22516 non-null float64 final.output.tail_sol 22445 non-null float64 final.output.tail_au 22635 non-null float64 primary_cleaner.input.sulfate 21107 non-null float64 primary_cleaner.input.depressant 21170 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 21565 non-null float64 primary_cleaner.output.concentrate_ag 22618 non-null float64 primary_cleaner.output.concentrate_pb 22268 non-null float64 primary_cleaner.output.concentrate_sol 21918 non-null float64 primary_cleaner.output.concentrate_au 22618 non-null float64 primary_cleaner.output.tail_ag 22614 non-null float64 primary_cleaner.output.tail_pb 22594 non-null float64 primary_cleaner.output.tail_sol 22365 non-null float64 primary_cleaner.output.tail_au 22617 non-null float64 primary_cleaner.state.floatbank8_a_air 22660 non-null float64 primary_cleaner.state.floatbank8_a_level 22667 non-null float64 primary_cleaner.state.floatbank8_b_air 22660 non-null float64 primary_cleaner.state.floatbank8_b_level 22673 non-null float64 primary_cleaner.state.floatbank8_c_air 22662 non-null float64 primary_cleaner.state.floatbank8_c_level 22673 non-null float64 primary_cleaner.state.floatbank8_d_air 22661 non-null float64 primary_cleaner.state.floatbank8_d_level 22673 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22672 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.au_pb_ratio 21089 non-null float64 rougher.input.feed_ag 22618 non-null float64 rougher.input.feed_pb 22472 non-null float64 rougher.input.feed_rate 22163 non-null float64 rougher.input.feed_size 22277 non-null float64 rougher.input.feed_sol 22357 non-null float64 rougher.input.feed_au 22617 non-null float64 rougher.input.floatbank10_sulfate 21415 non-null float64 rougher.input.floatbank10_xanthate 22247 non-null float64 rougher.input.floatbank11_sulfate 22038 non-null float64 rougher.input.floatbank11_xanthate 20459 non-null float64 rougher.output.concentrate_ag 22618 non-null float64 rougher.output.concentrate_pb 22618 non-null float64 rougher.output.concentrate_sol 22526 non-null float64 rougher.output.concentrate_au 22618 non-null float64 rougher.output.recovery 19597 non-null float64 rougher.output.tail_ag 19979 non-null float64 rougher.output.tail_pb 22618 non-null float64 rougher.output.tail_sol 19980 non-null float64 rougher.output.tail_au 19980 non-null float64 rougher.state.floatbank10_a_air 22646 non-null float64 rougher.state.floatbank10_a_level 22647 non-null float64 rougher.state.floatbank10_b_air 22646 non-null float64 rougher.state.floatbank10_b_level 22647 non-null float64 rougher.state.floatbank10_c_air 22646 non-null float64 rougher.state.floatbank10_c_level 22654 non-null float64 rougher.state.floatbank10_d_air 22641 non-null float64 rougher.state.floatbank10_d_level 22649 non-null float64 rougher.state.floatbank10_e_air 22096 non-null float64 rougher.state.floatbank10_e_level 22649 non-null float64 rougher.state.floatbank10_f_air 22641 non-null float64 rougher.state.floatbank10_f_level 22642 non-null float64 secondary_cleaner.output.tail_ag 22616 non-null float64 secondary_cleaner.output.tail_pb 22600 non-null float64 secondary_cleaner.output.tail_sol 20501 non-null float64 secondary_cleaner.output.tail_au 22618 non-null float64 secondary_cleaner.state.floatbank2_a_air 22333 non-null float64 secondary_cleaner.state.floatbank2_a_level 22591 non-null float64 secondary_cleaner.state.floatbank2_b_air 22538 non-null float64 secondary_cleaner.state.floatbank2_b_level 22588 non-null float64 secondary_cleaner.state.floatbank3_a_air 22585 non-null float64 secondary_cleaner.state.floatbank3_a_level 22587 non-null float64 secondary_cleaner.state.floatbank3_b_air 22592 non-null float64 secondary_cleaner.state.floatbank3_b_level 22590 non-null float64 secondary_cleaner.state.floatbank4_a_air 22571 non-null float64 secondary_cleaner.state.floatbank4_a_level 22587 non-null float64 secondary_cleaner.state.floatbank4_b_air 22608 non-null float64 secondary_cleaner.state.floatbank4_b_level 22607 non-null float64 secondary_cleaner.state.floatbank5_a_air 22615 non-null float64 secondary_cleaner.state.floatbank5_a_level 22615 non-null float64 secondary_cleaner.state.floatbank5_b_air 22615 non-null float64 secondary_cleaner.state.floatbank5_b_level 22616 non-null float64 secondary_cleaner.state.floatbank6_a_air 22597 non-null float64 secondary_cleaner.state.floatbank6_a_level 22615 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . None . **************************************************************************************************** Describe: . final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au primary_cleaner.input.sulfate ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 22627.000000 | 22629.000000 | 22331.000000 | 22630.000000 | 20753.000000 | 22633.000000 | 22516.000000 | 22445.000000 | 22635.000000 | 21107.000000 | ... | 22571.000000 | 22587.000000 | 22608.000000 | 22607.000000 | 22615.000000 | 22615.000000 | 22615.000000 | 22616.000000 | 22597.000000 | 22615.000000 | . mean | 4.781559 | 9.095308 | 8.640317 | 40.001172 | 67.447488 | 8.923690 | 2.488252 | 9.523632 | 2.827459 | 140.277672 | ... | 18.205125 | -499.878977 | 14.356474 | -476.532613 | 14.883276 | -503.323288 | 11.626743 | -500.521502 | 17.976810 | -519.361465 | . std | 2.030128 | 3.230797 | 3.785035 | 13.398062 | 11.616034 | 3.517917 | 1.189407 | 4.079739 | 1.262834 | 49.919004 | ... | 6.560700 | 80.273964 | 5.655791 | 93.822791 | 6.372811 | 72.925589 | 5.757449 | 78.956292 | 6.636203 | 75.477151 | . min | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000003 | ... | 0.000000 | -799.920713 | 0.000000 | -800.836914 | -0.423260 | -799.741097 | 0.427084 | -800.258209 | -0.079426 | -810.473526 | . 25% | 4.018525 | 8.750171 | 7.116799 | 42.383721 | 63.282393 | 7.684016 | 1.805376 | 8.143576 | 2.303108 | 110.177081 | ... | 14.095940 | -500.896232 | 10.882675 | -500.309169 | 10.941299 | -500.628697 | 8.037533 | -500.167897 | 13.968418 | -500.981671 | . 50% | 4.953729 | 9.914519 | 8.908792 | 44.653436 | 68.322258 | 9.484369 | 2.653001 | 10.212998 | 2.913794 | 141.330501 | ... | 18.007326 | -499.917108 | 14.947646 | -499.612292 | 14.859117 | -499.865158 | 10.989756 | -499.951980 | 18.004215 | -500.095463 | . 75% | 5.862593 | 10.929839 | 10.705824 | 46.111999 | 72.950836 | 11.084557 | 3.287790 | 11.860824 | 3.555077 | 174.049914 | ... | 22.998194 | -498.361545 | 17.977502 | -400.224147 | 18.014914 | -498.489381 | 14.001193 | -499.492354 | 23.009704 | -499.526388 | . max | 16.001945 | 17.031899 | 19.615720 | 53.611374 | 100.000000 | 19.552149 | 6.086532 | 22.861749 | 9.789625 | 274.409626 | ... | 60.000000 | -127.692333 | 31.269706 | -6.506986 | 63.116298 | -244.483566 | 39.846228 | -120.190931 | 54.876806 | -29.093593 | . 8 rows × 86 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . final.output.concentrate_ag | 89 | 0.39 | . final.output.concentrate_pb | 87 | 0.38 | . final.output.concentrate_sol | 385 | 1.69 | . final.output.concentrate_au | 86 | 0.38 | . ... | ... | ... | . secondary_cleaner.state.floatbank5_a_level | 101 | 0.44 | . secondary_cleaner.state.floatbank5_b_air | 101 | 0.44 | . secondary_cleaner.state.floatbank5_b_level | 100 | 0.44 | . secondary_cleaner.state.floatbank6_a_air | 119 | 0.52 | . secondary_cleaner.state.floatbank6_a_level | 101 | 0.44 | . 87 rows × 2 columns . **************************************************************************************************** Shape: . (22716, 87) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . get_information(df_train) . Head: . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16788 non-null float64 final.output.concentrate_pb 16788 non-null float64 final.output.concentrate_sol 16490 non-null float64 final.output.concentrate_au 16789 non-null float64 final.output.recovery 15339 non-null float64 final.output.tail_ag 16794 non-null float64 final.output.tail_pb 16677 non-null float64 final.output.tail_sol 16715 non-null float64 final.output.tail_au 16794 non-null float64 primary_cleaner.input.sulfate 15553 non-null float64 primary_cleaner.input.depressant 15598 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 15875 non-null float64 primary_cleaner.output.concentrate_ag 16778 non-null float64 primary_cleaner.output.concentrate_pb 16502 non-null float64 primary_cleaner.output.concentrate_sol 16224 non-null float64 primary_cleaner.output.concentrate_au 16778 non-null float64 primary_cleaner.output.tail_ag 16777 non-null float64 primary_cleaner.output.tail_pb 16761 non-null float64 primary_cleaner.output.tail_sol 16579 non-null float64 primary_cleaner.output.tail_au 16777 non-null float64 primary_cleaner.state.floatbank8_a_air 16820 non-null float64 primary_cleaner.state.floatbank8_a_level 16827 non-null float64 primary_cleaner.state.floatbank8_b_air 16820 non-null float64 primary_cleaner.state.floatbank8_b_level 16833 non-null float64 primary_cleaner.state.floatbank8_c_air 16822 non-null float64 primary_cleaner.state.floatbank8_c_level 16833 non-null float64 primary_cleaner.state.floatbank8_d_air 16821 non-null float64 primary_cleaner.state.floatbank8_d_level 16833 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.au_pb_ratio 15618 non-null float64 rougher.input.feed_ag 16778 non-null float64 rougher.input.feed_pb 16632 non-null float64 rougher.input.feed_rate 16347 non-null float64 rougher.input.feed_size 16443 non-null float64 rougher.input.feed_sol 16568 non-null float64 rougher.input.feed_au 16777 non-null float64 rougher.input.floatbank10_sulfate 15816 non-null float64 rougher.input.floatbank10_xanthate 16514 non-null float64 rougher.input.floatbank11_sulfate 16237 non-null float64 rougher.input.floatbank11_xanthate 14956 non-null float64 rougher.output.concentrate_ag 16778 non-null float64 rougher.output.concentrate_pb 16778 non-null float64 rougher.output.concentrate_sol 16698 non-null float64 rougher.output.concentrate_au 16778 non-null float64 rougher.output.recovery 14287 non-null float64 rougher.output.tail_ag 14610 non-null float64 rougher.output.tail_pb 16778 non-null float64 rougher.output.tail_sol 14611 non-null float64 rougher.output.tail_au 14611 non-null float64 rougher.state.floatbank10_a_air 16807 non-null float64 rougher.state.floatbank10_a_level 16807 non-null float64 rougher.state.floatbank10_b_air 16807 non-null float64 rougher.state.floatbank10_b_level 16807 non-null float64 rougher.state.floatbank10_c_air 16807 non-null float64 rougher.state.floatbank10_c_level 16814 non-null float64 rougher.state.floatbank10_d_air 16802 non-null float64 rougher.state.floatbank10_d_level 16809 non-null float64 rougher.state.floatbank10_e_air 16257 non-null float64 rougher.state.floatbank10_e_level 16809 non-null float64 rougher.state.floatbank10_f_air 16802 non-null float64 rougher.state.floatbank10_f_level 16802 non-null float64 secondary_cleaner.output.tail_ag 16776 non-null float64 secondary_cleaner.output.tail_pb 16764 non-null float64 secondary_cleaner.output.tail_sol 14874 non-null float64 secondary_cleaner.output.tail_au 16778 non-null float64 secondary_cleaner.state.floatbank2_a_air 16497 non-null float64 secondary_cleaner.state.floatbank2_a_level 16751 non-null float64 secondary_cleaner.state.floatbank2_b_air 16705 non-null float64 secondary_cleaner.state.floatbank2_b_level 16748 non-null float64 secondary_cleaner.state.floatbank3_a_air 16763 non-null float64 secondary_cleaner.state.floatbank3_a_level 16747 non-null float64 secondary_cleaner.state.floatbank3_b_air 16752 non-null float64 secondary_cleaner.state.floatbank3_b_level 16750 non-null float64 secondary_cleaner.state.floatbank4_a_air 16731 non-null float64 secondary_cleaner.state.floatbank4_a_level 16747 non-null float64 secondary_cleaner.state.floatbank4_b_air 16768 non-null float64 secondary_cleaner.state.floatbank4_b_level 16767 non-null float64 secondary_cleaner.state.floatbank5_a_air 16775 non-null float64 secondary_cleaner.state.floatbank5_a_level 16775 non-null float64 secondary_cleaner.state.floatbank5_b_air 16775 non-null float64 secondary_cleaner.state.floatbank5_b_level 16776 non-null float64 secondary_cleaner.state.floatbank6_a_air 16757 non-null float64 secondary_cleaner.state.floatbank6_a_level 16775 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB . None . **************************************************************************************************** Describe: . final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au primary_cleaner.input.sulfate ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 16788.000000 | 16788.000000 | 16490.000000 | 16789.000000 | 15339.000000 | 16794.000000 | 16677.000000 | 16715.000000 | 16794.000000 | 15553.000000 | ... | 16731.000000 | 16747.000000 | 16768.000000 | 16767.000000 | 16775.000000 | 16775.000000 | 16775.000000 | 16776.000000 | 16757.000000 | 16775.000000 | . mean | 4.716907 | 9.113559 | 8.301123 | 39.467217 | 67.213166 | 8.757048 | 2.360327 | 9.303932 | 2.687512 | 129.479789 | ... | 19.101874 | -494.164481 | 14.778164 | -476.600082 | 15.779488 | -500.230146 | 12.377241 | -498.956257 | 18.429208 | -521.801826 | . std | 2.096718 | 3.389495 | 3.825760 | 13.917227 | 11.960446 | 3.634103 | 1.215576 | 4.263208 | 1.272757 | 45.386931 | ... | 6.883163 | 84.803334 | 5.999149 | 89.381172 | 6.834703 | 76.983542 | 6.219989 | 82.146207 | 6.958294 | 77.170888 | . min | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000003 | ... | 0.000000 | -799.920713 | 0.000000 | -800.021781 | -0.423260 | -799.741097 | 0.427084 | -800.258209 | 0.024270 | -810.473526 | . 25% | 3.971262 | 8.825748 | 6.939185 | 42.055722 | 62.625685 | 7.610544 | 1.641604 | 7.870275 | 2.172953 | 103.064021 | ... | 14.508299 | -500.837689 | 10.741388 | -500.269182 | 10.977713 | -500.530594 | 8.925586 | -500.147603 | 13.977626 | -501.080595 | . 50% | 4.869346 | 10.065316 | 8.557228 | 44.498874 | 67.644601 | 9.220393 | 2.453690 | 10.021968 | 2.781132 | 131.783108 | ... | 19.986958 | -499.778379 | 14.943933 | -499.593286 | 15.998340 | -499.784231 | 11.092839 | -499.933330 | 18.034960 | -500.109898 | . 75% | 5.821176 | 11.054809 | 10.289741 | 45.976222 | 72.824595 | 10.971110 | 3.192404 | 11.648573 | 3.416936 | 159.539839 | ... | 24.983961 | -494.648754 | 20.023751 | -400.137948 | 20.000701 | -496.531781 | 15.979467 | -498.418000 | 24.984992 | -499.565540 | . max | 16.001945 | 17.031899 | 18.124851 | 53.611374 | 100.000000 | 19.552149 | 6.086532 | 22.317730 | 9.789625 | 251.999948 | ... | 60.000000 | -127.692333 | 28.003828 | -71.472472 | 63.116298 | -275.073125 | 39.846228 | -120.190931 | 54.876806 | -39.784927 | . 8 rows × 86 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . final.output.concentrate_ag | 72 | 0.43 | . final.output.concentrate_pb | 72 | 0.43 | . final.output.concentrate_sol | 370 | 2.19 | . final.output.concentrate_au | 71 | 0.42 | . ... | ... | ... | . secondary_cleaner.state.floatbank5_a_level | 85 | 0.50 | . secondary_cleaner.state.floatbank5_b_air | 85 | 0.50 | . secondary_cleaner.state.floatbank5_b_level | 84 | 0.50 | . secondary_cleaner.state.floatbank6_a_air | 103 | 0.61 | . secondary_cleaner.state.floatbank6_a_level | 85 | 0.50 | . 87 rows × 2 columns . **************************************************************************************************** Shape: . (16860, 87) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . get_information(df_test) . Head: . date primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-09-01 00:59:59 | 210.800909 | 14.993118 | 8.080000 | 1.005021 | 1398.981301 | -500.225577 | 1399.144926 | -499.919735 | 1400.102998 | ... | 12.023554 | -497.795834 | 8.016656 | -501.289139 | 7.946562 | -432.317850 | 4.872511 | -500.037437 | 26.705889 | -499.709414 | . 1 | 2016-09-01 01:59:59 | 215.392455 | 14.987471 | 8.080000 | 0.990469 | 1398.777912 | -500.057435 | 1398.055362 | -499.778182 | 1396.151033 | ... | 12.058140 | -498.695773 | 8.130979 | -499.634209 | 7.958270 | -525.839648 | 4.878850 | -500.162375 | 25.019940 | -499.819438 | . 2 | 2016-09-01 02:59:59 | 215.259946 | 12.884934 | 7.786667 | 0.996043 | 1398.493666 | -500.868360 | 1398.860436 | -499.764529 | 1398.075709 | ... | 11.962366 | -498.767484 | 8.096893 | -500.827423 | 8.071056 | -500.801673 | 4.905125 | -499.828510 | 24.994862 | -500.622559 | . 3 | 2016-09-01 03:59:59 | 215.336236 | 12.006805 | 7.640000 | 0.863514 | 1399.618111 | -498.863574 | 1397.440120 | -499.211024 | 1400.129303 | ... | 12.033091 | -498.350935 | 8.074946 | -499.474407 | 7.897085 | -500.868509 | 4.931400 | -499.963623 | 24.948919 | -498.709987 | . 4 | 2016-09-01 04:59:59 | 199.099327 | 10.682530 | 7.530000 | 0.805575 | 1401.268123 | -500.808305 | 1398.128818 | -499.504543 | 1402.172226 | ... | 12.025367 | -500.786497 | 8.054678 | -500.397500 | 8.107890 | -509.526725 | 4.957674 | -500.360026 | 25.003331 | -500.856333 | . 5 rows × 53 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5554 non-null float64 primary_cleaner.input.depressant 5572 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5690 non-null float64 primary_cleaner.state.floatbank8_a_air 5840 non-null float64 primary_cleaner.state.floatbank8_a_level 5840 non-null float64 primary_cleaner.state.floatbank8_b_air 5840 non-null float64 primary_cleaner.state.floatbank8_b_level 5840 non-null float64 primary_cleaner.state.floatbank8_c_air 5840 non-null float64 primary_cleaner.state.floatbank8_c_level 5840 non-null float64 primary_cleaner.state.floatbank8_d_air 5840 non-null float64 primary_cleaner.state.floatbank8_d_level 5840 non-null float64 rougher.input.feed_ag 5840 non-null float64 rougher.input.feed_pb 5840 non-null float64 rougher.input.feed_rate 5816 non-null float64 rougher.input.feed_size 5834 non-null float64 rougher.input.feed_sol 5789 non-null float64 rougher.input.feed_au 5840 non-null float64 rougher.input.floatbank10_sulfate 5599 non-null float64 rougher.input.floatbank10_xanthate 5733 non-null float64 rougher.input.floatbank11_sulfate 5801 non-null float64 rougher.input.floatbank11_xanthate 5503 non-null float64 rougher.state.floatbank10_a_air 5839 non-null float64 rougher.state.floatbank10_a_level 5840 non-null float64 rougher.state.floatbank10_b_air 5839 non-null float64 rougher.state.floatbank10_b_level 5840 non-null float64 rougher.state.floatbank10_c_air 5839 non-null float64 rougher.state.floatbank10_c_level 5840 non-null float64 rougher.state.floatbank10_d_air 5839 non-null float64 rougher.state.floatbank10_d_level 5840 non-null float64 rougher.state.floatbank10_e_air 5839 non-null float64 rougher.state.floatbank10_e_level 5840 non-null float64 rougher.state.floatbank10_f_air 5839 non-null float64 rougher.state.floatbank10_f_level 5840 non-null float64 secondary_cleaner.state.floatbank2_a_air 5836 non-null float64 secondary_cleaner.state.floatbank2_a_level 5840 non-null float64 secondary_cleaner.state.floatbank2_b_air 5833 non-null float64 secondary_cleaner.state.floatbank2_b_level 5840 non-null float64 secondary_cleaner.state.floatbank3_a_air 5822 non-null float64 secondary_cleaner.state.floatbank3_a_level 5840 non-null float64 secondary_cleaner.state.floatbank3_b_air 5840 non-null float64 secondary_cleaner.state.floatbank3_b_level 5840 non-null float64 secondary_cleaner.state.floatbank4_a_air 5840 non-null float64 secondary_cleaner.state.floatbank4_a_level 5840 non-null float64 secondary_cleaner.state.floatbank4_b_air 5840 non-null float64 secondary_cleaner.state.floatbank4_b_level 5840 non-null float64 secondary_cleaner.state.floatbank5_a_air 5840 non-null float64 secondary_cleaner.state.floatbank5_a_level 5840 non-null float64 secondary_cleaner.state.floatbank5_b_air 5840 non-null float64 secondary_cleaner.state.floatbank5_b_level 5840 non-null float64 secondary_cleaner.state.floatbank6_a_air 5840 non-null float64 secondary_cleaner.state.floatbank6_a_level 5840 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB . None . **************************************************************************************************** Describe: . primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air primary_cleaner.state.floatbank8_c_level ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 5554.000000 | 5572.000000 | 5856.000000 | 5690.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | ... | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | . mean | 170.515243 | 8.482873 | 7.264651 | 1.321420 | 1481.990241 | -509.057796 | 1486.908670 | -511.743956 | 1468.495216 | -509.741212 | ... | 15.636031 | -516.266074 | 13.145702 | -476.338907 | 12.308967 | -512.208126 | 9.470986 | -505.017827 | 16.678722 | -512.351694 | . std | 49.608602 | 3.353105 | 0.611526 | 0.693246 | 310.453166 | 61.339256 | 313.224286 | 67.139074 | 309.980748 | 62.671873 | ... | 4.660835 | 62.756748 | 4.304086 | 105.549424 | 3.762827 | 58.864651 | 3.312471 | 68.785898 | 5.404514 | 69.919839 | . min | 0.000103 | 0.000031 | 5.650000 | 0.000003 | 0.000000 | -799.773788 | 0.000000 | -800.029078 | 0.000000 | -799.995127 | ... | 0.000000 | -799.798523 | 0.000000 | -800.836914 | -0.223393 | -799.661076 | 0.528083 | -800.220337 | -0.079426 | -809.859706 | . 25% | 143.340022 | 6.411500 | 6.885625 | 0.888769 | 1497.190681 | -500.455211 | 1497.150234 | -500.936639 | 1437.050321 | -501.300441 | ... | 12.057838 | -501.054741 | 11.880119 | -500.419113 | 10.123459 | -500.879383 | 7.991208 | -500.223089 | 13.012422 | -500.833821 | . 50% | 176.103893 | 8.023252 | 7.259333 | 1.183362 | 1554.659783 | -499.997402 | 1553.268084 | -500.066588 | 1546.160672 | -500.079537 | ... | 17.001867 | -500.160145 | 14.952102 | -499.644328 | 12.062877 | -500.047621 | 9.980774 | -500.001338 | 16.007242 | -500.041085 | . 75% | 207.240761 | 10.017725 | 7.650000 | 1.763797 | 1601.681656 | -499.575313 | 1601.784707 | -499.323361 | 1600.785573 | -499.009545 | ... | 18.030985 | -499.441529 | 15.940011 | -401.523664 | 15.017881 | -499.297033 | 11.992176 | -499.722835 | 21.009076 | -499.395621 | . max | 274.409626 | 40.024582 | 15.500000 | 5.433169 | 2212.432090 | -57.195404 | 1975.147923 | -142.527229 | 1715.053773 | -150.937035 | ... | 30.051797 | -401.565212 | 31.269706 | -6.506986 | 25.258848 | -244.483566 | 14.090194 | -126.463446 | 26.705889 | -29.093593 | . 8 rows × 52 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . primary_cleaner.input.sulfate | 302 | 5.16 | . primary_cleaner.input.depressant | 284 | 4.85 | . primary_cleaner.input.feed_size | 0 | 0.00 | . primary_cleaner.input.xanthate | 166 | 2.83 | . primary_cleaner.state.floatbank8_a_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_a_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_b_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_b_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_c_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_c_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_d_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_d_level | 16 | 0.27 | . rougher.input.feed_ag | 16 | 0.27 | . rougher.input.feed_pb | 16 | 0.27 | . rougher.input.feed_rate | 40 | 0.68 | . rougher.input.feed_size | 22 | 0.38 | . rougher.input.feed_sol | 67 | 1.14 | . rougher.input.feed_au | 16 | 0.27 | . rougher.input.floatbank10_sulfate | 257 | 4.39 | . rougher.input.floatbank10_xanthate | 123 | 2.10 | . rougher.input.floatbank11_sulfate | 55 | 0.94 | . rougher.input.floatbank11_xanthate | 353 | 6.03 | . rougher.state.floatbank10_a_air | 17 | 0.29 | . rougher.state.floatbank10_a_level | 16 | 0.27 | . rougher.state.floatbank10_b_air | 17 | 0.29 | . rougher.state.floatbank10_b_level | 16 | 0.27 | . rougher.state.floatbank10_c_air | 17 | 0.29 | . rougher.state.floatbank10_c_level | 16 | 0.27 | . rougher.state.floatbank10_d_air | 17 | 0.29 | . rougher.state.floatbank10_d_level | 16 | 0.27 | . rougher.state.floatbank10_e_air | 17 | 0.29 | . rougher.state.floatbank10_e_level | 16 | 0.27 | . rougher.state.floatbank10_f_air | 17 | 0.29 | . rougher.state.floatbank10_f_level | 16 | 0.27 | . secondary_cleaner.state.floatbank2_a_air | 20 | 0.34 | . secondary_cleaner.state.floatbank2_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank2_b_air | 23 | 0.39 | . secondary_cleaner.state.floatbank2_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank3_a_air | 34 | 0.58 | . secondary_cleaner.state.floatbank3_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank3_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank3_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank4_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank4_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank4_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank4_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank5_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank5_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank5_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank5_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank6_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank6_a_level | 16 | 0.27 | . **************************************************************************************************** Shape: . (5856, 53) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . df (soruce data) has 86 columns in total which is the same with the training dataset. Though df has 22716 entries while the training dataset has 16860 entries. The test dataset has 52 columns total and 5856 total entries. There are many missing values and null values that will not be easily replaced or can be filled in. . Check that recovery is calculated correctly . Using the training set, calculate recovery for the rougher.output.recovery feature. Find the MAE between the calculated values and the feature values . def rough_recovery(row): concentrate = row[&#39;rougher.output.concentrate_au&#39;] feed = row[&#39;rougher.input.feed_au&#39;] tail = row[&#39;rougher.output.tail_au&#39;] try: recovery = ((concentrate * (feed - tail)) / (feed * (concentrate - tail))) return recovery * 100 except ZeroDivisionError: return 0 . au_calc_recovery = df_train.apply(rough_recovery, axis=1) display(au_calc_recovery) . 0 87.107763 1 86.843261 2 86.842308 3 87.226430 4 86.688794 ... 16855 89.574376 16856 87.724007 16857 88.890579 16858 89.858126 16859 89.514960 Length: 16860, dtype: float64 . y_true, y_pred = df_train[&#39;rougher.output.recovery&#39;], au_calc_recovery au_recovery = pd.concat([y_true, y_pred], axis=1, sort=False) au_recovery.columns = [&#39;rougher.output.recovery&#39;, &#39;au_calc_recovery&#39;] au_recovery.head(5) . rougher.output.recovery au_calc_recovery . 0 | 87.107763 | 87.107763 | . 1 | 86.843261 | 86.843261 | . 2 | 86.842308 | 86.842308 | . 3 | 87.226430 | 87.226430 | . 4 | 86.688794 | 86.688794 | . def calc_MAE(y_true, y_pred): try: return mean_absolute_error(y_true , y_pred) except ValueError: return 0 . au_recovery[&#39;MAE&#39;] = calc_MAE(au_recovery[&#39;rougher.output.recovery&#39;], au_recovery[&#39;au_calc_recovery&#39;]) display(au_recovery.info()) display(au_recovery) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 3 columns): rougher.output.recovery 14287 non-null float64 au_calc_recovery 14610 non-null float64 MAE 16860 non-null int64 dtypes: float64(2), int64(1) memory usage: 395.3 KB . None . rougher.output.recovery au_calc_recovery MAE . 0 | 87.107763 | 87.107763 | 0 | . 1 | 86.843261 | 86.843261 | 0 | . 2 | 86.842308 | 86.842308 | 0 | . 3 | 87.226430 | 87.226430 | 0 | . 4 | 86.688794 | 86.688794 | 0 | . ... | ... | ... | ... | . 16855 | 89.574376 | 89.574376 | 0 | . 16856 | 87.724007 | 87.724007 | 0 | . 16857 | 88.890579 | 88.890579 | 0 | . 16858 | 89.858126 | 89.858126 | 0 | . 16859 | 89.514960 | 89.514960 | 0 | . 16860 rows × 3 columns . au_recovery[&#39;MAE&#39;].plot.hist(grid=True) plt.title(&quot;Histogram for Variance of MAE calculation&quot;) . Text(0.5, 1.0, &#39;Histogram for Variance of MAE calculation&#39;) . print(&#39;Maximum value on au_recovery MAE calculation:&#39;, au_recovery[&#39;MAE&#39;].max()) print(&#39;Minimum value on au_recovery MAE calculation:&#39;, au_recovery[&#39;MAE&#39;].min()) . Maximum value on au_recovery MAE calculation: 0 Minimum value on au_recovery MAE calculation: 0 . There were missing values for both the calculated recovery and also for the rougher.output.recovery feature. 0 was replaced for those rows where a MAE calculation could not be made. It was discovered that 0 was the only outcome for MAE calculation between the two which indicates that there is no error between the calculated values and the actual values. . Analyze the features not available in the test set . cols_not_test = set(df).difference(set(df_test)) print(&#39;Number of columns missing from test dataset:&#39;, len(cols_not_test)) display(cols_not_test) . Number of columns missing from test dataset: 34 . {&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_au&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.recovery&#39;, &#39;final.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;final.output.tail_pb&#39;, &#39;final.output.tail_sol&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;primary_cleaner.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.recovery&#39;, &#39;rougher.output.tail_ag&#39;, &#39;rougher.output.tail_au&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;secondary_cleaner.output.tail_sol&#39;} . There are 34 columns missing from the test dataset and all the dtype for these columns is float which are output values or calculations. It&#39;s good to note that the test dataset is missing the two target columns (final.output.recovery and rougher.output.recovery). . Preprocessing Data . We can use our formula to calculate the rougher.output.recovery since earlier we&#39;ve determined that the MAE shows 0 bewteen the calculation values and the actual values. This will help fill in the missing gaps in rougher.output.recovery column. . df[&#39;rougher.output.recovery&#39;] = df.apply(rough_recovery, axis=1) df_train[&#39;rougher.output.recovery&#39;] = df_train.apply(rough_recovery, axis=1) . It might be a good idea to fill missing values and null values in the datasets with fillna(method=‘ffill’) since the data ordered by time. . df.fillna(method=&#39;ffill&#39;, inplace=True) display(df.info()) df_test.fillna(method=&#39;ffill&#39;, inplace=True) display(df_test.info()) df_train.fillna(method=&#39;ffill&#39;, inplace=True) df_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22716 non-null float64 final.output.concentrate_pb 22716 non-null float64 final.output.concentrate_sol 22716 non-null float64 final.output.concentrate_au 22716 non-null float64 final.output.recovery 22716 non-null float64 final.output.tail_ag 22716 non-null float64 final.output.tail_pb 22716 non-null float64 final.output.tail_sol 22716 non-null float64 final.output.tail_au 22716 non-null float64 primary_cleaner.input.sulfate 22716 non-null float64 primary_cleaner.input.depressant 22716 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 22716 non-null float64 primary_cleaner.output.concentrate_ag 22716 non-null float64 primary_cleaner.output.concentrate_pb 22716 non-null float64 primary_cleaner.output.concentrate_sol 22716 non-null float64 primary_cleaner.output.concentrate_au 22716 non-null float64 primary_cleaner.output.tail_ag 22716 non-null float64 primary_cleaner.output.tail_pb 22716 non-null float64 primary_cleaner.output.tail_sol 22716 non-null float64 primary_cleaner.output.tail_au 22716 non-null float64 primary_cleaner.state.floatbank8_a_air 22716 non-null float64 primary_cleaner.state.floatbank8_a_level 22716 non-null float64 primary_cleaner.state.floatbank8_b_air 22716 non-null float64 primary_cleaner.state.floatbank8_b_level 22716 non-null float64 primary_cleaner.state.floatbank8_c_air 22716 non-null float64 primary_cleaner.state.floatbank8_c_level 22716 non-null float64 primary_cleaner.state.floatbank8_d_air 22716 non-null float64 primary_cleaner.state.floatbank8_d_level 22716 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22716 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22716 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22716 non-null float64 rougher.calculation.au_pb_ratio 22716 non-null float64 rougher.input.feed_ag 22716 non-null float64 rougher.input.feed_pb 22716 non-null float64 rougher.input.feed_rate 22716 non-null float64 rougher.input.feed_size 22716 non-null float64 rougher.input.feed_sol 22716 non-null float64 rougher.input.feed_au 22716 non-null float64 rougher.input.floatbank10_sulfate 22716 non-null float64 rougher.input.floatbank10_xanthate 22716 non-null float64 rougher.input.floatbank11_sulfate 22716 non-null float64 rougher.input.floatbank11_xanthate 22716 non-null float64 rougher.output.concentrate_ag 22716 non-null float64 rougher.output.concentrate_pb 22716 non-null float64 rougher.output.concentrate_sol 22716 non-null float64 rougher.output.concentrate_au 22716 non-null float64 rougher.output.recovery 22716 non-null float64 rougher.output.tail_ag 22716 non-null float64 rougher.output.tail_pb 22716 non-null float64 rougher.output.tail_sol 22716 non-null float64 rougher.output.tail_au 22716 non-null float64 rougher.state.floatbank10_a_air 22716 non-null float64 rougher.state.floatbank10_a_level 22716 non-null float64 rougher.state.floatbank10_b_air 22716 non-null float64 rougher.state.floatbank10_b_level 22716 non-null float64 rougher.state.floatbank10_c_air 22716 non-null float64 rougher.state.floatbank10_c_level 22716 non-null float64 rougher.state.floatbank10_d_air 22716 non-null float64 rougher.state.floatbank10_d_level 22716 non-null float64 rougher.state.floatbank10_e_air 22716 non-null float64 rougher.state.floatbank10_e_level 22716 non-null float64 rougher.state.floatbank10_f_air 22716 non-null float64 rougher.state.floatbank10_f_level 22716 non-null float64 secondary_cleaner.output.tail_ag 22716 non-null float64 secondary_cleaner.output.tail_pb 22716 non-null float64 secondary_cleaner.output.tail_sol 22716 non-null float64 secondary_cleaner.output.tail_au 22716 non-null float64 secondary_cleaner.state.floatbank2_a_air 22716 non-null float64 secondary_cleaner.state.floatbank2_a_level 22716 non-null float64 secondary_cleaner.state.floatbank2_b_air 22716 non-null float64 secondary_cleaner.state.floatbank2_b_level 22716 non-null float64 secondary_cleaner.state.floatbank3_a_air 22716 non-null float64 secondary_cleaner.state.floatbank3_a_level 22716 non-null float64 secondary_cleaner.state.floatbank3_b_air 22716 non-null float64 secondary_cleaner.state.floatbank3_b_level 22716 non-null float64 secondary_cleaner.state.floatbank4_a_air 22716 non-null float64 secondary_cleaner.state.floatbank4_a_level 22716 non-null float64 secondary_cleaner.state.floatbank4_b_air 22716 non-null float64 secondary_cleaner.state.floatbank4_b_level 22716 non-null float64 secondary_cleaner.state.floatbank5_a_air 22716 non-null float64 secondary_cleaner.state.floatbank5_a_level 22716 non-null float64 secondary_cleaner.state.floatbank5_b_air 22716 non-null float64 secondary_cleaner.state.floatbank5_b_level 22716 non-null float64 secondary_cleaner.state.floatbank6_a_air 22716 non-null float64 secondary_cleaner.state.floatbank6_a_level 22716 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5856 non-null float64 primary_cleaner.input.depressant 5856 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5856 non-null float64 primary_cleaner.state.floatbank8_a_air 5856 non-null float64 primary_cleaner.state.floatbank8_a_level 5856 non-null float64 primary_cleaner.state.floatbank8_b_air 5856 non-null float64 primary_cleaner.state.floatbank8_b_level 5856 non-null float64 primary_cleaner.state.floatbank8_c_air 5856 non-null float64 primary_cleaner.state.floatbank8_c_level 5856 non-null float64 primary_cleaner.state.floatbank8_d_air 5856 non-null float64 primary_cleaner.state.floatbank8_d_level 5856 non-null float64 rougher.input.feed_ag 5856 non-null float64 rougher.input.feed_pb 5856 non-null float64 rougher.input.feed_rate 5856 non-null float64 rougher.input.feed_size 5856 non-null float64 rougher.input.feed_sol 5856 non-null float64 rougher.input.feed_au 5856 non-null float64 rougher.input.floatbank10_sulfate 5856 non-null float64 rougher.input.floatbank10_xanthate 5856 non-null float64 rougher.input.floatbank11_sulfate 5856 non-null float64 rougher.input.floatbank11_xanthate 5856 non-null float64 rougher.state.floatbank10_a_air 5856 non-null float64 rougher.state.floatbank10_a_level 5856 non-null float64 rougher.state.floatbank10_b_air 5856 non-null float64 rougher.state.floatbank10_b_level 5856 non-null float64 rougher.state.floatbank10_c_air 5856 non-null float64 rougher.state.floatbank10_c_level 5856 non-null float64 rougher.state.floatbank10_d_air 5856 non-null float64 rougher.state.floatbank10_d_level 5856 non-null float64 rougher.state.floatbank10_e_air 5856 non-null float64 rougher.state.floatbank10_e_level 5856 non-null float64 rougher.state.floatbank10_f_air 5856 non-null float64 rougher.state.floatbank10_f_level 5856 non-null float64 secondary_cleaner.state.floatbank2_a_air 5856 non-null float64 secondary_cleaner.state.floatbank2_a_level 5856 non-null float64 secondary_cleaner.state.floatbank2_b_air 5856 non-null float64 secondary_cleaner.state.floatbank2_b_level 5856 non-null float64 secondary_cleaner.state.floatbank3_a_air 5856 non-null float64 secondary_cleaner.state.floatbank3_a_level 5856 non-null float64 secondary_cleaner.state.floatbank3_b_air 5856 non-null float64 secondary_cleaner.state.floatbank3_b_level 5856 non-null float64 secondary_cleaner.state.floatbank4_a_air 5856 non-null float64 secondary_cleaner.state.floatbank4_a_level 5856 non-null float64 secondary_cleaner.state.floatbank4_b_air 5856 non-null float64 secondary_cleaner.state.floatbank4_b_level 5856 non-null float64 secondary_cleaner.state.floatbank5_a_air 5856 non-null float64 secondary_cleaner.state.floatbank5_a_level 5856 non-null float64 secondary_cleaner.state.floatbank5_b_air 5856 non-null float64 secondary_cleaner.state.floatbank5_b_level 5856 non-null float64 secondary_cleaner.state.floatbank6_a_air 5856 non-null float64 secondary_cleaner.state.floatbank6_a_level 5856 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16860 non-null float64 final.output.concentrate_pb 16860 non-null float64 final.output.concentrate_sol 16860 non-null float64 final.output.concentrate_au 16860 non-null float64 final.output.recovery 16860 non-null float64 final.output.tail_ag 16860 non-null float64 final.output.tail_pb 16860 non-null float64 final.output.tail_sol 16860 non-null float64 final.output.tail_au 16860 non-null float64 primary_cleaner.input.sulfate 16860 non-null float64 primary_cleaner.input.depressant 16860 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 16860 non-null float64 primary_cleaner.output.concentrate_ag 16860 non-null float64 primary_cleaner.output.concentrate_pb 16860 non-null float64 primary_cleaner.output.concentrate_sol 16860 non-null float64 primary_cleaner.output.concentrate_au 16860 non-null float64 primary_cleaner.output.tail_ag 16860 non-null float64 primary_cleaner.output.tail_pb 16860 non-null float64 primary_cleaner.output.tail_sol 16860 non-null float64 primary_cleaner.output.tail_au 16860 non-null float64 primary_cleaner.state.floatbank8_a_air 16860 non-null float64 primary_cleaner.state.floatbank8_a_level 16860 non-null float64 primary_cleaner.state.floatbank8_b_air 16860 non-null float64 primary_cleaner.state.floatbank8_b_level 16860 non-null float64 primary_cleaner.state.floatbank8_c_air 16860 non-null float64 primary_cleaner.state.floatbank8_c_level 16860 non-null float64 primary_cleaner.state.floatbank8_d_air 16860 non-null float64 primary_cleaner.state.floatbank8_d_level 16860 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16860 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16860 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16860 non-null float64 rougher.calculation.au_pb_ratio 16860 non-null float64 rougher.input.feed_ag 16860 non-null float64 rougher.input.feed_pb 16860 non-null float64 rougher.input.feed_rate 16860 non-null float64 rougher.input.feed_size 16860 non-null float64 rougher.input.feed_sol 16860 non-null float64 rougher.input.feed_au 16860 non-null float64 rougher.input.floatbank10_sulfate 16860 non-null float64 rougher.input.floatbank10_xanthate 16860 non-null float64 rougher.input.floatbank11_sulfate 16860 non-null float64 rougher.input.floatbank11_xanthate 16860 non-null float64 rougher.output.concentrate_ag 16860 non-null float64 rougher.output.concentrate_pb 16860 non-null float64 rougher.output.concentrate_sol 16860 non-null float64 rougher.output.concentrate_au 16860 non-null float64 rougher.output.recovery 16860 non-null float64 rougher.output.tail_ag 16860 non-null float64 rougher.output.tail_pb 16860 non-null float64 rougher.output.tail_sol 16860 non-null float64 rougher.output.tail_au 16860 non-null float64 rougher.state.floatbank10_a_air 16860 non-null float64 rougher.state.floatbank10_a_level 16860 non-null float64 rougher.state.floatbank10_b_air 16860 non-null float64 rougher.state.floatbank10_b_level 16860 non-null float64 rougher.state.floatbank10_c_air 16860 non-null float64 rougher.state.floatbank10_c_level 16860 non-null float64 rougher.state.floatbank10_d_air 16860 non-null float64 rougher.state.floatbank10_d_level 16860 non-null float64 rougher.state.floatbank10_e_air 16860 non-null float64 rougher.state.floatbank10_e_level 16860 non-null float64 rougher.state.floatbank10_f_air 16860 non-null float64 rougher.state.floatbank10_f_level 16860 non-null float64 secondary_cleaner.output.tail_ag 16860 non-null float64 secondary_cleaner.output.tail_pb 16860 non-null float64 secondary_cleaner.output.tail_sol 16860 non-null float64 secondary_cleaner.output.tail_au 16860 non-null float64 secondary_cleaner.state.floatbank2_a_air 16860 non-null float64 secondary_cleaner.state.floatbank2_a_level 16860 non-null float64 secondary_cleaner.state.floatbank2_b_air 16860 non-null float64 secondary_cleaner.state.floatbank2_b_level 16860 non-null float64 secondary_cleaner.state.floatbank3_a_air 16860 non-null float64 secondary_cleaner.state.floatbank3_a_level 16860 non-null float64 secondary_cleaner.state.floatbank3_b_air 16860 non-null float64 secondary_cleaner.state.floatbank3_b_level 16860 non-null float64 secondary_cleaner.state.floatbank4_a_air 16860 non-null float64 secondary_cleaner.state.floatbank4_a_level 16860 non-null float64 secondary_cleaner.state.floatbank4_b_air 16860 non-null float64 secondary_cleaner.state.floatbank4_b_level 16860 non-null float64 secondary_cleaner.state.floatbank5_a_air 16860 non-null float64 secondary_cleaner.state.floatbank5_a_level 16860 non-null float64 secondary_cleaner.state.floatbank5_b_air 16860 non-null float64 secondary_cleaner.state.floatbank5_b_level 16860 non-null float64 secondary_cleaner.state.floatbank6_a_air 16860 non-null float64 secondary_cleaner.state.floatbank6_a_level 16860 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB . . We were able to fill in some missing values with our calculation of the rougher.output.recover for both training and test datasets as well as fill in the rest of the missing and null values using the fillna(method=‘ffill’) since the data ordered by time (it fills in null values with the previous data from the instance). . Visualize . Concentrations of metals (Au, Ag, Pb) at each different purification stage . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;final [Au]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Au]&#39;) plt.hist(df[&#39;rougher.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;rougher [Au]&#39;) plt.hist(df[&#39;rougher.input.feed_au&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Au]&#39;) plt.xlabel(&#39;Concentrations of Au at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Au&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 55) . . (0, 55) . Au (gold) concentrations get more concentrated with each stage of purification which is to be expected as the impurities are removed with each stage and the target being gold recovery that concentrations of Au would be greatest at the final stage. . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;final [Ag]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Ag]&#39;) plt.hist(df[&#39;rougher.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;rougher [Ag]&#39;) plt.hist(df[&#39;rougher.input.feed_ag&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Ag]&#39;) plt.xlabel(&#39;Concentrations of Ag at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Ag&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 22) . . (0, 22) . Concentrations for Ag (silver) decrease with each stage of purification which is to be expected as Ag is considered a impurity in the target gold recovery. . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;final [Pb]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Pb]&#39;) plt.hist(df[&#39;rougher.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;rougher [Pb]&#39;) plt.hist(df[&#39;rougher.input.feed_pb&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Pb]&#39;) plt.xlabel(&#39;Concentrations of Pb at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Pb&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 17) . . (0, 17) . Interesting with Pb concentrations (lead) seems to have slightly increase with each stage of purification. This might be due to how the process of gold recovery is done where the solute might be not be able to filter out lead as easily as other impurities. Though the concentration really is not nearly as concentrated in comparison to gold. . There appears to be outliers where concentraions of metals are 0. . Compare the feed particle size distributions in the training set and in the test set . If the distributions vary significantly, the model evaluation will be incorrect. . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_au&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_au&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Au&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Au from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_au&#39;, &#39;test rougher.input.feed_au&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_ag&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_ag&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Ag&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Ag from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_ag&#39;, &#39;test rougher.input.feed_ag&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_pb&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_pb&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Pb&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Pb from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_pb&#39;, &#39;test rougher.input.feed_pb&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_sol&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_sol&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of sol&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of sol from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_sol&#39;, &#39;test rougher.input.feed_sol&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_size&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_size&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed size&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed size from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_size&#39;, &#39;test rougher.input.feed_size&#39;],loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(-10, 175) . . (-10, 175) . From the plots we can see that the particle size feed for each Au, Ag, Pb, sol, and feed size from train and test datasets are fairly similar. . Total concentrations of all substances at different stages: raw feed, rougher concentrate, and final concentrate . #collapse-hide def total_rough_feed(df): au = df[&#39;rougher.input.feed_au&#39;] ag = df[&#39;rougher.input.feed_ag&#39;] pb = df[&#39;rougher.input.feed_pb&#39;] sol = df[&#39;rougher.input.feed_sol&#39;] total = au + ag + pb + sol return total def total_rough_conc(df): au = df[&#39;rougher.output.concentrate_au&#39;] ag = df[&#39;rougher.output.concentrate_ag&#39;] pb = df[&#39;rougher.output.concentrate_pb&#39;] sol = df[&#39;rougher.output.concentrate_sol&#39;] total = au + ag + pb + sol return total def total_final_conc(df): au = df[&#39;final.output.concentrate_au&#39;] ag = df[&#39;final.output.concentrate_ag&#39;] pb = df[&#39;final.output.concentrate_pb&#39;] sol = df[&#39;final.output.concentrate_sol&#39;] total = au + ag + pb + sol return total . . df[&#39;total_rough_feed&#39;] = total_rough_feed(df) df[&#39;total_rough_conc&#39;] = total_rough_conc(df) df[&#39;total_final_conc&#39;] = total_final_conc(df) df[[&#39;date&#39;,&#39;total_rough_feed&#39;,&#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]] . date total_rough_feed total_rough_conc total_final_conc . 0 | 2016-01-15 00:00:00 | 51.680034 | 66.424950 | 63.644396 | . 1 | 2016-01-15 01:00:00 | 50.659114 | 67.012710 | 63.957723 | . 2 | 2016-01-15 02:00:00 | 50.609929 | 66.103793 | 64.311180 | . 3 | 2016-01-15 03:00:00 | 51.061546 | 65.752751 | 63.573449 | . 4 | 2016-01-15 04:00:00 | 47.859163 | 65.908382 | 64.004667 | . ... | ... | ... | ... | ... | . 22711 | 2018-08-18 06:59:59 | 53.415050 | 70.781325 | 68.098589 | . 22712 | 2018-08-18 07:59:59 | 53.696482 | 70.539603 | 68.274362 | . 22713 | 2018-08-18 08:59:59 | 54.589604 | 55.376330 | 68.226068 | . 22714 | 2018-08-18 09:59:59 | 54.027355 | 69.201689 | 68.200449 | . 22715 | 2018-08-18 10:59:59 | 53.535054 | 69.544003 | 68.353154 | . 22716 rows × 4 columns . plt.figure(figsize=(10,5)) plt.hist(df[&#39;total_rough_feed&#39;], bins=100, alpha=0.5, label=&#39;total_rough_feed&#39;) plt.hist(df[&#39;total_rough_conc&#39;], bins=100, alpha=0.5, label=&#39;total rough []&#39;) plt.hist(df[&#39;total_final_conc&#39;], bins=100, alpha=0.5, label=&#39;total final []&#39;) plt.xlabel(&#39;Total concentrations of elements at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for total concentrations of elements&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) . . From the histogram of total concentrations of all elements the obvious abnormal values are the ones that linger around 0. It&#39;s best to drop these values moving forward. It is ideal to remove these values from both test and train datasets because if we only remove from train, the model will be shocked with such examples on test in comparison. . df = df[df[&#39;total_rough_feed&#39;] &gt; 1] df = df[df[&#39;total_rough_conc&#39;] &gt; 1] df = df[df[&#39;total_final_conc&#39;] &gt; 1] . # Merging columns &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; from df to aid in removing abnormal values df_train_clean = pd.merge(df_train, df[[&#39;date&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_train_clean = df_train_clean.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) df_train_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14612 entries, 0 to 16859 Data columns (total 90 columns): date 14612 non-null object final.output.concentrate_ag 14612 non-null float64 final.output.concentrate_pb 14612 non-null float64 final.output.concentrate_sol 14612 non-null float64 final.output.concentrate_au 14612 non-null float64 final.output.recovery 14612 non-null float64 final.output.tail_ag 14612 non-null float64 final.output.tail_pb 14612 non-null float64 final.output.tail_sol 14612 non-null float64 final.output.tail_au 14612 non-null float64 primary_cleaner.input.sulfate 14612 non-null float64 primary_cleaner.input.depressant 14612 non-null float64 primary_cleaner.input.feed_size 14612 non-null float64 primary_cleaner.input.xanthate 14612 non-null float64 primary_cleaner.output.concentrate_ag 14612 non-null float64 primary_cleaner.output.concentrate_pb 14612 non-null float64 primary_cleaner.output.concentrate_sol 14612 non-null float64 primary_cleaner.output.concentrate_au 14612 non-null float64 primary_cleaner.output.tail_ag 14612 non-null float64 primary_cleaner.output.tail_pb 14612 non-null float64 primary_cleaner.output.tail_sol 14612 non-null float64 primary_cleaner.output.tail_au 14612 non-null float64 primary_cleaner.state.floatbank8_a_air 14612 non-null float64 primary_cleaner.state.floatbank8_a_level 14612 non-null float64 primary_cleaner.state.floatbank8_b_air 14612 non-null float64 primary_cleaner.state.floatbank8_b_level 14612 non-null float64 primary_cleaner.state.floatbank8_c_air 14612 non-null float64 primary_cleaner.state.floatbank8_c_level 14612 non-null float64 primary_cleaner.state.floatbank8_d_air 14612 non-null float64 primary_cleaner.state.floatbank8_d_level 14612 non-null float64 rougher.calculation.sulfate_to_au_concentrate 14612 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 14612 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 14612 non-null float64 rougher.calculation.au_pb_ratio 14612 non-null float64 rougher.input.feed_ag 14612 non-null float64 rougher.input.feed_pb 14612 non-null float64 rougher.input.feed_rate 14612 non-null float64 rougher.input.feed_size 14612 non-null float64 rougher.input.feed_sol 14612 non-null float64 rougher.input.feed_au 14612 non-null float64 rougher.input.floatbank10_sulfate 14612 non-null float64 rougher.input.floatbank10_xanthate 14612 non-null float64 rougher.input.floatbank11_sulfate 14612 non-null float64 rougher.input.floatbank11_xanthate 14612 non-null float64 rougher.output.concentrate_ag 14612 non-null float64 rougher.output.concentrate_pb 14612 non-null float64 rougher.output.concentrate_sol 14612 non-null float64 rougher.output.concentrate_au 14612 non-null float64 rougher.output.recovery 14612 non-null float64 rougher.output.tail_ag 14612 non-null float64 rougher.output.tail_pb 14612 non-null float64 rougher.output.tail_sol 14612 non-null float64 rougher.output.tail_au 14612 non-null float64 rougher.state.floatbank10_a_air 14612 non-null float64 rougher.state.floatbank10_a_level 14612 non-null float64 rougher.state.floatbank10_b_air 14612 non-null float64 rougher.state.floatbank10_b_level 14612 non-null float64 rougher.state.floatbank10_c_air 14612 non-null float64 rougher.state.floatbank10_c_level 14612 non-null float64 rougher.state.floatbank10_d_air 14612 non-null float64 rougher.state.floatbank10_d_level 14612 non-null float64 rougher.state.floatbank10_e_air 14612 non-null float64 rougher.state.floatbank10_e_level 14612 non-null float64 rougher.state.floatbank10_f_air 14612 non-null float64 rougher.state.floatbank10_f_level 14612 non-null float64 secondary_cleaner.output.tail_ag 14612 non-null float64 secondary_cleaner.output.tail_pb 14612 non-null float64 secondary_cleaner.output.tail_sol 14612 non-null float64 secondary_cleaner.output.tail_au 14612 non-null float64 secondary_cleaner.state.floatbank2_a_air 14612 non-null float64 secondary_cleaner.state.floatbank2_a_level 14612 non-null float64 secondary_cleaner.state.floatbank2_b_air 14612 non-null float64 secondary_cleaner.state.floatbank2_b_level 14612 non-null float64 secondary_cleaner.state.floatbank3_a_air 14612 non-null float64 secondary_cleaner.state.floatbank3_a_level 14612 non-null float64 secondary_cleaner.state.floatbank3_b_air 14612 non-null float64 secondary_cleaner.state.floatbank3_b_level 14612 non-null float64 secondary_cleaner.state.floatbank4_a_air 14612 non-null float64 secondary_cleaner.state.floatbank4_a_level 14612 non-null float64 secondary_cleaner.state.floatbank4_b_air 14612 non-null float64 secondary_cleaner.state.floatbank4_b_level 14612 non-null float64 secondary_cleaner.state.floatbank5_a_air 14612 non-null float64 secondary_cleaner.state.floatbank5_a_level 14612 non-null float64 secondary_cleaner.state.floatbank5_b_air 14612 non-null float64 secondary_cleaner.state.floatbank5_b_level 14612 non-null float64 secondary_cleaner.state.floatbank6_a_air 14612 non-null float64 secondary_cleaner.state.floatbank6_a_level 14612 non-null float64 total_rough_feed 14612 non-null float64 total_rough_conc 14612 non-null float64 total_final_conc 14612 non-null float64 dtypes: float64(89), object(1) memory usage: 10.1+ MB . . # Merging columns &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; from df to aid in removing abnormal values df_test_clean = pd.merge(df_test, df[[&#39;date&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_test_clean = df_test_clean.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) df_test_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5290 entries, 0 to 5855 Data columns (total 56 columns): date 5290 non-null object primary_cleaner.input.sulfate 5290 non-null float64 primary_cleaner.input.depressant 5290 non-null float64 primary_cleaner.input.feed_size 5290 non-null float64 primary_cleaner.input.xanthate 5290 non-null float64 primary_cleaner.state.floatbank8_a_air 5290 non-null float64 primary_cleaner.state.floatbank8_a_level 5290 non-null float64 primary_cleaner.state.floatbank8_b_air 5290 non-null float64 primary_cleaner.state.floatbank8_b_level 5290 non-null float64 primary_cleaner.state.floatbank8_c_air 5290 non-null float64 primary_cleaner.state.floatbank8_c_level 5290 non-null float64 primary_cleaner.state.floatbank8_d_air 5290 non-null float64 primary_cleaner.state.floatbank8_d_level 5290 non-null float64 rougher.input.feed_ag 5290 non-null float64 rougher.input.feed_pb 5290 non-null float64 rougher.input.feed_rate 5290 non-null float64 rougher.input.feed_size 5290 non-null float64 rougher.input.feed_sol 5290 non-null float64 rougher.input.feed_au 5290 non-null float64 rougher.input.floatbank10_sulfate 5290 non-null float64 rougher.input.floatbank10_xanthate 5290 non-null float64 rougher.input.floatbank11_sulfate 5290 non-null float64 rougher.input.floatbank11_xanthate 5290 non-null float64 rougher.state.floatbank10_a_air 5290 non-null float64 rougher.state.floatbank10_a_level 5290 non-null float64 rougher.state.floatbank10_b_air 5290 non-null float64 rougher.state.floatbank10_b_level 5290 non-null float64 rougher.state.floatbank10_c_air 5290 non-null float64 rougher.state.floatbank10_c_level 5290 non-null float64 rougher.state.floatbank10_d_air 5290 non-null float64 rougher.state.floatbank10_d_level 5290 non-null float64 rougher.state.floatbank10_e_air 5290 non-null float64 rougher.state.floatbank10_e_level 5290 non-null float64 rougher.state.floatbank10_f_air 5290 non-null float64 rougher.state.floatbank10_f_level 5290 non-null float64 secondary_cleaner.state.floatbank2_a_air 5290 non-null float64 secondary_cleaner.state.floatbank2_a_level 5290 non-null float64 secondary_cleaner.state.floatbank2_b_air 5290 non-null float64 secondary_cleaner.state.floatbank2_b_level 5290 non-null float64 secondary_cleaner.state.floatbank3_a_air 5290 non-null float64 secondary_cleaner.state.floatbank3_a_level 5290 non-null float64 secondary_cleaner.state.floatbank3_b_air 5290 non-null float64 secondary_cleaner.state.floatbank3_b_level 5290 non-null float64 secondary_cleaner.state.floatbank4_a_air 5290 non-null float64 secondary_cleaner.state.floatbank4_a_level 5290 non-null float64 secondary_cleaner.state.floatbank4_b_air 5290 non-null float64 secondary_cleaner.state.floatbank4_b_level 5290 non-null float64 secondary_cleaner.state.floatbank5_a_air 5290 non-null float64 secondary_cleaner.state.floatbank5_a_level 5290 non-null float64 secondary_cleaner.state.floatbank5_b_air 5290 non-null float64 secondary_cleaner.state.floatbank5_b_level 5290 non-null float64 secondary_cleaner.state.floatbank6_a_air 5290 non-null float64 secondary_cleaner.state.floatbank6_a_level 5290 non-null float64 total_rough_feed 5290 non-null float64 total_rough_conc 5290 non-null float64 total_final_conc 5290 non-null float64 dtypes: float64(55), object(1) memory usage: 2.3+ MB . . # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_clean = df.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) display(df.info()) df_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 19902 entries, 0 to 22715 Data columns (total 90 columns): date 19902 non-null object final.output.concentrate_ag 19902 non-null float64 final.output.concentrate_pb 19902 non-null float64 final.output.concentrate_sol 19902 non-null float64 final.output.concentrate_au 19902 non-null float64 final.output.recovery 19902 non-null float64 final.output.tail_ag 19902 non-null float64 final.output.tail_pb 19902 non-null float64 final.output.tail_sol 19902 non-null float64 final.output.tail_au 19902 non-null float64 primary_cleaner.input.sulfate 19902 non-null float64 primary_cleaner.input.depressant 19902 non-null float64 primary_cleaner.input.feed_size 19902 non-null float64 primary_cleaner.input.xanthate 19902 non-null float64 primary_cleaner.output.concentrate_ag 19902 non-null float64 primary_cleaner.output.concentrate_pb 19902 non-null float64 primary_cleaner.output.concentrate_sol 19902 non-null float64 primary_cleaner.output.concentrate_au 19902 non-null float64 primary_cleaner.output.tail_ag 19902 non-null float64 primary_cleaner.output.tail_pb 19902 non-null float64 primary_cleaner.output.tail_sol 19902 non-null float64 primary_cleaner.output.tail_au 19902 non-null float64 primary_cleaner.state.floatbank8_a_air 19902 non-null float64 primary_cleaner.state.floatbank8_a_level 19902 non-null float64 primary_cleaner.state.floatbank8_b_air 19902 non-null float64 primary_cleaner.state.floatbank8_b_level 19902 non-null float64 primary_cleaner.state.floatbank8_c_air 19902 non-null float64 primary_cleaner.state.floatbank8_c_level 19902 non-null float64 primary_cleaner.state.floatbank8_d_air 19902 non-null float64 primary_cleaner.state.floatbank8_d_level 19902 non-null float64 rougher.calculation.sulfate_to_au_concentrate 19902 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.au_pb_ratio 19902 non-null float64 rougher.input.feed_ag 19902 non-null float64 rougher.input.feed_pb 19902 non-null float64 rougher.input.feed_rate 19902 non-null float64 rougher.input.feed_size 19902 non-null float64 rougher.input.feed_sol 19902 non-null float64 rougher.input.feed_au 19902 non-null float64 rougher.input.floatbank10_sulfate 19902 non-null float64 rougher.input.floatbank10_xanthate 19902 non-null float64 rougher.input.floatbank11_sulfate 19902 non-null float64 rougher.input.floatbank11_xanthate 19902 non-null float64 rougher.output.concentrate_ag 19902 non-null float64 rougher.output.concentrate_pb 19902 non-null float64 rougher.output.concentrate_sol 19902 non-null float64 rougher.output.concentrate_au 19902 non-null float64 rougher.output.recovery 19902 non-null float64 rougher.output.tail_ag 19902 non-null float64 rougher.output.tail_pb 19902 non-null float64 rougher.output.tail_sol 19902 non-null float64 rougher.output.tail_au 19902 non-null float64 rougher.state.floatbank10_a_air 19902 non-null float64 rougher.state.floatbank10_a_level 19902 non-null float64 rougher.state.floatbank10_b_air 19902 non-null float64 rougher.state.floatbank10_b_level 19902 non-null float64 rougher.state.floatbank10_c_air 19902 non-null float64 rougher.state.floatbank10_c_level 19902 non-null float64 rougher.state.floatbank10_d_air 19902 non-null float64 rougher.state.floatbank10_d_level 19902 non-null float64 rougher.state.floatbank10_e_air 19902 non-null float64 rougher.state.floatbank10_e_level 19902 non-null float64 rougher.state.floatbank10_f_air 19902 non-null float64 rougher.state.floatbank10_f_level 19902 non-null float64 secondary_cleaner.output.tail_ag 19902 non-null float64 secondary_cleaner.output.tail_pb 19902 non-null float64 secondary_cleaner.output.tail_sol 19902 non-null float64 secondary_cleaner.output.tail_au 19902 non-null float64 secondary_cleaner.state.floatbank2_a_air 19902 non-null float64 secondary_cleaner.state.floatbank2_a_level 19902 non-null float64 secondary_cleaner.state.floatbank2_b_air 19902 non-null float64 secondary_cleaner.state.floatbank2_b_level 19902 non-null float64 secondary_cleaner.state.floatbank3_a_air 19902 non-null float64 secondary_cleaner.state.floatbank3_a_level 19902 non-null float64 secondary_cleaner.state.floatbank3_b_air 19902 non-null float64 secondary_cleaner.state.floatbank3_b_level 19902 non-null float64 secondary_cleaner.state.floatbank4_a_air 19902 non-null float64 secondary_cleaner.state.floatbank4_a_level 19902 non-null float64 secondary_cleaner.state.floatbank4_b_air 19902 non-null float64 secondary_cleaner.state.floatbank4_b_level 19902 non-null float64 secondary_cleaner.state.floatbank5_a_air 19902 non-null float64 secondary_cleaner.state.floatbank5_a_level 19902 non-null float64 secondary_cleaner.state.floatbank5_b_air 19902 non-null float64 secondary_cleaner.state.floatbank5_b_level 19902 non-null float64 secondary_cleaner.state.floatbank6_a_air 19902 non-null float64 secondary_cleaner.state.floatbank6_a_level 19902 non-null float64 total_rough_feed 19902 non-null float64 total_rough_conc 19902 non-null float64 total_final_conc 19902 non-null float64 dtypes: float64(89), object(1) memory usage: 13.8+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 19902 entries, 0 to 22715 Data columns (total 90 columns): date 19902 non-null object final.output.concentrate_ag 19902 non-null float64 final.output.concentrate_pb 19902 non-null float64 final.output.concentrate_sol 19902 non-null float64 final.output.concentrate_au 19902 non-null float64 final.output.recovery 19902 non-null float64 final.output.tail_ag 19902 non-null float64 final.output.tail_pb 19902 non-null float64 final.output.tail_sol 19902 non-null float64 final.output.tail_au 19902 non-null float64 primary_cleaner.input.sulfate 19902 non-null float64 primary_cleaner.input.depressant 19902 non-null float64 primary_cleaner.input.feed_size 19902 non-null float64 primary_cleaner.input.xanthate 19902 non-null float64 primary_cleaner.output.concentrate_ag 19902 non-null float64 primary_cleaner.output.concentrate_pb 19902 non-null float64 primary_cleaner.output.concentrate_sol 19902 non-null float64 primary_cleaner.output.concentrate_au 19902 non-null float64 primary_cleaner.output.tail_ag 19902 non-null float64 primary_cleaner.output.tail_pb 19902 non-null float64 primary_cleaner.output.tail_sol 19902 non-null float64 primary_cleaner.output.tail_au 19902 non-null float64 primary_cleaner.state.floatbank8_a_air 19902 non-null float64 primary_cleaner.state.floatbank8_a_level 19902 non-null float64 primary_cleaner.state.floatbank8_b_air 19902 non-null float64 primary_cleaner.state.floatbank8_b_level 19902 non-null float64 primary_cleaner.state.floatbank8_c_air 19902 non-null float64 primary_cleaner.state.floatbank8_c_level 19902 non-null float64 primary_cleaner.state.floatbank8_d_air 19902 non-null float64 primary_cleaner.state.floatbank8_d_level 19902 non-null float64 rougher.calculation.sulfate_to_au_concentrate 19902 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.au_pb_ratio 19902 non-null float64 rougher.input.feed_ag 19902 non-null float64 rougher.input.feed_pb 19902 non-null float64 rougher.input.feed_rate 19902 non-null float64 rougher.input.feed_size 19902 non-null float64 rougher.input.feed_sol 19902 non-null float64 rougher.input.feed_au 19902 non-null float64 rougher.input.floatbank10_sulfate 19902 non-null float64 rougher.input.floatbank10_xanthate 19902 non-null float64 rougher.input.floatbank11_sulfate 19902 non-null float64 rougher.input.floatbank11_xanthate 19902 non-null float64 rougher.output.concentrate_ag 19902 non-null float64 rougher.output.concentrate_pb 19902 non-null float64 rougher.output.concentrate_sol 19902 non-null float64 rougher.output.concentrate_au 19902 non-null float64 rougher.output.recovery 19902 non-null float64 rougher.output.tail_ag 19902 non-null float64 rougher.output.tail_pb 19902 non-null float64 rougher.output.tail_sol 19902 non-null float64 rougher.output.tail_au 19902 non-null float64 rougher.state.floatbank10_a_air 19902 non-null float64 rougher.state.floatbank10_a_level 19902 non-null float64 rougher.state.floatbank10_b_air 19902 non-null float64 rougher.state.floatbank10_b_level 19902 non-null float64 rougher.state.floatbank10_c_air 19902 non-null float64 rougher.state.floatbank10_c_level 19902 non-null float64 rougher.state.floatbank10_d_air 19902 non-null float64 rougher.state.floatbank10_d_level 19902 non-null float64 rougher.state.floatbank10_e_air 19902 non-null float64 rougher.state.floatbank10_e_level 19902 non-null float64 rougher.state.floatbank10_f_air 19902 non-null float64 rougher.state.floatbank10_f_level 19902 non-null float64 secondary_cleaner.output.tail_ag 19902 non-null float64 secondary_cleaner.output.tail_pb 19902 non-null float64 secondary_cleaner.output.tail_sol 19902 non-null float64 secondary_cleaner.output.tail_au 19902 non-null float64 secondary_cleaner.state.floatbank2_a_air 19902 non-null float64 secondary_cleaner.state.floatbank2_a_level 19902 non-null float64 secondary_cleaner.state.floatbank2_b_air 19902 non-null float64 secondary_cleaner.state.floatbank2_b_level 19902 non-null float64 secondary_cleaner.state.floatbank3_a_air 19902 non-null float64 secondary_cleaner.state.floatbank3_a_level 19902 non-null float64 secondary_cleaner.state.floatbank3_b_air 19902 non-null float64 secondary_cleaner.state.floatbank3_b_level 19902 non-null float64 secondary_cleaner.state.floatbank4_a_air 19902 non-null float64 secondary_cleaner.state.floatbank4_a_level 19902 non-null float64 secondary_cleaner.state.floatbank4_b_air 19902 non-null float64 secondary_cleaner.state.floatbank4_b_level 19902 non-null float64 secondary_cleaner.state.floatbank5_a_air 19902 non-null float64 secondary_cleaner.state.floatbank5_a_level 19902 non-null float64 secondary_cleaner.state.floatbank5_b_air 19902 non-null float64 secondary_cleaner.state.floatbank5_b_level 19902 non-null float64 secondary_cleaner.state.floatbank6_a_air 19902 non-null float64 secondary_cleaner.state.floatbank6_a_level 19902 non-null float64 total_rough_feed 19902 non-null float64 total_rough_conc 19902 non-null float64 total_final_conc 19902 non-null float64 dtypes: float64(89), object(1) memory usage: 13.8+ MB . . plt.figure(figsize=(10,5)) plt.hist(df_clean[&#39;total_rough_feed&#39;], bins=100, alpha=0.5, label=&#39;total_rough_feed&#39;) plt.hist(df_clean[&#39;total_rough_conc&#39;], bins=100, alpha=0.5, label=&#39;total rough []&#39;) plt.hist(df_clean[&#39;total_final_conc&#39;], bins=100, alpha=0.5, label=&#39;total final []&#39;) plt.xlabel(&#39;Total concentrations of elements at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for total concentrations of elements&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) . . We were able to successfully drop most of abnormal values from our distrubtion from the full dataset, train dataset, and test dataset. . Model Building . Evaluation Metrics . Write a function to calculate the final sMAPE value . def calc_smape(target, prediction): try: smape = 100/len(target) * np.abs(np.sum(2 * np.abs(prediction - target) / (np.abs(target) + np.abs(prediction)))) return smape except ZeroDivisionError: return 0 def calc_f_smape(target, prediction): r_smape = calc_smape(target[0], prediction[0]) f_smape = calc_smape(target[1], prediction[1]) final_smape = (0.25 * r_smape) + (0.75 * f_smape) return final_smape . smape_scorer = make_scorer(calc_f_smape, greater_is_better=False) smape_scorer . make_scorer(calc_f_smape, greater_is_better=False) . Parameter greater_is_better=False is used so further algorithms could understand that we want to minimize the score. . Prepare data subsets . # Dropping columns missing in original test dataset from train dataset and any other unnecessary columns df_train_complete = df_train_clean.drop([&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_au&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;final.output.tail_pb&#39;, &#39;final.output.tail_sol&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;primary_cleaner.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.tail_ag&#39;, &#39;rougher.output.tail_au&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;secondary_cleaner.output.tail_sol&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;, &#39;date&#39;], axis=1) df_train_complete.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14612 entries, 0 to 16859 Data columns (total 54 columns): final.output.recovery 14612 non-null float64 primary_cleaner.input.sulfate 14612 non-null float64 primary_cleaner.input.depressant 14612 non-null float64 primary_cleaner.input.feed_size 14612 non-null float64 primary_cleaner.input.xanthate 14612 non-null float64 primary_cleaner.state.floatbank8_a_air 14612 non-null float64 primary_cleaner.state.floatbank8_a_level 14612 non-null float64 primary_cleaner.state.floatbank8_b_air 14612 non-null float64 primary_cleaner.state.floatbank8_b_level 14612 non-null float64 primary_cleaner.state.floatbank8_c_air 14612 non-null float64 primary_cleaner.state.floatbank8_c_level 14612 non-null float64 primary_cleaner.state.floatbank8_d_air 14612 non-null float64 primary_cleaner.state.floatbank8_d_level 14612 non-null float64 rougher.input.feed_ag 14612 non-null float64 rougher.input.feed_pb 14612 non-null float64 rougher.input.feed_rate 14612 non-null float64 rougher.input.feed_size 14612 non-null float64 rougher.input.feed_sol 14612 non-null float64 rougher.input.feed_au 14612 non-null float64 rougher.input.floatbank10_sulfate 14612 non-null float64 rougher.input.floatbank10_xanthate 14612 non-null float64 rougher.input.floatbank11_sulfate 14612 non-null float64 rougher.input.floatbank11_xanthate 14612 non-null float64 rougher.output.recovery 14612 non-null float64 rougher.state.floatbank10_a_air 14612 non-null float64 rougher.state.floatbank10_a_level 14612 non-null float64 rougher.state.floatbank10_b_air 14612 non-null float64 rougher.state.floatbank10_b_level 14612 non-null float64 rougher.state.floatbank10_c_air 14612 non-null float64 rougher.state.floatbank10_c_level 14612 non-null float64 rougher.state.floatbank10_d_air 14612 non-null float64 rougher.state.floatbank10_d_level 14612 non-null float64 rougher.state.floatbank10_e_air 14612 non-null float64 rougher.state.floatbank10_e_level 14612 non-null float64 rougher.state.floatbank10_f_air 14612 non-null float64 rougher.state.floatbank10_f_level 14612 non-null float64 secondary_cleaner.state.floatbank2_a_air 14612 non-null float64 secondary_cleaner.state.floatbank2_a_level 14612 non-null float64 secondary_cleaner.state.floatbank2_b_air 14612 non-null float64 secondary_cleaner.state.floatbank2_b_level 14612 non-null float64 secondary_cleaner.state.floatbank3_a_air 14612 non-null float64 secondary_cleaner.state.floatbank3_a_level 14612 non-null float64 secondary_cleaner.state.floatbank3_b_air 14612 non-null float64 secondary_cleaner.state.floatbank3_b_level 14612 non-null float64 secondary_cleaner.state.floatbank4_a_air 14612 non-null float64 secondary_cleaner.state.floatbank4_a_level 14612 non-null float64 secondary_cleaner.state.floatbank4_b_air 14612 non-null float64 secondary_cleaner.state.floatbank4_b_level 14612 non-null float64 secondary_cleaner.state.floatbank5_a_air 14612 non-null float64 secondary_cleaner.state.floatbank5_a_level 14612 non-null float64 secondary_cleaner.state.floatbank5_b_air 14612 non-null float64 secondary_cleaner.state.floatbank5_b_level 14612 non-null float64 secondary_cleaner.state.floatbank6_a_air 14612 non-null float64 secondary_cleaner.state.floatbank6_a_level 14612 non-null float64 dtypes: float64(54) memory usage: 6.1 MB . . We removed the columns that were not present in the test dataset from training. These additional columns can confuse the model if they are useless (since they were not present in the original test dataset). If several features were added with random values to any model it will likely reduce the quality. That’s why it’s important to use only proper features. This is why we are prepping the train dataset for the model training. . # Getting columns final.output.recovery and rougher.output.recovery for test dataset and removing &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns df_test_complete = pd.merge(df_test_clean, df[[&#39;date&#39;, &#39;final.output.recovery&#39;, &#39;rougher.output.recovery&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) df_test_complete = df_test_complete.drop([&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;, &#39;date&#39;], axis=1) df_test_complete.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5290 entries, 0 to 5289 Data columns (total 54 columns): primary_cleaner.input.sulfate 5290 non-null float64 primary_cleaner.input.depressant 5290 non-null float64 primary_cleaner.input.feed_size 5290 non-null float64 primary_cleaner.input.xanthate 5290 non-null float64 primary_cleaner.state.floatbank8_a_air 5290 non-null float64 primary_cleaner.state.floatbank8_a_level 5290 non-null float64 primary_cleaner.state.floatbank8_b_air 5290 non-null float64 primary_cleaner.state.floatbank8_b_level 5290 non-null float64 primary_cleaner.state.floatbank8_c_air 5290 non-null float64 primary_cleaner.state.floatbank8_c_level 5290 non-null float64 primary_cleaner.state.floatbank8_d_air 5290 non-null float64 primary_cleaner.state.floatbank8_d_level 5290 non-null float64 rougher.input.feed_ag 5290 non-null float64 rougher.input.feed_pb 5290 non-null float64 rougher.input.feed_rate 5290 non-null float64 rougher.input.feed_size 5290 non-null float64 rougher.input.feed_sol 5290 non-null float64 rougher.input.feed_au 5290 non-null float64 rougher.input.floatbank10_sulfate 5290 non-null float64 rougher.input.floatbank10_xanthate 5290 non-null float64 rougher.input.floatbank11_sulfate 5290 non-null float64 rougher.input.floatbank11_xanthate 5290 non-null float64 rougher.state.floatbank10_a_air 5290 non-null float64 rougher.state.floatbank10_a_level 5290 non-null float64 rougher.state.floatbank10_b_air 5290 non-null float64 rougher.state.floatbank10_b_level 5290 non-null float64 rougher.state.floatbank10_c_air 5290 non-null float64 rougher.state.floatbank10_c_level 5290 non-null float64 rougher.state.floatbank10_d_air 5290 non-null float64 rougher.state.floatbank10_d_level 5290 non-null float64 rougher.state.floatbank10_e_air 5290 non-null float64 rougher.state.floatbank10_e_level 5290 non-null float64 rougher.state.floatbank10_f_air 5290 non-null float64 rougher.state.floatbank10_f_level 5290 non-null float64 secondary_cleaner.state.floatbank2_a_air 5290 non-null float64 secondary_cleaner.state.floatbank2_a_level 5290 non-null float64 secondary_cleaner.state.floatbank2_b_air 5290 non-null float64 secondary_cleaner.state.floatbank2_b_level 5290 non-null float64 secondary_cleaner.state.floatbank3_a_air 5290 non-null float64 secondary_cleaner.state.floatbank3_a_level 5290 non-null float64 secondary_cleaner.state.floatbank3_b_air 5290 non-null float64 secondary_cleaner.state.floatbank3_b_level 5290 non-null float64 secondary_cleaner.state.floatbank4_a_air 5290 non-null float64 secondary_cleaner.state.floatbank4_a_level 5290 non-null float64 secondary_cleaner.state.floatbank4_b_air 5290 non-null float64 secondary_cleaner.state.floatbank4_b_level 5290 non-null float64 secondary_cleaner.state.floatbank5_a_air 5290 non-null float64 secondary_cleaner.state.floatbank5_a_level 5290 non-null float64 secondary_cleaner.state.floatbank5_b_air 5290 non-null float64 secondary_cleaner.state.floatbank5_b_level 5290 non-null float64 secondary_cleaner.state.floatbank6_a_air 5290 non-null float64 secondary_cleaner.state.floatbank6_a_level 5290 non-null float64 final.output.recovery 5290 non-null float64 rougher.output.recovery 5290 non-null float64 dtypes: float64(54) memory usage: 2.2 MB . . We added in the missing target columns to the test dataset from the original df. . x_train = df_train_complete.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) y_train = df_train_complete[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] # Get features/target for test sub dataset x_test = df_test_complete.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) y_test = df_test_complete[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] . Train different models . Evaluate them using cross-validation. Pick the best model and test it using the test sample. . def get_cv_scores(classifier, x_train, y_train): kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True) cross_val_smape = np.abs(cross_val_score(classifier, x_train, y_train.to_numpy(), scoring=smape_scorer, cv=kfold)) print(&#39;Mean Cross-Validation sMAPE Score:&#39;, cross_val_smape.mean()) print(&#39;Min Cross-Validation sMAPE Score:&#39;, cross_val_smape.min()) print(&#39;Max Cross-Validation sMAPE Score:&#39;, cross_val_smape.max()) . LinearRegression . lr = MultiOutputRegressor(LinearRegression()) print(&#39;Linear Regression model:&#39;) get_cv_scores(lr, x_train, y_train) . Linear Regression model: Mean Cross-Validation sMAPE Score: 34.341736599192494 Min Cross-Validation sMAPE Score: 7.541454024526066 Max Cross-Validation sMAPE Score: 47.99015041175999 . DecisionTree . dt = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE)) print(&#39;Decision Tree model:&#39;) get_cv_scores(dt, x_train, y_train) . Decision Tree model: Mean Cross-Validation sMAPE Score: 2.1941193666185566 Min Cross-Validation sMAPE Score: 0.6229571145328955 Max Cross-Validation sMAPE Score: 3.9114996607788086 . RandomForest . rf = MultiOutputRegressor(RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=100)) print(&#39;Random Forest model:&#39;) get_cv_scores(rf, x_train, y_train) . Random Forest model: Mean Cross-Validation sMAPE Score: 2.3734912002748216 Min Cross-Validation sMAPE Score: 1.2144453600511473 Max Cross-Validation sMAPE Score: 4.28980393715649 . Conclusion . Cross-Validation of 5 fold sMAPE Scores Mean Min Max . Linear Regression | 34.34 | 7.54 | 47.99 | . Decision Tree | 2.19 | 0.623 | 3.91 | . Random Forest | 2.37 | 1.21 | 4.29 | . Looking that the sMAPE scores from the cross-validation of 5 fold, Decision Tree Regression model has the best lowest sMAPE scores across the mean, min, and max. . Parameter tuning . param_grid = { &#39;estimator__max_depth&#39;: range(1, 15), &#39;estimator__min_samples_leaf&#39;: range(1, 15) } # Create a based model model = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE)) # Instantiate the grid search model grid_search = GridSearchCV(estimator= model, param_grid= param_grid, scoring= smape_scorer, cv= 3, n_jobs= -1, verbose= 2) . # Fit the grid search to the data grid_search.fit(x_train, y_train.to_numpy()) best_param = grid_search.best_params_ print(&quot; n The best score across ALL searched params: n&quot;, grid_search.best_score_) print(&quot; n The best parameters across ALL searched params: n&quot;, grid_search.best_params_) . Fitting 3 folds for each of 196 candidates, totalling 588 fits [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... . [Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=-1)]: Done 1 out of 1 | elapsed: 0.1s remaining: 0.0s . [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s . [Parallel(n_jobs=-1)]: Done 588 out of 588 | elapsed: 6.2min finished . The best score across ALL searched params: -2.457959649299341 The best parameters across ALL searched params: {&#39;estimator__max_depth&#39;: 2, &#39;estimator__min_samples_leaf&#39;: 13} . . Model Selection . model = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE, max_depth=2, min_samples_leaf=13)) model.fit(x_train, y_train.to_numpy()) prediction = model.predict(x_test) . smape = calc_smape(y_test.to_numpy(), prediction) print(&#39;Final sMAPE score: &#39;, smape) . Final sMAPE score: 14.464377261082916 . Overall Conclusion . To fill the null values in the original datasets, we used fillna(method=&#39;ffill&#39;) since the data is ordered by time. MAE calculation values were also used to replace the original values in the datasets. We dropped the abnormal values for concentrations that were equal to and/or close to 0. And then we trained Linear Regression, Decision Tree Regressor, and Random Forest Regressor using the train dataset with a cross-validation of 5 fold to obtain sMAPE scores. Decision Tree had the best lowest sMAPE scores overall and was selected undergo hyper parameterization with GridSearchCV. Below sums up the final Decision Tree model and it&#39;s parameters: . Decision Tree Regression . Parameters: | random_state= 42, max_depth=2, min_samples_leaf=13 | . Final sMAPE score | 14.4644 | . Testing the final model using the test dataset gave us the final sMAPE score of 14.46% which is higher than our cross-validation mean score which used the only used train dataset. .",
            "url": "https://cmdang-mochi.github.io/ds-projects/machine%20learning/python/exploratory%20analysis/pandas/numpy/mathplotlib/scikit-learn/2020/12/16/gold_recovery.html",
            "relUrl": "/machine%20learning/python/exploratory%20analysis/pandas/numpy/mathplotlib/scikit-learn/2020/12/16/gold_recovery.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Profit Risks for New Oil Wells",
            "content": "Project Description . Analyze potential profit and risks for an oil company. . Create a model that will analyze different proposed locations for a new oil well that will maximize profits while minimizing risk. . Goal is to find the best place for a new oil well using provided data from three different regions. . Steps to choose the location: . Collect the oil well parameters in the selected region: oil quality and volume of reserves; | Build a model for predicting the volume of reserves in the new wells; | Pick the oil wells with the highest estimated values; | Pick the region with the highest total profit for the selected oil wells. | . There is data on oil samples from three regions. Parameters of each oil well in the region are already known. Build a model that will help to pick the region with the highest profit margin. Analyze potential profit and risks using the Bootstrapping technique. . Data description . Geological exploration data for the three regions are stored in files: . id — unique oil well identifier | f0, f1, f2 — three features of points (their specific meaning is unimportant, but the features themselves are significant) | product — volume of reserves in the oil well (thousand barrels). | . Conditions: . Only linear regression is suitable for model training (the rest are not sufficiently predictable). | When exploring the region, a study of 500 points is carried with picking the best 200 points for the profit calculation. | The budget for development of 200 oil wells is 100 USD million. | One barrel of raw materials brings 4.5 USD of revenue The revenue from one unit of product is 4,500 dollars (volume of reserves is in thousand barrels). | After the risk evaluation, keep only the regions with the risk of losses lower than 2.5%. From the ones that fit the criteria, the region with the highest average profit should be selected. | . The data is synthetic: contract details and well characteristics are not disclosed. . Import Libraries . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from numpy.random import RandomState from tabulate import tabulate from scipy import stats as st . Load data . geo_0 = pd.read_csv(&#39;/content/datasets/geo_data_0.csv&#39;) geo_1 = pd.read_csv(&#39;/content/datasets/geo_data_1.csv&#39;) geo_2 = pd.read_csv(&#39;/content/datasets/geo_data_2.csv&#39;) . # Functions to get descriptions and info from dataframe def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . . # Opening dataset for geo_0 get_information(geo_0) . Head: . id f0 f1 f2 product . 0 txEyH | 0.705745 | -0.497823 | 1.221170 | 105.280062 | . 1 2acmU | 1.334711 | -0.340164 | 4.365080 | 73.037750 | . 2 409Wp | 1.022732 | 0.151990 | 1.419926 | 85.265647 | . 3 iJLyR | -0.032172 | 0.139033 | 2.978566 | 168.620776 | . 4 Xdl7t | 1.988431 | 0.155413 | 4.751769 | 154.036647 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 0.500419 | 0.250143 | 2.502647 | 92.500000 | . std 0.871832 | 0.504433 | 3.248248 | 44.288691 | . min -1.408605 | -0.848218 | -12.088328 | 0.000000 | . 25% -0.072580 | -0.200881 | 0.287748 | 56.497507 | . 50% 0.502360 | 0.250252 | 2.515969 | 91.849972 | . 75% 1.073581 | 0.700646 | 4.715088 | 128.564089 | . max 2.362331 | 1.343769 | 16.003790 | 185.364347 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_0 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . # Opening dataset for geo_1 get_information(geo_1) . Head: . id f0 f1 f2 product . 0 kBEdx | -15.001348 | -8.276000 | -0.005876 | 3.179103 | . 1 62mP7 | 14.272088 | -3.475083 | 0.999183 | 26.953261 | . 2 vyE1P | 6.263187 | -5.948386 | 5.001160 | 134.766305 | . 3 KcrkZ | -13.081196 | -11.506057 | 4.999415 | 137.945408 | . 4 AHL4O | 12.702195 | -8.147433 | 5.004363 | 134.766305 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 1.141296 | -4.796579 | 2.494541 | 68.825000 | . std 8.965932 | 5.119872 | 1.703572 | 45.944423 | . min -31.609576 | -26.358598 | -0.018144 | 0.000000 | . 25% -6.298551 | -8.267985 | 1.000021 | 26.953261 | . 50% 1.153055 | -4.813172 | 2.011479 | 57.085625 | . 75% 8.621015 | -1.332816 | 3.999904 | 107.813044 | . max 29.421755 | 18.734063 | 5.019721 | 137.945408 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_1 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . # Opening dataset for geo_2 get_information(geo_2) . Head: . id f0 f1 f2 product . 0 fwXo0 | -1.146987 | 0.963328 | -0.828965 | 27.758673 | . 1 WJtFt | 0.262778 | 0.269839 | -2.530187 | 56.069697 | . 2 ovLUW | 0.194587 | 0.289035 | -5.586433 | 62.871910 | . 3 q6cA6 | 2.236060 | -0.553760 | 0.930038 | 114.572842 | . 4 WPMUX | -0.515993 | 1.716266 | 5.899011 | 149.600746 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 0.002023 | -0.002081 | 2.495128 | 95.000000 | . std 1.732045 | 1.730417 | 3.473445 | 44.749921 | . min -8.760004 | -7.084020 | -11.970335 | 0.000000 | . 25% -1.162288 | -1.174820 | 0.130359 | 59.450441 | . 50% 0.009424 | -0.009482 | 2.484236 | 94.925613 | . 75% 1.158535 | 1.163678 | 4.858794 | 130.595027 | . max 7.238262 | 7.844801 | 16.739402 | 190.029838 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_2 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . From all the datasets, we might be able to drop the id column as it does not provide the models much information. . Preprocessing data . geo_0_data = geo_0.drop([&#39;id&#39;], axis=1) display(geo_0_data.head()) geo_1_data = geo_1.drop([&#39;id&#39;], axis=1) display(geo_1_data.head()) geo_2_data = geo_2.drop([&#39;id&#39;], axis=1) display(geo_2_data.head()) . f0 f1 f2 product . 0 0.705745 | -0.497823 | 1.221170 | 105.280062 | . 1 1.334711 | -0.340164 | 4.365080 | 73.037750 | . 2 1.022732 | 0.151990 | 1.419926 | 85.265647 | . 3 -0.032172 | 0.139033 | 2.978566 | 168.620776 | . 4 1.988431 | 0.155413 | 4.751769 | 154.036647 | . f0 f1 f2 product . 0 -15.001348 | -8.276000 | -0.005876 | 3.179103 | . 1 14.272088 | -3.475083 | 0.999183 | 26.953261 | . 2 6.263187 | -5.948386 | 5.001160 | 134.766305 | . 3 -13.081196 | -11.506057 | 4.999415 | 137.945408 | . 4 12.702195 | -8.147433 | 5.004363 | 134.766305 | . f0 f1 f2 product . 0 -1.146987 | 0.963328 | -0.828965 | 27.758673 | . 1 0.262778 | 0.269839 | -2.530187 | 56.069697 | . 2 0.194587 | 0.289035 | -5.586433 | 62.871910 | . 3 2.236060 | -0.553760 | 0.930038 | 114.572842 | . 4 -0.515993 | 1.716266 | 5.899011 | 149.600746 | . The datasets geo_0, geo_1, geo_2 all have 100,000 entries with no missing values and have datatypes that seem to be correct. We were able to drop the id column as this would not provide much information for our model. . Split Data . RANDOM_STATE = 12345 #Random_State def get_train_valid(df): df_train, df_valid = train_test_split(df, test_size=0.25, random_state=RANDOM_STATE) # Splits data up to 75% train and 25% test return df_train, df_valid . geo0_target = geo_0_data[&#39;product&#39;] geo0_features = geo_0_data.drop([&#39;product&#39;], axis=1) geo0_x_train, geo0_x_valid = get_train_valid(geo0_features) geo0_y_train, geo0_y_valid = get_train_valid(geo0_target) assert geo0_x_train.shape[0] == geo0_y_train.shape[0] assert geo0_x_valid.shape[0] == geo0_y_valid.shape[0] print(&#39;geo_0 datasets: n&#39;) print(&#39;Train:&#39;, geo0_x_train.shape, &#39; Target Train:&#39;, geo0_y_train.shape) print(&#39;Validation:&#39;, geo0_x_valid.shape, &#39; Target Validation:&#39;, geo0_y_valid.shape) . geo_0 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . geo1_target = geo_1_data[&#39;product&#39;] geo1_features = geo_1_data.drop([&#39;product&#39;], axis=1) geo1_x_train, geo1_x_valid = get_train_valid(geo1_features) geo1_y_train, geo1_y_valid = get_train_valid(geo1_target) assert geo1_x_train.shape[0] == geo1_y_train.shape[0] assert geo1_x_valid.shape[0] == geo1_y_valid.shape[0] print(&#39;geo_1 datasets: n&#39;) print(&#39;Train:&#39;, geo1_x_train.shape, &#39; Target Train:&#39;, geo1_y_train.shape) print(&#39;Validation:&#39;, geo1_x_valid.shape, &#39; Target Validation:&#39;, geo1_y_valid.shape) . geo_1 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . geo2_target = geo_2_data[&#39;product&#39;] geo2_features = geo_2_data.drop([&#39;product&#39;], axis=1) geo2_x_train, geo2_x_valid = get_train_valid(geo2_features) geo2_y_train, geo2_y_valid = get_train_valid(geo2_target) assert geo2_x_train.shape[0] == geo2_y_train.shape[0] assert geo2_x_valid.shape[0] == geo2_y_valid.shape[0] print(&#39;geo_2 datasets: n&#39;) print(&#39;Train:&#39;, geo2_x_train.shape, &#39; Target Train:&#39;, geo2_y_train.shape) print(&#39;Validation:&#39;, geo2_x_valid.shape, &#39; Target Validation:&#39;, geo2_y_valid.shape) . geo_2 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . Needs fixing: We don&#39;t need valid part in the task because we don&#39;t tune parameters. Create only train(75%) and test(25%). Updated to only have validation(25%) and train(75%) datasets, since we do not need the test dataset for tunning parameters on our models . The dataset for the three different regions were successfully split into train and validation datasets with a 75:25 ratio. There is no testing sub-dataset as the models will not be hyper-tuned. . Model building . def LinReg_sanity_check(x_train, x_valid, y_train, y_valid): model = LinearRegression() model.fit(x_train, y_train) predictions = model.predict(x_valid) print(&#39;Model validation/prediction datasets scores&#39;) print(&#39;Accuracy:&#39;, model.score(x_valid, y_valid)) print(&#39;R2:&#39;, r2_score(y_valid, predictions)) print(&#39;RMSE:&#39;, mean_squared_error(y_valid, predictions, squared=False)) . geo_0 region . geo0_model = LinearRegression() geo0_model.fit(geo0_x_train, geo0_y_train) geo0_predictions = geo0_model.predict(geo0_x_valid) print(&#39;Sanity check for geo_0 model:&#39;) LinReg_sanity_check(geo0_x_train, geo0_x_valid, geo0_y_train, geo0_y_valid) print(&#39; nAverage volume of predicted reserves in geo_0 (thousand barrels):&#39;, geo0_predictions.mean()) . Sanity check for geo_0 model: Model validation/prediction datasets scores Accuracy: 0.27994321524487786 R2: 0.27994321524487786 RMSE: 37.5794217150813 Average volume of predicted reserves in geo_0 (thousand barrels): 92.59256778438038 . For the data in geo_0, the accuracy, R2 and RMSE scores are pretty terrble. The accuracy and R2 score are very low and the RMSE score is very big. . geo_1 region . geo1_model = LinearRegression() geo1_model.fit(geo1_x_train, geo1_y_train) geo1_predictions = geo1_model.predict(geo1_x_valid) print(&#39;Sanity check for geo_1 model: n&#39;) LinReg_sanity_check(geo1_x_train, geo1_x_valid, geo1_y_train, geo1_y_valid) print(&#39; nAverage volume of predicted reserves in geo_1 (thousand barrels):&#39;, geo1_predictions.mean()) . Sanity check for geo_1 model: Model validation/prediction datasets scores Accuracy: 0.9996233978805126 R2: 0.9996233978805127 RMSE: 0.893099286775616 Average volume of predicted reserves in geo_1 (thousand barrels): 68.728546895446 . The scores for data in geo_1 are actually pretty good with accuracy and R2 score pretty close to 1 and RMSE score being very low. . geo_2 region . geo2_model = LinearRegression() geo2_model.fit(geo2_x_train, geo2_y_train) geo2_predictions = geo2_model.predict(geo2_x_valid) print(&#39;Sanity check for geo_2 model: n&#39;) LinReg_sanity_check(geo2_x_train, geo2_x_valid, geo2_y_train, geo2_y_valid) print(&#39; nAverage volume of predicted reserves in geo_2 (thousand barrels):&#39;, geo2_predictions.mean()) . Sanity check for geo_2 model: Model validation/prediction datasets scores Accuracy: 0.20524758386040443 R2: 0.20524758386040443 RMSE: 40.02970873393434 Average volume of predicted reserves in geo_2 (thousand barrels): 94.96504596800489 . Similar to the model for geo_0, the scores here for geo_2 model are not that great with a very low accuracy and R2 score and high RMSE score. . Conclusion . The table below shows each models&#39; results using the valdiation datasets to make predictions: . . geo_0 geo_1 geo_2 . Accuracy Score | 0.2799 | 0.9996 | 0.2052 | . R2 Score | 0.2799 | 0.9996 | 0.2052 | . RMSE Score | 37.579 | 0.8903 | 40.030 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . . R2 score shows the relative measure of fit, while RMSE is an absolute measure of fit. Lower values of RMSE indicate a better fit. . The model for geo_1 has the best scores for accuracy, R2, and RMSE. With accuracy and R2 being close to 1 and with a very small RMSE it seems like the model can predict pretty well using the dataset provided for geo_1. However, predicted average volume of reserves in ge0_1 is the lowest of the 3 regions being at 68.968 thousands barrels. . The models for geo_0 and geo_2 performed pretty similar for the datasets given for these two regions. The accuracy and R2 score are very small ranging around 0.20 - 0.30 and have RMSE score around 40. In comparison, though these two have similar scores in accuracy, R2 and RMSE, the model for geo_0 did slightly better with a slightly higher accuracy and R2 score and a bit lower RMSE score compared to the model for geo_2. . The model for geo_2 predicted largest average volume of reserves with 94.955 thousands barrels. With geo_0 coming in second with the largest predicted average volume of reserves at 92.708 thousands barrels. geo_1 came in last with the smallest predicted average volume at 68.968 thousand barrels. . Having such a large RMSE score for the models for geo_0 and geo_2 shows that perhaps a linear regression model might not be ideal for these two regions. . Profit calculation . BUDGET = 100000000 # budget for development of 200 oil wells is 100 USD million POINT_PER_BUDGET = 200 # number of wells in the budget PRODUCT_PRICE = 4500 # revenue from one unit of product is 4,500 dollars (volume of reserves is in thousand barrels) #Calculation for the volume of reserves sufficient for developing a new well without losses volume_no_loss = (BUDGET/POINT_PER_BUDGET) / PRODUCT_PRICE print(&#39;Volume of reserves sufficient for developing a new well without losses:&#39;, volume_no_loss, &#39;(thousand barrels)&#39;) def average_volume(df): return df[&#39;product&#39;].mean() print(&#39; nAverage volume of reserves in each region:&#39;) print(&#39;* geo_0: &#39;, average_volume(geo_0), &#39;(thousand barrels)&#39;) print(&#39;* geo_1: &#39;, average_volume(geo_1), &#39;(thousand barrels)&#39;) print(&#39;* geo_2: &#39;, average_volume(geo_2), &#39;(thousand barrels)&#39;) . Volume of reserves sufficient for developing a new well without losses: 111.11111111111111 (thousand barrels) Average volume of reserves in each region: * geo_0: 92.49999999999976 (thousand barrels) * geo_1: 68.82500000002561 (thousand barrels) * geo_2: 95.00000000000041 (thousand barrels) . . The minimum volume of reserves need to develop a new well without losses is 111.11 (thousand barrels). . Tables showing average volume in each region: . geo_0 geo_1 geo_2 . Average volume of reserves (thousand barrels) | 92.499 | 68.825 | 95.00 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . Just by looking at the average volume of reserves in reach region (actual and predicted volumes), it doesn&#39;t seem that a developing a new well would be profitable. We would have to look at the top producing wells (actual and predicted) in each region to see if the calculation differs. . Profit from a set of selected oil wells and model predictions . def revenue(y_valid, predictions, count): predictions = pd.Series(predictions) y_valid = pd.Series(y_valid.values) predict_sorted = predictions.sort_values(ascending=False) selected_wells = y_valid[predict_sorted.index][:count] return PRODUCT_PRICE * selected_wells.sum() def profit(revenue): profit = revenue - BUDGET return profit . rev_geo0 = revenue(geo0_y_valid, geo0_predictions, 200) print(&#39;Predicted profit for region geo_0 with top best 200 wells:&#39;, profit(rev_geo0).round(2)) . Predicted profit for region geo_0 with top best 200 wells: 33208260.43 . rev_geo1 = revenue(geo1_y_valid, geo1_predictions, 200) print(&#39;Predicted profit for region geo_1 with top best 200 wells:&#39;, profit(rev_geo1).round(2)) . Predicted profit for region geo_1 with top best 200 wells: 24150866.97 . rev_geo2 = revenue(geo2_y_valid, geo2_predictions, 200) print(&#39;Predicted profit for region geo_2 with top best 200 wells:&#39;, profit(rev_geo2).round(2)) . Predicted profit for region geo_2 with top best 200 wells: 27103499.64 . . From the calculations in this section, it&#39;s best to develop new wells in region geo_0. The calculation for predicted profit is highest in this region at 33,208,260.43 USD. geo_1 had the lowest predicted profit at 24,150,866.97 USD and geo_2 came out in the middle at 27,103,499.64 USD. . Calculate risks and profit for each region . def profit_distribution(y_valid, predictions): y_valid = pd.Series(y_valid.values) state = np.random.RandomState(12345) values = [] for i in range(1000): target_subsample = y_valid.sample(n=500, replace=True, random_state=state) predict_subsample = predictions[target_subsample.index] rev = revenue(target_subsample, predict_subsample, 200) values.append(rev) values = pd.Series(values) values = values.sort_values() profit_values = profit(values) return profit_values #Calculation for confidence interval at 95% def confidence_interval(profit_values): confidence_interval = st.t.interval( 0.95, len(profit_values)-1, profit_values.mean(), profit_values.sem()) return print(&quot;95% confidence interval:&quot;, confidence_interval) #Value at Risk (Risk of Losses) | Confidence Level calculation def risk_of_loss(profit_values): &quot;&quot;&quot; Value at risk (VaR) is a measure of the risk of loss for investments. It estimates how much a set of investments might lose (with a given probability), given normal market conditions, in a set time period such as a day &quot;&quot;&quot; upper = profit_values.quantile(0.975) lower = profit_values.quantile(0.025) return print(&#39; n&#39;, tabulate([[&#39;2.5%&#39;, lower], [&#39;97.5%&#39;, upper]], floatfmt=&#39;.2f&#39;, headers=[&#39;Confidence Level&#39;, &#39;Value at Risk&#39;])) #Sum of loss (negative profit) within our 1000 sample def loss(profit_values): loss = sum(i for i in profit_values if i &lt; 0) return loss #Get number of instances where profit sample is negative def count_loss(profit_values): num_loss = profit_values.lt(0).sum().sum() return num_loss #Sum of gain (positive profit) within our 1000 sample def gain(profit_values): gain = sum(i for i in profit_values if i &gt; 0) return gain #Get number of instances where profit sample is positive def count_gain(profit_values): num_gain = profit_values.gt(0).sum().sum() return num_gain #Calculation for profit/loss ratio def proft_loss_ratio(profit_values): total_gain = gain(profit_values) total_loss = loss(profit_values) total_num_gain = count_gain(profit_values) total_num_loss = count_loss(profit_values) ratio = ((total_gain/total_num_gain) / (total_loss/total_num_loss)) return ratio #Loss probability calculation def loss_prob(profit_values): &quot;&quot;&quot; Loss probability is percentage of negative values in profit array &quot;&quot;&quot; total_num_loss = count_loss(profit_values) #Count of negative values in profit array prob = total_num_loss / 1000 #Sample size of 1000 for profit array return prob . geo0_profit_values = profit_distribution(geo0_y_valid, geo0_predictions) print(&#39;Average profit for region geo_0:&#39;, round(geo0_profit_values.mean(), 2)) confidence_interval(geo0_profit_values) risk_of_loss(geo0_profit_values) #Loss probability print(&#39; nLoss probability for geo_0:&#39;, loss_prob(geo0_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo0_profit_values))) . Average profit for region geo_0: 3961649.85 95% confidence interval: (3796203.151479729, 4127096.544567701) Confidence Level Value at Risk 2.5% -1112155.46 97.5% 9097669.42 Loss probability for geo_0: 0.069 or 6.90% . geo1_profit_values = profit_distribution(geo1_y_valid, geo1_predictions) print(&#39;Average profit for region geo_1:&#39;, round(geo1_profit_values.mean(), 2)) confidence_interval(geo1_profit_values) risk_of_loss(geo1_profit_values) #Loss probability print(&#39; nLoss probability for geo_1:&#39;, loss_prob(geo1_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo1_profit_values))) . Average profit for region geo_1: 4560451.06 95% confidence interval: (4431472.486639012, 4689429.629094217) Confidence Level Value at Risk 2.5% 338205.09 97.5% 8522894.54 Loss probability for geo_1: 0.015 or 1.50% . geo2_profit_values = profit_distribution(geo2_y_valid, geo2_predictions) print(&#39;Average profit for region geo_2:&#39;, round(geo2_profit_values.mean(), 2)) confidence_interval(geo2_profit_values) risk_of_loss(geo2_profit_values) #Loss probability print(&#39; nLoss probability for geo_2:&#39;, loss_prob(geo2_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo2_profit_values))) . Average profit for region geo_2: 4044038.67 95% confidence interval: (3874457.974712804, 4213619.356654332) Confidence Level Value at Risk 2.5% -1633504.13 97.5% 9503595.75 Loss probability for geo_2: 0.076 or 7.60% . . Based on the calculations in this section, the best region get develop new oil wells would be region geo_1. Out of the 1000 samples that was obtained in each region, geo_1 had the smallest probablity of loss at 1.50%. The calculated average profit for region geo_1 out of the 1000 sample slice came in at 4,560,451.06 USD which was the highest average profit out of the three regions. The table below summarizes the findings in this secion: . geo_0 geo_1 geo_2 . Average profit (USD) | 3,961,649.85 | 4,560,451.06 | 4,044,038.67 | . 95% confidence interval | (3796203.15, 4127096.54) | (4431472.49, 4689429.63) | (3874457.97, 4213619.36) | . Risk of loss at 2.5% confidence level | -1112155.46 | 338205.09 | -1633504.13 | . Risk of loss at 97.5% confidence level | 9097669.42 | 8522894.54 | 9503595.75 | . Loss probability (%) | 6.90% | 1.50% | 7.60% | . Overall Conclusion . In conlusion, with the calculations made for the provided datasets it seems as though region geo_1 is the best region develop new wells. The model for geo_1 had the best scores. The predicted profit for geo_1 came in second out of the three regions which isn&#39;t bad. And overall, geo_1 when sampled 1000 wells had the lowest loss probablity and had highest average profit. . The table below summarize the sections we calculated: . geo_0 geo_1 geo_2 . Accuracy Score | 0.2799 | 0.9996 | 0.2052 | . R2 Score | 0.2799 | 0.9996 | 0.2052 | . RMSE Score | 37.579 | 0.8903 | 40.030 | . Average volume of reserves (thousand barrels) | 92.499 | 68.825 | 95.00 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . Predicted profit (USD) | 33,208,260.43 | 24,150,866.97 | 27,103,499.64 | . Average profit (USD) | 3,961,649.85 | 4,560,451.06 | 4,044,038.67 | . 95% confidence interval | (3796203.15, 4127096.54) | (4431472.49, 4689429.63) | (3874457.97, 4213619.36) | . Risk of loss at 2.5% confidence level | -1112155.46 | 338205.09 | -1633504.13 | . Risk of loss at 97.5% confidence level | 9097669.42 | 8522894.54 | 9503595.75 | . Loss probability (%) | 6.90% | 1.50% | 7.60% | .",
            "url": "https://cmdang-mochi.github.io/ds-projects/machine%20learning/python/pandas/numpy/scikit-learn/tabulate/scipy/2020/11/22/profit_risks_for_new_oil_wells.html",
            "relUrl": "/machine%20learning/python/pandas/numpy/scikit-learn/tabulate/scipy/2020/11/22/profit_risks_for_new_oil_wells.html",
            "date": " • Nov 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Video Game Success Patterns/Trends",
            "content": "Project description . User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. You need to identify patterns that determine whether a game succeeds or not. This will allow you to spot potential big winners and plan advertising campaigns. . In front of you is data going back to 2016. Let’s imagine that it’s December 2016 and you’re planning a campaign for 2017. . (The important thing is to get experience working with data. It doesn&#39;t really matter whether you&#39;re forecasting 2017 sales based on data from 2016 or 2027 sales based on data from 2016.) . The dataset contains the abbreviation ESRB. The Entertainment Software Rating Board evaluates a game&#39;s content and assigns an age rating such as Teen or Mature. . Data description . Data is from open sources 2016 - historical game sales data: user and expert ratings, country sales, genres and platforms, ESRB ratings . Name - the name of the game | Platform - platform | Year_of_Release - release year | Genre - game genre | NA_sales - sales in North America (millions of copies sold) | EU_sales - sales in Europe (millions of copies sold) | JP_sales - sales in Japan (millions of copies sold) | Other_sales - sales in other countries (millions of copies sold) | Critic_Score - critic scores (maximum 100) | User_Score - user rating (maximum 10) | Rating - rating by the ESRB (Entertainment Software Rating Board), which determines the rating of the game and the appropriate age category | . Import Libraries . # Import in libraries to use in project import pandas as pd import numpy as np import matplotlib.pyplot as plt import re from scipy import stats as st import seaborn as sns import matplotlib.ticker as ticker import seaborn.apionly as sns . /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The examples.directory rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2. In the future, examples will be found relative to the &#39;datapath&#39; directory. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/site-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version. warnings.warn(msg, UserWarning) . . Load Data . # Functions to get descriptions and info from dataframe def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) display(df.describe(include=&#39;object&#39;)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . . df_games = pd.read_csv(&#39;/datasets/games.csv&#39;) # this is correct path get_information(df_games) . Head: . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 | Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 | Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 | Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 | Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 | Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): Name 16713 non-null object Platform 16715 non-null object Year_of_Release 16446 non-null float64 Genre 16713 non-null object NA_sales 16715 non-null float64 EU_sales 16715 non-null float64 JP_sales 16715 non-null float64 Other_sales 16715 non-null float64 Critic_Score 8137 non-null float64 User_Score 10014 non-null object Rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . None . **************************************************************************************************** Describe: . Year_of_Release NA_sales EU_sales JP_sales Other_sales Critic_Score . count | 16446.000000 | 16715.000000 | 16715.000000 | 16715.000000 | 16715.000000 | 8137.000000 | . mean | 2006.484616 | 0.263377 | 0.145060 | 0.077617 | 0.047342 | 68.967679 | . std | 5.877050 | 0.813604 | 0.503339 | 0.308853 | 0.186731 | 13.938165 | . min | 1980.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | . 25% | 2003.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 60.000000 | . 50% | 2007.000000 | 0.080000 | 0.020000 | 0.000000 | 0.010000 | 71.000000 | . 75% | 2010.000000 | 0.240000 | 0.110000 | 0.040000 | 0.030000 | 79.000000 | . max | 2016.000000 | 41.360000 | 28.960000 | 10.220000 | 10.570000 | 98.000000 | . Name Platform Genre User_Score Rating . count | 16713 | 16715 | 16713 | 10014 | 9949 | . unique | 11559 | 31 | 12 | 96 | 8 | . top | Need for Speed: Most Wanted | PS2 | Action | tbd | E | . freq | 12 | 2161 | 3369 | 2424 | 3990 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . Name | 2 | 0.01 | . Platform | 0 | 0.00 | . Year_of_Release | 269 | 1.61 | . Genre | 2 | 0.01 | . NA_sales | 0 | 0.00 | . EU_sales | 0 | 0.00 | . JP_sales | 0 | 0.00 | . Other_sales | 0 | 0.00 | . Critic_Score | 8578 | 51.32 | . User_Score | 6701 | 40.09 | . Rating | 6766 | 40.48 | . **************************************************************************************************** Shape: . (16715, 11) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Columns Critic_Score, User_Score, and Rating have a lot of null values, Critic_Score with about 51% null values, and User_Score and Rating having about 40% null values. These columns seem to have the most missing values. Year_of_Release have about 269 values missing but it is a small percentage compared the first three mentioned. The dataset has 16715 entries in total. . Pre-process Data . Replace the column names (make them lowercase) . df_games.columns . Index([&#39;Name&#39;, &#39;Platform&#39;, &#39;Year_of_Release&#39;, &#39;Genre&#39;, &#39;NA_sales&#39;, &#39;EU_sales&#39;, &#39;JP_sales&#39;, &#39;Other_sales&#39;, &#39;Critic_Score&#39;, &#39;User_Score&#39;, &#39;Rating&#39;], dtype=&#39;object&#39;) . df_games.columns = [columns.lower().replace(&#39; &#39;, &#39;_&#39;) for columns in df_games.columns] df_games.columns . Index([&#39;name&#39;, &#39;platform&#39;, &#39;year_of_release&#39;, &#39;genre&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;, &#39;critic_score&#39;, &#39;user_score&#39;, &#39;rating&#39;], dtype=&#39;object&#39;) . Convert data types and deal with missing values . df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . We have to keep in mind about specifying explicit values, as this may cause errors in real business tasks. Depending on the data and how many values are missing we might have to just drop the missing values or fill then in with our best judgement. . name and genre are missing the 2 values and I believe we can leave these as N. . | year_of_release should be an int instead of a float and we need to replace some of the missing values in that column. . | critic_score has a lot of missing values. . | user_score should be a float or int but again we have to deal with the missing values in this column or figure out how to deal with the TBD values. . | rating just like critic_score has a ton of missing values. . | . def convert_to_type(df, column, type_value): &quot;&quot;&quot; Convert to columns to certain type values&quot;&quot;&quot; for col in column: df[col] = df[col].astype(type_value) . df_games[&#39;name&#39;].fillna(np.nan, inplace=True) df_games[&#39;genre&#39;].fillna(np.nan, inplace=True) convert_to_type(df_games, [&#39;name&#39;], str) convert_to_type(df_games, [&#39;genre&#39;], str) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . name and genre missing values have been filled in with NAN (only 2 values missing). . # Grouping the data by platform and seeing the mean and median for the year_of_release df_platform = df_games.groupby([&#39;platform&#39;]).agg({&#39;year_of_release&#39;:[&#39;mean&#39;, &#39;median&#39;]}) display(df_platform) . year_of_release . mean median . platform . 2600 | 1982.137931 | 1982.0 | . 3DO | 1994.666667 | 1995.0 | . 3DS | 2013.126953 | 2013.0 | . DC | 1999.942308 | 2000.0 | . DS | 2008.185290 | 2008.0 | . GB | 1995.958763 | 1997.0 | . GBA | 2003.210851 | 2003.0 | . GC | 2003.400369 | 2003.0 | . GEN | 1993.034483 | 1993.0 | . GG | 1992.000000 | 1992.0 | . N64 | 1998.531646 | 1999.0 | . NES | 1987.153061 | 1986.5 | . NG | 1994.500000 | 1994.5 | . PC | 2008.914316 | 2010.0 | . PCFX | 1996.000000 | 1996.0 | . PS | 1998.005882 | 1998.0 | . PS2 | 2004.583921 | 2005.0 | . PS3 | 2010.840735 | 2011.0 | . PS4 | 2015.145408 | 2015.0 | . PSP | 2008.731769 | 2009.0 | . PSV | 2014.132867 | 2014.0 | . SAT | 1996.028902 | 1996.0 | . SCD | 1993.833333 | 1994.0 | . SNES | 1993.845188 | 1994.0 | . TG16 | 1995.000000 | 1995.0 | . WS | 2000.000000 | 2000.0 | . Wii | 2008.966563 | 2009.0 | . WiiU | 2013.659864 | 2013.0 | . X360 | 2009.880682 | 2010.0 | . XB | 2003.636364 | 2004.0 | . XOne | 2014.951417 | 2015.0 | . df_platform_median = df_games.groupby([&#39;platform&#39;]).agg({&#39;year_of_release&#39;:&#39;median&#39;}).reset_index() # Fill in NaN values in year_of_release with median year_of_release per platform df_games.year_of_release = df_games.year_of_release.fillna(df_games.platform.map(df_platform_median.set_index(&#39;platform&#39;).year_of_release)) convert_to_type(df_games, [&#39;year_of_release&#39;], int) df_games[&#39;year_of_release&#39;] . 0 2006 1 1985 2 2008 3 2009 4 1996 ... 16710 2016 16711 2006 16712 2016 16713 2003 16714 2016 Name: year_of_release, Length: 16715, dtype: int64 . year_of_release missing values was filled in with median year_of_release per platform. This way at least the filled in missing values are a bit more tailored towards the game in regards to the platform. We were successful in converting the data type from float to int as well. . df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(5), int64(1), object(5) memory usage: 1.4+ MB . df_critic_score = df_games.groupby([&#39;genre&#39;]).agg({&#39;critic_score&#39;:[&#39;mean&#39;, &#39;median&#39;, pd.Series.mode]}) display(df_critic_score) print(&quot;Mean critic_score: &quot;, df_games[&#39;critic_score&#39;].mean()) print(&quot;Median critic_score&quot;, df_games[&#39;critic_score&#39;].median()) . critic_score . mean median mode . genre . Action | 66.629101 | 68.0 | 71 | . Adventure | 65.331269 | 66.0 | 66 | . Fighting | 69.217604 | 72.0 | 74 | . Misc | 66.619503 | 69.0 | 73 | . Platform | 68.058350 | 69.0 | 71 | . Puzzle | 67.424107 | 70.0 | 75 | . Racing | 67.963612 | 69.0 | 82 | . Role-Playing | 72.652646 | 74.0 | 77 | . Shooter | 70.181144 | 73.0 | [72.0, 74.0, 78.0, 81.0] | . Simulation | 68.619318 | 70.0 | 65 | . Sports | 71.968174 | 75.0 | 80 | . Strategy | 72.086093 | 73.0 | [69.0, 75.0] | . nan | NaN | NaN | [] | . Mean critic_score: 68.96767850559173 Median critic_score 71.0 . I feel like it&#39;s best to fill in with median critic_score per genre and for the NaN we can use the median overall critic_score. I am not sure if it makes much of a difference here if we used mean or median but looking at the mode for critic_score it seems like it would be best to use median as the mean seems a bit lower than expected comparing to the mode (which shows what is the most reoccuring critic_score in that genre). . df_critic_score_median = df_games.groupby([&#39;genre&#39;]).agg({&#39;critic_score&#39;:&#39;median&#39;}).reset_index() df_games.critic_score = df_games.critic_score.fillna(df_games.genre.map(df_critic_score_median.set_index(&#39;genre&#39;).critic_score)) df_games[&#39;critic_score&#39;].fillna(np.nan, inplace=True) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(5), int64(1), object(5) memory usage: 1.4+ MB . # Replacing TBD with NaN and converting to float data type df_games[&#39;user_score&#39;].replace([&#39;tbd&#39;], np.nan, inplace=True) df_games[&#39;user_score&#39;].fillna(np.nan, inplace=True) convert_to_type(df_games, [&#39;user_score&#39;], float) df_user_score = df_games.groupby([&#39;genre&#39;]).agg({&#39;user_score&#39;:[&#39;mean&#39;, &#39;median&#39;, pd.Series.mode]}) display(df_user_score) print(&quot;Mean user_score: &quot;, df_games[&#39;user_score&#39;].mean()) print(&quot;Median user_score&quot;, df_games[&#39;user_score&#39;].median()) . user_score . mean median mode . genre . Action | 7.054044 | 7.4 | 8 | . Adventure | 7.133000 | 7.6 | 8.2 | . Fighting | 7.302506 | 7.6 | [7.9, 8.5] | . Misc | 6.819362 | 7.1 | 7.8 | . Platform | 7.301402 | 7.7 | 8.6 | . Puzzle | 7.175000 | 7.5 | 7.5 | . Racing | 7.036193 | 7.4 | 8.2 | . Role-Playing | 7.619515 | 7.8 | 8.2 | . Shooter | 7.041883 | 7.4 | [7.8, 8.2] | . Simulation | 7.134593 | 7.5 | [7.3, 8.0, 8.2, 8.8] | . Sports | 6.961197 | 7.4 | 7.8 | . Strategy | 7.295177 | 7.8 | 8.3 | . nan | NaN | NaN | [] | . Mean user_score: 7.125046113306982 Median user_score 7.5 . The data for user_score has a similar pattern with mean, median, and mode as the critic_score. With the same logic, I think it&#39;s best to replace the missing values for user_score with median user_score per genre. . df_user_score_median = df_games.groupby([&#39;genre&#39;]).agg({&#39;user_score&#39;:&#39;median&#39;}).reset_index() df_games.user_score = df_games.user_score.fillna(df_games.genre.map(df_user_score_median.set_index(&#39;genre&#39;).user_score)) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 16713 non-null float64 rating 9949 non-null object dtypes: float64(6), int64(1), object(4) memory usage: 1.4+ MB . df_games[&#39;rating&#39;].fillna(np.nan, inplace=True) df_rating = df_games.groupby([&#39;genre&#39;]).agg({&#39;rating&#39;: pd.Series.mode}).reset_index() df_rating . genre rating . 0 | Action | T | . 1 | Adventure | E | . 2 | Fighting | T | . 3 | Misc | E | . 4 | Platform | E | . 5 | Puzzle | E | . 6 | Racing | E | . 7 | Role-Playing | T | . 8 | Shooter | M | . 9 | Simulation | E | . 10 | Sports | E | . 11 | Strategy | T | . 12 | nan | [] | . Since rating is categorical we can&#39;t just use mean and median as easily. Mode might be a good filler here. We can used the mode for rating per genre to fill in the missing values. . df_games.rating = df_games.rating.fillna(df_games.genre.map(df_rating.set_index(&#39;genre&#39;).rating)) convert_to_type(df_games, [&#39;rating&#39;], str) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 16713 non-null float64 rating 16715 non-null object dtypes: float64(6), int64(1), object(4) memory usage: 1.4+ MB . Calculate the total sales (the sum of sales in all regions) for each game and put these values in a separate column . df_games[&#39;total_sales&#39;] = df_games[[&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]].sum(axis=1) df_games . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | 69.0 | 7.7 | E | 40.24 | . 2 | Mario Kart Wii | Wii | 2008 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | 74.0 | 7.8 | T | 31.38 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 16710 | Samurai Warriors: Sanada Maru | PS3 | 2016 | Action | 0.00 | 0.00 | 0.01 | 0.00 | 68.0 | 7.4 | T | 0.01 | . 16711 | LMA Manager 2007 | X360 | 2006 | Sports | 0.00 | 0.01 | 0.00 | 0.00 | 75.0 | 7.4 | E | 0.01 | . 16712 | Haitaka no Psychedelica | PSV | 2016 | Adventure | 0.00 | 0.00 | 0.01 | 0.00 | 66.0 | 7.6 | E | 0.01 | . 16713 | Spirits &amp; Spells | GBA | 2003 | Platform | 0.01 | 0.00 | 0.00 | 0.00 | 69.0 | 7.7 | E | 0.01 | . 16714 | Winning Post 8 2016 | PSV | 2016 | Simulation | 0.00 | 0.00 | 0.01 | 0.00 | 70.0 | 7.5 | E | 0.01 | . 16715 rows × 12 columns . Name of columns were converted to lowercase. | name and genre missing values were placed by NaN (only 2 missing values) | year_of_release missing values were filled with median year_of_release per platform and then converted data type to int. | critic_score and user_score were filled with median values per genre. user_score&#39;s TBD had to be changed to np.nan value and then filled. user_score also underwent a data type change to float. There are 2 missing values left as NaN and these seem to connect to the 2 missing name and genre. I believe having these values remain NaN is okay for later analysis. | rating missing values were filled with the mode value per genre. | total_sales is the sum of sales in all the regions of that game. | . Exploratory Data Analysis . Number of games released in different years . def get_annotation(ax): &quot;&quot;&quot;Get annotation for graphs on top of bar graphs&quot;&quot;&quot; for p in ax.patches: ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.05)) . num_of_games_per_year = df_games.groupby([&#39;year_of_release&#39;])[&#39;name&#39;].agg(&#39;count&#39;) num_of_games_per_year . year_of_release 1980 9 1981 46 1982 53 1983 17 1984 14 1985 14 1986 21 1987 16 1988 15 1989 17 1990 16 1991 41 1992 43 1993 62 1994 121 1995 219 1996 263 1997 290 1998 386 1999 341 2000 350 2001 482 2002 829 2003 800 2004 783 2005 973 2006 1006 2007 1197 2008 1457 2009 1476 2010 1302 2011 1161 2012 653 2013 552 2014 582 2015 606 2016 502 Name: name, dtype: int64 . get_annotation(num_of_games_per_year.plot(kind=&#39;bar&#39;, figsize=(14,6))) plt.ylabel(&#39;number_of_games&#39;) plt.title(&quot;Number of games released per year&quot;) plt.grid(True) . In the 1980s to 1990s, there is a small amount of games released. Then there is a surge of games released passed 1994 where it seems to have an exponential growth of titles realeased. I have a feeling that this is due to release of home game consoles/platforms making more games more readily available. We can take a look at the consoles in each year to see if this is true. Though it&#39;s interesting as there is a number of games tapers off in 2011 and remains steady around 500-600 range of titles. . df_games[&#39;platform&#39;].unique() . array([&#39;Wii&#39;, &#39;NES&#39;, &#39;GB&#39;, &#39;DS&#39;, &#39;X360&#39;, &#39;PS3&#39;, &#39;PS2&#39;, &#39;SNES&#39;, &#39;GBA&#39;, &#39;PS4&#39;, &#39;3DS&#39;, &#39;N64&#39;, &#39;PS&#39;, &#39;XB&#39;, &#39;PC&#39;, &#39;2600&#39;, &#39;PSP&#39;, &#39;XOne&#39;, &#39;WiiU&#39;, &#39;GC&#39;, &#39;GEN&#39;, &#39;DC&#39;, &#39;PSV&#39;, &#39;SAT&#39;, &#39;SCD&#39;, &#39;WS&#39;, &#39;NG&#39;, &#39;TG16&#39;, &#39;3DO&#39;, &#39;GG&#39;, &#39;PCFX&#39;], dtype=object) . platform_per_year = df_games.groupby([&#39;year_of_release&#39;])[&#39;platform&#39;].nunique() platform_per_year . year_of_release 1980 1 1981 1 1982 1 1983 2 1984 2 1985 4 1986 2 1987 2 1988 4 1989 3 1990 4 1991 4 1992 6 1993 5 1994 10 1995 8 1996 8 1997 6 1998 7 1999 8 2000 9 2001 10 2002 8 2003 6 2004 7 2005 8 2006 10 2007 11 2008 9 2009 7 2010 7 2011 9 2012 9 2013 11 2014 10 2015 10 2016 9 Name: platform, dtype: int64 . get_annotation(platform_per_year.plot(kind=&#39;bar&#39;, figsize=(14,6))) plt.ylabel(&#39;number_of_platforms&#39;) plt.title(&quot;Number of platforms per year with game releases&quot;) plt.grid(True) . As we saw from above, there is a height in number of platforms in 1994. It&#39;s interesting to note that perhaps platforms past this point have end-of-life around 5-6 years as there seems to be a peak for number of plaforms with games released around that time frame. Though, interesting like we saw previous with the number of games released at around 2011-2012, the numbers past this point seem to stablize compared to previous pattern observed. . Change in sales per platform . df_games.groupby(&#39;platform&#39;)[&#39;total_sales&#39;].agg(sum).sort_values(ascending=False) . platform PS2 1255.77 X360 971.42 PS3 939.65 Wii 907.51 DS 806.12 PS 730.86 GBA 317.85 PS4 314.14 PSP 294.05 PC 259.52 3DS 259.00 XB 257.74 GB 255.46 NES 251.05 N64 218.68 SNES 200.04 GC 198.93 XOne 159.32 2600 96.98 WiiU 82.19 PSV 54.07 SAT 33.59 GEN 30.77 DC 15.95 SCD 1.86 NG 1.44 WS 1.42 TG16 0.16 3DO 0.10 GG 0.04 PCFX 0.03 Name: total_sales, dtype: float64 . Top selling platforms include: PS2, X360, PS3, Wii, DS, and PS. We can do a quick graph comparison of all platforms game sales over time just to have a quick overview and mental note of the comparison, but it would be wise just to focus on analysis of these top selling platforms. . def plot_bar(df, x, y, column=&#39;&#39;, value=&#39;&#39;, func=np.sum): if column != &#39;&#39; and value !=&#39;&#39;: filter_df = df[df[column] == value] plot_df = filter_df.pivot_table(index= x, values= y, aggfunc= func) values_to_plot = plot_df[y].values else: plot_df = df.pivot_table(index= x, values= y, aggfunc= func) values_to_plot = plot_df[y].values title = str(value) + &#39; - &#39; + str(y) + &#39; vs. &#39; + x ax = plot_df.plot(kind= &#39;bar&#39;, figsize=(14,6), rot=45, title=title, legend=False, grid=True) for p in ax.patches: ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.05)) . # Bar graph for all platforms versus the total sales of games per year for platform in df_games[&#39;platform&#39;].unique(): plot_bar(df_games, &#39;year_of_release&#39;, &#39;total_sales&#39;, &#39;platform&#39;, platform) plt.ylabel(&#39;total_sales&#39;) . /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). fig = self.plt.figure(figsize=self.figsize) . . Wii sales title released started in 2006 but then declined around 2011 and really died down passed that point. . | NES title sales started around 1983 though the sales took off in 1984 spiked in 1985 and then again died down and spiked again in 1988 but died down after that. . | GB title sales started in 1988 but really took off the next year in 1989 but sales seemed to declined after this point (might be due to no new game releases). Sales start increase in 1992 but again there is a decline until 1996 where this is another surge. However, it declines again and then increases for the interval from 1998 to 1999 and declines and stops at 2001. It should be noted that there is no distinction on our data for GB versus GB Color (since both are the same plaform but one was a later release with color resolution). This might explain the long life of sales for this platform. . | DS game sales starts to take off in 2004 and peaks in 2005 staying fairly constant until 2010 when sales start to decline and it really dies off in 2013. . | X360 game sales start in 2005 but really start climbing in 2006 and peaks around 2010 and games sales after this have gradual decline after this point. . | PS3 game sales start at 2006 and climb and peak at 2011 and slowly declines with sales really dropping after 2014. . | PS2 game sales start in 2000 then stay pretty high in the period between 2001 to 2005. After this point, game sales for the PS2 start really declining in 2006 and stop after 2011. . | SNES game sales start in 1990 and staky fairly constant and peak in 1993. Game sales for SNES start to decline in 1996 and really drop off after this and ends in 1999. . | GBA game sales start in 2000 but really take off in 2001. It stays pretty constant until 2005 where it really starts to decline and sales stop at 2007. . | PS4 game sales start in 2013 but takes off in 2014 and is still pretty constant with our data, however, we can&#39;t see past 2016 to really see when sales will start to decline for this the PS4. . | 3DS game sales start off strong in 2011 and there is a slight decline after 2015. . | N64 game sales start in 1996 (which make sense with the decline of SNES in 1996) and take off peaking at 1999. There is decline after 2000 and sales stop after 2002. . | PS game sales start in 1994 and start a gradual climb and peak in 1998 and slowly declines after this point with a sales really stopping in 2003. . | . . Quick analysis from the graphs consoles seem to have peak and decline around 4-5 years. General exceptions for these are hand-held consoles (GB, GBA, DS, PSV) and PC and earlier consoles. Platforms/consoles have sales past 4-5 year point might have game titles released on both newer and older platforms. Though from the data, the top 4 selling games releases throughout the years were on the following platforms: PS2, X360, PS3, Wii, DS, and PS . Determining the actual period with leading platforms . Though we know the platforms with the leading game sales of all time from our data consists of: PS2, X360, PS3, Wii, DS, and PS, these platforms wouldn&#39;t really show up for our prognosis for 2017 since these are considered end-of-life already. Though looking at their patterns we can try to forcast the sales for the currently/lastest leading platforms. . Knowing that the lastest platforms seemed to be released around 2013, we should take a look around this period and use the data from the former leading platforms to make a prognosis for 2017. . df_period = df_games[(df_games[&#39;year_of_release&#39;] &gt;= 2013) &amp; (df_games[&#39;year_of_release&#39;].notnull())] df_period.pivot_table(index=&#39;platform&#39;, values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;).sort_values(by=&#39;total_sales&#39;, ascending=False) . total_sales . platform . PS4 | 314.14 | . PS3 | 181.43 | . XOne | 159.32 | . 3DS | 144.44 | . X360 | 136.80 | . WiiU | 64.63 | . PC | 39.43 | . PSV | 33.25 | . Wii | 13.66 | . PSP | 3.50 | . DS | 1.54 | . Starting in from 2013 until 2016, it seems like the platform with leading in game sales is PS4. PS3, though coming in second on this list, we need to keep in mind that the sales for this should be in decline do the newer PS4 release. XOne should follow the same pattern with X360 falling behind in sales with the release of the new platform. Same with 3DS and the DS, WiiU and Wii, PSV and PSP. We should see a decline in sales for the older platforms in 2017. PC is the only platform that would not have newer releases for the platform, though the sales for PC do vary, this might be because of number of title releases for the PC or so other underlying cause. . # Newer platforms df_ps4 = df_games.query(&#39;platform == &quot;PS4&quot; &amp; year_of_release &gt;= 2013&#39;) df_xone = df_games.query(&#39;platform == &quot;XOne&quot; &amp; year_of_release &gt;= 2013&#39;) df_3ds = df_games.query(&#39;platform == &quot;3DS&quot; &amp; year_of_release &gt;= 2013&#39;) df_wiiu = df_games.query(&#39;platform == &quot;WiiU&quot; &amp; year_of_release &gt;= 2013&#39;) df_psv = df_games.query(&#39;platform == &quot;PSV&quot; &amp; year_of_release &gt;= 2013&#39;) # Older platforms df_ps3 = df_games.query(&#39;platform == &quot;PS3&quot; &amp; year_of_release &gt;= 2013&#39;) df_x360 = df_games.query(&#39;platform == &quot;X360&quot; &amp; year_of_release &gt;= 2013&#39;) df_ds = df_games.query(&#39;platform == &quot;DS&quot; &amp; year_of_release &gt;= 2013&#39;) df_wii = df_games.query(&#39;platform == &quot;Wii&quot; &amp; year_of_release &gt;= 2013&#39;) df_psp = df_games.query(&#39;platform == &quot;PSP&quot; &amp; year_of_release &gt;= 2013&#39;) . plot_bar(df_ps4,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PS4 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_ps3,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PS3 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . PS4 seemed to have been introduced in 2013 where PS3 seems to be in a height of sales but as expected the sales for PS3 decrease over time and the sames should decrease even more or be close to non-existent in 2017. PS4 sale should be on the raise but since it seems like 2016 data is not complete it&#39;s hard to determine if PS4 sales will spike in 2017. . plot_bar(df_xone,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;XOne - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_x360,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;X360 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what we see with PS4 vs PS3, XOne and X360 follow the same pattern. With sales for X360 high in 2013 but with the introduction of XOne in 2013 there is a raise in sales for XOne and decline for X360. By 2017, X360 should be similar to PS3 sales. Sales for XOne in 2017 should increase too for 2017 or have as similar raise to that of PS4. . plot_bar(df_3ds,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;3DS - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_ds,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;DS - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . It seems as though the hand-held platforms here have a different pattern. The 3DS seems to have peaked sales in 2013 but is on the decline versus the DS which seems like was on it&#39;s decline and stopped in 2013 with no data after. for 3DS sales in 2017, it seems like it might decrease a bit more. . plot_bar(df_wiiu,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;WiiU - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_wii,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;Wii - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what was seen with the PS4 vs PS3 and XOne vs X360, WiiU and Wii are following the same pattern with the decline of Wii starting in 2013 and the raise of WiiU. Though it seems like WiiU is declining and this might be do to lack of new title releases for the platform. Though for Wii in 2017, it seems like the platform will cease sales. . plot_bar(df_psv,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PSV - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_psp,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PSV - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what we have seen with newer platforms being introduced, the PSP sales decreased after 2013 and ceased in after 2015. PSV has a slight decline as well but this might be do to lack of new title realeses for the platform. Interesting enough, this is also a hand-held platform and from what was seen above with 3DS vs DS it seems like the older platform sales for these really do come to a quick stop in sales after introduction of a new platform versus the other platforms. . . The leading platforms that we would see sales in 2017 are: PS4, XOne, 3DS, WiiU, PSV, and PC. These platforms are the lastest and most current platforms to date. So long as new game titles are released we should see the sales stay stable or grow. With anticipation of new platforms, ever 5-6 years or so we need to keep in mind that these platforms were newly introduced around 2013 so close to 2017 we might see a decrease in sales as well as we are past the point of peaking sales. Thought the hand-held consoles (PSV and 3DS) it&#39;s hard to determine their end-of-life as they vary slightly compared to the other platforms. PC sales will stay mostly consistent as there is no threat of new platforms. . The platforms that have reached their end-of-life (shrinking in sales) are: PS3, X360, and Wii. As we saw from above, the introduction of new platforms brought the decline to these older platforms. For the hand-held platforms (PSP and DS) sales declined and came to a halt much more sudden than the other platforms. Since these older platforms still have little sales, we might see a halt of sales in 2017. . Distribution of global sales by platform . plt.figure(figsize=(30,10)) ax = sns.boxplot(x=df_games[&#39;total_sales&#39;], y=df_games[&#39;platform&#39;]) ax.yaxis.grid(True) # Show the horizontal gridlines ax.xaxis.grid(True) # Show the vertical gridlines ax.xaxis.set_major_locator(ticker.MultipleLocator(.1)) ax.xaxis.set_major_formatter(ticker.ScalarFormatter()) ax.set(xlim=(0, 4)) . [(0, 4)] . There is a difference in range for all the game sales per platform across the globe.It seems like the median for NES game sales is the highest at about 1.38 million USD compared to the other platforms. GB game median game sales comes second next to the NES at about 1.16 million USD. There are some game sales that are significantly outside quartile on the right. These games must be best sellers to have a big right skew outside the quartile. . display(df_games.sort_values(by=&#39;total_sales&#39;, ascending=False).head(10)) . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | 69.0 | 7.7 | E | 40.24 | . 2 | Mario Kart Wii | Wii | 2008 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | 74.0 | 7.8 | T | 31.38 | . 5 | Tetris | GB | 1989 | Puzzle | 23.20 | 2.26 | 4.22 | 0.58 | 70.0 | 7.5 | E | 30.26 | . 6 | New Super Mario Bros. | DS | 2006 | Platform | 11.28 | 9.14 | 6.50 | 2.88 | 89.0 | 8.5 | E | 29.80 | . 7 | Wii Play | Wii | 2006 | Misc | 13.96 | 9.18 | 2.93 | 2.84 | 58.0 | 6.6 | E | 28.91 | . 8 | New Super Mario Bros. Wii | Wii | 2009 | Platform | 14.44 | 6.94 | 4.70 | 2.24 | 87.0 | 8.4 | E | 28.32 | . 9 | Duck Hunt | NES | 1984 | Shooter | 26.93 | 0.63 | 0.28 | 0.47 | 73.0 | 7.4 | M | 28.31 | . Wii Sports seems to be the top selling game overall from our data at 82.54 million USD on the Wii platform. Super Mario Bros comes in second at 40.24 million USD on the NES. . pd.pivot_table(df_games, values=&#39;total_sales&#39;, index=&#39;platform&#39;, aggfunc={&#39;sum&#39; , &#39;median&#39;, &#39;mean&#39;}).sort_values(by=&#39;mean&#39;, ascending=False) . mean median sum . platform . GB | 2.606735 | 1.165 | 255.46 | . NES | 2.561735 | 1.375 | 251.05 | . GEN | 1.061034 | 0.150 | 30.77 | . SNES | 0.836987 | 0.320 | 200.04 | . PS4 | 0.801378 | 0.200 | 314.14 | . X360 | 0.769746 | 0.280 | 971.42 | . 2600 | 0.729173 | 0.460 | 96.98 | . PS3 | 0.705973 | 0.270 | 939.65 | . Wii | 0.687508 | 0.190 | 907.51 | . N64 | 0.685517 | 0.270 | 218.68 | . XOne | 0.645020 | 0.220 | 159.32 | . PS | 0.610576 | 0.260 | 730.86 | . PS2 | 0.581106 | 0.230 | 1255.77 | . WiiU | 0.559116 | 0.220 | 82.19 | . 3DS | 0.498077 | 0.120 | 259.00 | . GBA | 0.386679 | 0.160 | 317.85 | . DS | 0.374765 | 0.110 | 806.12 | . GC | 0.357788 | 0.150 | 198.93 | . XB | 0.312791 | 0.150 | 257.74 | . SCD | 0.310000 | 0.065 | 1.86 | . DC | 0.306731 | 0.135 | 15.95 | . PC | 0.266448 | 0.050 | 259.52 | . PSP | 0.243218 | 0.090 | 294.05 | . WS | 0.236667 | 0.215 | 1.42 | . SAT | 0.194162 | 0.120 | 33.59 | . PSV | 0.125744 | 0.055 | 54.07 | . NG | 0.120000 | 0.100 | 1.44 | . TG16 | 0.080000 | 0.080 | 0.16 | . GG | 0.040000 | 0.040 | 0.04 | . 3DO | 0.033333 | 0.020 | 0.10 | . PCFX | 0.030000 | 0.030 | 0.03 | . GB platform has the highest mean value at 2.6 million USD and like we saw above with the boxplot the second highest median value at 1.65 million USD. NES comes in second with the highest mean value of 2.56 million USD and the highest overall median at 1.37 million USD. Though, the highest selling game sales is found in PS2 platform at 1255.77 million USD but the mean value for this is 0.58 million USD and median is 0.23 million USD. The distribution for these values are pretty interesting in terms of sum of top selling versus the mean and median values. . . Overall, the median values are lower compared to the mean values for game sales per platform. . impact of user and critic feedback on sales . wii = df_games.query(&#39;platform == &quot;Wii&quot;&#39;) print(&#39;***critic_score median:&#39;, wii[&#39;critic_score&#39;].median()) display(wii[&#39;critic_score&#39;].describe()) print(&#39; n***user_score median:&#39;, wii[&#39;user_score&#39;].median()) display(wii[&#39;user_score&#39;].describe()) . ***critic_score median: 69.0 . count 1320.000000 mean 67.103030 std 10.823695 min 19.000000 25% 66.000000 50% 69.000000 75% 73.000000 max 97.000000 Name: critic_score, dtype: float64 . ***user_score median: 7.4 . count 1320.000000 mean 7.107197 std 1.183492 min 0.200000 25% 7.100000 50% 7.400000 75% 7.600000 max 9.300000 Name: user_score, dtype: float64 . The critic_score and user_score are close in mean and median values if we scale the critic_score to match user_score. Overall, the user_score seems to have a higher scoring compared to the critic_score. . wii[&#39;critic_score_scale&#39;] = wii[&#39;critic_score&#39;] / 10.0 ax = wii.plot.scatter(x=&#39;user_score&#39;, y=&#39;total_sales&#39;, label=&#39;user_score&#39;, figsize=(20,8), alpha=0.4) wii.plot.scatter(x=&#39;critic_score_scale&#39;, y=&#39;total_sales&#39;, color=&#39;DarkGreen&#39;, label=&#39;critic_score&#39;, ax=ax, alpha=0.4) plt.legend() plt.grid(True) plt.xlabel(&#39;scoring&#39;) plt.title(&quot;critic_score and user_score on the Wii per total sales&quot;) . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Text(0.5, 1.0, &#39;critic_score and user_score on the Wii per total sales&#39;) . Looking at the scatter plot it doesn&#39;t seem like there is a linear correlation relationship between scoring and sales. Though, we do see a huge rating around 7-8 scoring range where there is a spike in total_sales but this seems like it&#39;s only for a certain game title as sales don&#39;t remain this high as we move higher on the scoring. From the scatter plot, there is a slight increase in total_sales with increase of rating/scoring with both critic and user, which would make sense as popular game titles I imagine would have higher ratings/scoring from both critics and users. . print(&#39;critic_score correlation calculation:&#39;, &#39;{:.1%}&#39;.format(wii[&#39;critic_score&#39;].corr(wii[&#39;total_sales&#39;]))) print(&#39;user_score correlation calculation:&#39;, &#39;{:.1%}&#39;.format(wii[&#39;user_score&#39;].corr(wii[&#39;total_sales&#39;]))) . critic_score correlation calculation: 11.1% user_score correlation calculation: 6.2% . There doesn&#39;t seem to be a significant correlation between scoring/rating and sales from the linear correlation calculation. Though critic_score seems to have a higher influence than user_score. . . Overall, critic_score and user_score doesn&#39;t seem to have a linear correlation with total_sales. Though from our scatter plot, it does seem like there is a relationship with scoring and total sales as it seeems like there is a higher density of total sales with higher scoring/rating. critic_score seems to have a higher influence in total_sales compared to user_score. . Compare the sales of the same games on other platforms . cross_platform = set(df_period.groupby([&#39;name&#39;]).filter(lambda x: x.shape[0] &gt; 1)[&#39;name&#39;]) #List of unique games that are cross-platform df_popular_games = df_period.query(&#39;name in @cross_platform&#39;).groupby([&#39;name&#39;, &#39;platform&#39;]).agg({&#39;total_sales&#39;: &#39;sum&#39;}) df_popular_games.sort_values(by=&#39;total_sales&#39;, ascending=False, inplace=True) df_popular_games.query(&#39;total_sales &gt; 6&#39;).head(10) . total_sales . name platform . Grand Theft Auto V | PS3 | 21.05 | . X360 | 16.27 | . Call of Duty: Black Ops 3 | PS4 | 14.63 | . Grand Theft Auto V | PS4 | 12.62 | . Call of Duty: Ghosts | X360 | 10.24 | . PS3 | 9.36 | . Minecraft | X360 | 9.18 | . FIFA 16 | PS4 | 8.58 | . Star Wars Battlefront (2015) | PS4 | 7.98 | . Call of Duty: Advanced Warfare | PS4 | 7.66 | . Across platforms starting from 2013, PS4 seems to be the most popular platfrom and second to that is X360. Most popular being Grand Theft Auto V on PS3, X360, and PS4. All other games seem to fall for PS4 for popularity. . Distribution of games by genre . genres = df_period.groupby(&#39;genre&#39;).agg({&#39;total_sales&#39;: &#39;sum&#39;}).sort_values(by=&#39;total_sales&#39;, ascending=False) plot_bar(genres,&#39;genre&#39;, &#39;total_sales&#39;) plt.title(&#39;total_sales vs genre&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . From the start of 2013, the most popular genre of game sold overall is Action. Second would be Shooter and then Sport and Role-Playing come close to thrid and fourth. . for year in [2013 + i for i in range(4)]: plot_bar(df_period, &#39;genre&#39;, &#39;total_sales&#39;, column=&#39;year_of_release&#39;, value=year, func=&#39;count&#39;) plt.ylabel(&#39;total_sales&#39;) . 2013 the most popular genre was Action followed by Role-Playing with Adventure and Shooter close in sales. . | 2014 similar to what was seen in 2013, Action was the top selling genre with Role-Playing falling second and Adventure being in the top 3. Sports came in at fourth place. . | 2015 was by far the top selling genre with count sold being over 200 which is much different from what we saw previously. . | 2016 has a similar pattern to 2013 and 2014 with Action being top selling. . | . . Action is the top selling genre overall. All other genre seem to stay moderately the same with sales though Shooter seems to fluctuate along with Adventure throughout the changing years. This might be due to number of releases per year per genre. Puzzle remains the lowest selling genre overall. . Statistical Data Analysis . Create a user profile for each region . regions = [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;] for region in regions: print(region.title()) display(df_period.pivot_table(index=&#39;platform&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) display(df_period.pivot_table(index=&#39;genre&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) display(df_period.pivot_table(index=&#39;rating&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) print(&#39;******&#39;*10, &#39; n&#39;) . Na_Sales . na_sales . platform . PS4 | 108.74 | . XOne | 93.12 | . X360 | 81.66 | . PS3 | 63.50 | . 3DS | 39.04 | . na_sales . genre . Action | 126.23 | . Shooter | 109.78 | . Sports | 65.27 | . Role-Playing | 46.51 | . Misc | 27.49 | . na_sales . rating . M | 184.77 | . E | 102.49 | . T | 96.90 | . E10+ | 54.50 | . ************************************************************ Eu_Sales . eu_sales . platform . PS4 | 141.09 | . PS3 | 67.81 | . XOne | 51.59 | . X360 | 42.52 | . 3DS | 31.17 | . eu_sales . genre . Action | 118.32 | . Shooter | 87.88 | . Sports | 60.52 | . Role-Playing | 37.02 | . Racing | 20.19 | . eu_sales . rating . M | 162.21 | . E | 108.39 | . T | 79.01 | . E10+ | 42.88 | . ************************************************************ Jp_Sales . jp_sales . platform . 3DS | 67.86 | . PS3 | 23.35 | . PSV | 18.66 | . PS4 | 15.96 | . WiiU | 10.88 | . jp_sales . genre . Role-Playing | 51.14 | . Action | 40.49 | . Misc | 9.20 | . Fighting | 7.65 | . Shooter | 6.61 | . jp_sales . rating . T | 86.72 | . E | 33.37 | . M | 14.92 | . E10+ | 5.89 | . ************************************************************ Other_Sales . other_sales . platform . PS4 | 48.35 | . PS3 | 26.77 | . XOne | 14.27 | . X360 | 12.11 | . 3DS | 6.37 | . other_sales . genre . Action | 37.23 | . Shooter | 28.78 | . Sports | 19.45 | . Role-Playing | 11.51 | . Misc | 6.09 | . other_sales . rating . M | 52.82 | . E | 28.97 | . T | 25.69 | . E10+ | 12.61 | . ************************************************************ . NA region: . For the period starting from 2013, PS4 is the top selling platform with XOne, X360 and PS3 trailing behind and 3DS being at the bottom. | Action being to top genre, and Shooter, Sports, and Role-Playing falling behind. | M rating is top rating with E and T behind that. | . EU region: . For the period starting from 2013, PS4 is the top selling platform with PS3, XOne, X360 and 3DS falling behind in that order. | Top selling genres are similar order to NA with Action being the best genre, Shooter, Sports and Role-Playing falling behind. Different from NA though is Racing genre rather than Misc genre compared to NA. | Same pattern as we saw with NA the rating comes with M being top selling and E and T falling behind that. | . JP region: . Different from what we see with NA and EU the top selling platform in JP is 3DS. PS3, PSV, PS4, and WiiU falling behind the 3DS which is much different from NA and EU region. The hand-held platforms are much more dominate in JP and no XOne or X360 found in sales. | Another difference we see here compared to NA and EU region with genre, Role-Playing is the top selling genre with Action, Misc, Fighting, and Shooter on the bottom. | Another difference from NA and EU region is with the rating in JP, T is the top selling genre and E and M being behind that. | . Other region: . Similar to what NA and EU region, PS4 is the top selling platform. PS3, XOne, X360, and 3DS trailing behind. | Again similar to NA and EU region, Action is the top selling genre. Following behind is Shooter, Sports, Role-Playing, and Misc. | Same pattern we see with NA rating, M is the top selling rating of games and E and T follow behind M. | . . Interesting that most of the regions have similar patterns for top selling platforms, genre, and rating except for JP. JP doesn&#39;t even have the XBox series platform in the top selling. Also, having 3DS and PSV as top selling, it seems like hand-held platforms are more popular in JP than other regions. Genre is different in JP as well compared to other regions and again this might be do to 3DS being the top selling. . Hypotheses testing . Null hypothesis vs Alternative hypothesis and Significance level . Null hypothesis is believed to a be a true statement. The alternative hypothesis is formulated to disprove the null hypothesis with at least one statisical representation example that makes the null hypothesis false. Since the null hypothesis would be false it has to be rejected. . Alternative hypothesis is usually the mathematical opposite of the null hypothesis. . We choose a significance level of 0.05 which indicates a 5% risk of concluding that a difference exists when there is no actual difference. Anything higher seems like it would be risky. . Testing if average user ratings of the Xbox One and PC platforms are the same. . Null hypothesis: The average user rating of Xbox One and PC platforms do not differ. . Alternative hypothesis: The average user rating of Xbox One and PC platforms do differ. . . The alternative hypothesis will be tested to disprove the null hypothesis. . What to test: . Compare the averages/means user rating from both XOne and PC, if they are off by an alpha (that we specify) this means we could reject the null hypothesis. | . xone_user_rating = df_games.query(&#39;platform == &quot;XOne&quot;&#39;)[&#39;user_score&#39;] pc_user_rating = df_games.query(&#39;platform == &quot;PC&quot;&#39;)[&#39;user_score&#39;] print(&quot;Average user rating for XOne: &quot;, xone_user_rating.mean(), &quot; nAverage user rating for PC: &quot;, pc_user_rating.mean()) . Average user rating for XOne: 6.7615384615384615 Average user rating for PC: 7.16170431211499 . alpha = 0.05 #critical statistical significance results = st.ttest_ind( xone_user_rating, pc_user_rating) print(&#39;p-value: &#39;, results.pvalue, &#39; nalpha: &#39;, alpha) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 3.5341366694379986e-05 alpha: 0.05 We reject the null hypothesis . Mathematically, it was proven that the average user rating from XOne and PC do differ. The t-test confirms that it appears that these ratings are differ. . With means of: . Average user rating for XOne: 6.761538461538461 | Average user rating for PC: 7.161704312115005 | . Testing if average user ratings for the Action and Sports genres are different. . Null hypothesis: The average user ratings for the Action and Sports genres do not differ. . Alternative hypothesis: The average user ratings for the Action and Sports genres do differ. . . The alternative hypothesis will be tested to disprove the null hypothesis. . What to test: . Compare the averages/means user rating from both Action and Sports genres, if they are off by an alpha (that we specify) this means we could reject the null hypothesis. | . action_user_rating = df_games.query(&#39;genre == &quot;Action&quot;&#39;)[&#39;user_score&#39;] sports_user_rating = df_games.query(&#39;genre == &quot;Sports&quot;&#39;)[&#39;user_score&#39;] print(&quot;Average user rating for Action genre: &quot;, action_user_rating.mean(), &quot; nAverage user rating for Sports genre: &quot;, sports_user_rating.mean()) . Average user rating for Action genre: 7.21208073612348 Average user rating for Sports genre: 7.193867120954002 . alpha = 0.05 #critical statistical significance results = st.ttest_ind( action_user_rating, sports_user_rating) print(&#39;p-value: &#39;, results.pvalue, &#39; nalpha: &#39;, alpha) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.5342570517131008 alpha: 0.05 We can&#39;t reject the null hypothesis . Mathematically, it was proven that the average user rating from Action genre and Sports do not differ, since we are not able to reject the null hypothesis. The t-test confirms that it appears that these ratings are similar. . With means of: . Average user rating for Action genre: 7.212080736123694 | Average user rating for Sports genre: 7.193867120953876 | . The average/means are pretty similar between Action and Sports genres. . . With the t-test and an alpha of 0.05, we were able to prove that the average user rating from XOne and PC do differ and also with seperate t-test we concluded that the average user rating from Action genre and Sports do not differ. . Conclusion . critic_score and user_score had lots of missing values which may have caused influences in the analysis with how we decided to fill in those values. . | Using similar genre with the game titles, we were able to generate a relevant rating for the game titles that had missing values. . | Overall, PS2 had the highest selling games titles for it&#39;s platform with the data provided. . | The typical lifespan of a platform ranges from 5-6 years, as this seems to be where new platforms are announced and introduced to the market. Then it takes about 2-3 years for the older platform to become irrelevant after the introduction to the newer platform. . | Action is the top selling genre of games overall, though it gets interesting when we take a closer look at the regions where it seems to differ in some regions. . | NA Region: the top selling platform is PS4, top selling genre is Action, and top selling rating of games is M. . | EU Region: the top selling platform is PS4, top selling genre is Action, and top selling rating of games is M. . | JP Region: the top selling platform is 3DS, top selling genre is Role-Playing, and top selling rating of games is T followed behind by E. . | . . It&#39;s interesting overall to see that hand-held consoles are more popular in Japan versus other places in the world and also that the XBox series is not targeted towards those in Japan at all but is everywhere else in the world. Also, with the rating being M being top selling in NA and EU it does seem like shooter games are more popular in these regions vs what is seen in JP. This seems like there is a cultural influence with game sells and their target audience. .",
            "url": "https://cmdang-mochi.github.io/ds-projects/python/exploratory%20analysis/statistical%20analysis/data%20preprocessing/product%20analysis/pandas/numpy/mathplotlib/scripy/seaborn/2020/10/23/successful_video_games_sales_trends.html",
            "relUrl": "/python/exploratory%20analysis/statistical%20analysis/data%20preprocessing/product%20analysis/pandas/numpy/mathplotlib/scripy/seaborn/2020/10/23/successful_video_games_sales_trends.html",
            "date": " • Oct 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome! . This website displays my project portfolio Connie Dang 1. . check out my Linkedin page. &#8617; . |",
          "url": "https://cmdang-mochi.github.io/ds-projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cmdang-mochi.github.io/ds-projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}