{
  
    
        "post0": {
            "title": "Gold Recovery Optimization",
            "content": "Project Description . Zyfra develops solutions for the efficient operation of industrial plants. The company needs a machine learning model to predict the recovery rate of gold from gold ore. . Optimize the work of a gold-mining company by predicting amount of gold extracted. . Train a model which can predict the amount of gold extracted from gold ore based on extraction and purification data. . The data is stored in three files: . gold_recovery_train.csv — training dataset | gold_recovery_test.csv — test dataset | gold_recovery_full.csv — source dataset | . Data is indexed with the date and time of acquisition (date feature). Parameters that are next to each other in terms of time are often similar. Some parameters are not available because they were measured and/or calculated much later. That&#39;s why, some of the features that are present in the training set may be absent from the test set. The test set also doesn&#39;t contain targets. . The source dataset contains the training and test sets with all the features. . Technological process . . When the mined ore undergoes primary processing, a crushed mixture is obtained. It is sent for flotation (enrichment) and two-stage purification: . Flotation - A mixture of gold-bearing ore is fed into the flotation plant. After beneficiation, a rough concentrate and &quot;tailings&quot;, i.e. product leftovers with a low concentration of valuable metals, are obtained. The stability of this process is affected by the inconsistent and non-optimal physical and chemical state of the flotation slurry (mixture of solid particles and liquid). . | Purification - The crude concentrate undergoes two purifications. The output is the final concentrate and new tailings. . | Data description . Technological process . Rougher feed — raw material | Rougher additions (or reagent additions) — flotation reagents: Xanthate, Sulphate, Depressant Xanthate — promoter or flotation activator; | Sulphate — sodium sulphide for this particular process; | Depressant — sodium silicate. | . | Rougher process — flotation | Rougher tails — product residues | Float banks — flotation unit | Cleaner process — purification | Rougher Au — rougher gold concentrate | Final Au — final gold concentrate | . Parameters of stages . air amount — volume of air | fluid levels | feed size — feed particle size | feed rate | . Feature naming . [stage].[parameter_type].[parameter_name] Example: rougher.input.feed_ag . Possible values for [stage]: . rougher — flotation | primary_cleaner — primary purification | secondary_cleaner — secondary purification | final — final characteristics | . Possible values for [parameter_type]: . input — raw material parameters | output — product parameters | state — parameters characterizing the current state of the stage | calculation — calculation characteristics | . . Import Libraries . #collapse-hide import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy import stats as st from sklearn.metrics import mean_absolute_error, make_scorer from sklearn.model_selection import cross_val_score, GridSearchCV, KFold from sklearn.multioutput import MultiOutputRegressor from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor RANDOM_STATE = 42 . . Exploratory data analysis . Load Data . df_train = pd.read_csv(&#39;/datasets/gold_recovery_train.csv&#39;) df_test = pd.read_csv(&#39;/datasets/gold_recovery_test.csv&#39;) df = pd.read_csv(&#39;/datasets/gold_recovery_full.csv&#39;) . #collapse-hide def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . . get_information(df) . Head: . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22627 non-null float64 final.output.concentrate_pb 22629 non-null float64 final.output.concentrate_sol 22331 non-null float64 final.output.concentrate_au 22630 non-null float64 final.output.recovery 20753 non-null float64 final.output.tail_ag 22633 non-null float64 final.output.tail_pb 22516 non-null float64 final.output.tail_sol 22445 non-null float64 final.output.tail_au 22635 non-null float64 primary_cleaner.input.sulfate 21107 non-null float64 primary_cleaner.input.depressant 21170 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 21565 non-null float64 primary_cleaner.output.concentrate_ag 22618 non-null float64 primary_cleaner.output.concentrate_pb 22268 non-null float64 primary_cleaner.output.concentrate_sol 21918 non-null float64 primary_cleaner.output.concentrate_au 22618 non-null float64 primary_cleaner.output.tail_ag 22614 non-null float64 primary_cleaner.output.tail_pb 22594 non-null float64 primary_cleaner.output.tail_sol 22365 non-null float64 primary_cleaner.output.tail_au 22617 non-null float64 primary_cleaner.state.floatbank8_a_air 22660 non-null float64 primary_cleaner.state.floatbank8_a_level 22667 non-null float64 primary_cleaner.state.floatbank8_b_air 22660 non-null float64 primary_cleaner.state.floatbank8_b_level 22673 non-null float64 primary_cleaner.state.floatbank8_c_air 22662 non-null float64 primary_cleaner.state.floatbank8_c_level 22673 non-null float64 primary_cleaner.state.floatbank8_d_air 22661 non-null float64 primary_cleaner.state.floatbank8_d_level 22673 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22672 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.au_pb_ratio 21089 non-null float64 rougher.input.feed_ag 22618 non-null float64 rougher.input.feed_pb 22472 non-null float64 rougher.input.feed_rate 22163 non-null float64 rougher.input.feed_size 22277 non-null float64 rougher.input.feed_sol 22357 non-null float64 rougher.input.feed_au 22617 non-null float64 rougher.input.floatbank10_sulfate 21415 non-null float64 rougher.input.floatbank10_xanthate 22247 non-null float64 rougher.input.floatbank11_sulfate 22038 non-null float64 rougher.input.floatbank11_xanthate 20459 non-null float64 rougher.output.concentrate_ag 22618 non-null float64 rougher.output.concentrate_pb 22618 non-null float64 rougher.output.concentrate_sol 22526 non-null float64 rougher.output.concentrate_au 22618 non-null float64 rougher.output.recovery 19597 non-null float64 rougher.output.tail_ag 19979 non-null float64 rougher.output.tail_pb 22618 non-null float64 rougher.output.tail_sol 19980 non-null float64 rougher.output.tail_au 19980 non-null float64 rougher.state.floatbank10_a_air 22646 non-null float64 rougher.state.floatbank10_a_level 22647 non-null float64 rougher.state.floatbank10_b_air 22646 non-null float64 rougher.state.floatbank10_b_level 22647 non-null float64 rougher.state.floatbank10_c_air 22646 non-null float64 rougher.state.floatbank10_c_level 22654 non-null float64 rougher.state.floatbank10_d_air 22641 non-null float64 rougher.state.floatbank10_d_level 22649 non-null float64 rougher.state.floatbank10_e_air 22096 non-null float64 rougher.state.floatbank10_e_level 22649 non-null float64 rougher.state.floatbank10_f_air 22641 non-null float64 rougher.state.floatbank10_f_level 22642 non-null float64 secondary_cleaner.output.tail_ag 22616 non-null float64 secondary_cleaner.output.tail_pb 22600 non-null float64 secondary_cleaner.output.tail_sol 20501 non-null float64 secondary_cleaner.output.tail_au 22618 non-null float64 secondary_cleaner.state.floatbank2_a_air 22333 non-null float64 secondary_cleaner.state.floatbank2_a_level 22591 non-null float64 secondary_cleaner.state.floatbank2_b_air 22538 non-null float64 secondary_cleaner.state.floatbank2_b_level 22588 non-null float64 secondary_cleaner.state.floatbank3_a_air 22585 non-null float64 secondary_cleaner.state.floatbank3_a_level 22587 non-null float64 secondary_cleaner.state.floatbank3_b_air 22592 non-null float64 secondary_cleaner.state.floatbank3_b_level 22590 non-null float64 secondary_cleaner.state.floatbank4_a_air 22571 non-null float64 secondary_cleaner.state.floatbank4_a_level 22587 non-null float64 secondary_cleaner.state.floatbank4_b_air 22608 non-null float64 secondary_cleaner.state.floatbank4_b_level 22607 non-null float64 secondary_cleaner.state.floatbank5_a_air 22615 non-null float64 secondary_cleaner.state.floatbank5_a_level 22615 non-null float64 secondary_cleaner.state.floatbank5_b_air 22615 non-null float64 secondary_cleaner.state.floatbank5_b_level 22616 non-null float64 secondary_cleaner.state.floatbank6_a_air 22597 non-null float64 secondary_cleaner.state.floatbank6_a_level 22615 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . None . **************************************************************************************************** Describe: . final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au primary_cleaner.input.sulfate ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 22627.000000 | 22629.000000 | 22331.000000 | 22630.000000 | 20753.000000 | 22633.000000 | 22516.000000 | 22445.000000 | 22635.000000 | 21107.000000 | ... | 22571.000000 | 22587.000000 | 22608.000000 | 22607.000000 | 22615.000000 | 22615.000000 | 22615.000000 | 22616.000000 | 22597.000000 | 22615.000000 | . mean | 4.781559 | 9.095308 | 8.640317 | 40.001172 | 67.447488 | 8.923690 | 2.488252 | 9.523632 | 2.827459 | 140.277672 | ... | 18.205125 | -499.878977 | 14.356474 | -476.532613 | 14.883276 | -503.323288 | 11.626743 | -500.521502 | 17.976810 | -519.361465 | . std | 2.030128 | 3.230797 | 3.785035 | 13.398062 | 11.616034 | 3.517917 | 1.189407 | 4.079739 | 1.262834 | 49.919004 | ... | 6.560700 | 80.273964 | 5.655791 | 93.822791 | 6.372811 | 72.925589 | 5.757449 | 78.956292 | 6.636203 | 75.477151 | . min | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000003 | ... | 0.000000 | -799.920713 | 0.000000 | -800.836914 | -0.423260 | -799.741097 | 0.427084 | -800.258209 | -0.079426 | -810.473526 | . 25% | 4.018525 | 8.750171 | 7.116799 | 42.383721 | 63.282393 | 7.684016 | 1.805376 | 8.143576 | 2.303108 | 110.177081 | ... | 14.095940 | -500.896232 | 10.882675 | -500.309169 | 10.941299 | -500.628697 | 8.037533 | -500.167897 | 13.968418 | -500.981671 | . 50% | 4.953729 | 9.914519 | 8.908792 | 44.653436 | 68.322258 | 9.484369 | 2.653001 | 10.212998 | 2.913794 | 141.330501 | ... | 18.007326 | -499.917108 | 14.947646 | -499.612292 | 14.859117 | -499.865158 | 10.989756 | -499.951980 | 18.004215 | -500.095463 | . 75% | 5.862593 | 10.929839 | 10.705824 | 46.111999 | 72.950836 | 11.084557 | 3.287790 | 11.860824 | 3.555077 | 174.049914 | ... | 22.998194 | -498.361545 | 17.977502 | -400.224147 | 18.014914 | -498.489381 | 14.001193 | -499.492354 | 23.009704 | -499.526388 | . max | 16.001945 | 17.031899 | 19.615720 | 53.611374 | 100.000000 | 19.552149 | 6.086532 | 22.861749 | 9.789625 | 274.409626 | ... | 60.000000 | -127.692333 | 31.269706 | -6.506986 | 63.116298 | -244.483566 | 39.846228 | -120.190931 | 54.876806 | -29.093593 | . 8 rows × 86 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . final.output.concentrate_ag | 89 | 0.39 | . final.output.concentrate_pb | 87 | 0.38 | . final.output.concentrate_sol | 385 | 1.69 | . final.output.concentrate_au | 86 | 0.38 | . ... | ... | ... | . secondary_cleaner.state.floatbank5_a_level | 101 | 0.44 | . secondary_cleaner.state.floatbank5_b_air | 101 | 0.44 | . secondary_cleaner.state.floatbank5_b_level | 100 | 0.44 | . secondary_cleaner.state.floatbank6_a_air | 119 | 0.52 | . secondary_cleaner.state.floatbank6_a_level | 101 | 0.44 | . 87 rows × 2 columns . **************************************************************************************************** Shape: . (22716, 87) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . get_information(df_train) . Head: . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16788 non-null float64 final.output.concentrate_pb 16788 non-null float64 final.output.concentrate_sol 16490 non-null float64 final.output.concentrate_au 16789 non-null float64 final.output.recovery 15339 non-null float64 final.output.tail_ag 16794 non-null float64 final.output.tail_pb 16677 non-null float64 final.output.tail_sol 16715 non-null float64 final.output.tail_au 16794 non-null float64 primary_cleaner.input.sulfate 15553 non-null float64 primary_cleaner.input.depressant 15598 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 15875 non-null float64 primary_cleaner.output.concentrate_ag 16778 non-null float64 primary_cleaner.output.concentrate_pb 16502 non-null float64 primary_cleaner.output.concentrate_sol 16224 non-null float64 primary_cleaner.output.concentrate_au 16778 non-null float64 primary_cleaner.output.tail_ag 16777 non-null float64 primary_cleaner.output.tail_pb 16761 non-null float64 primary_cleaner.output.tail_sol 16579 non-null float64 primary_cleaner.output.tail_au 16777 non-null float64 primary_cleaner.state.floatbank8_a_air 16820 non-null float64 primary_cleaner.state.floatbank8_a_level 16827 non-null float64 primary_cleaner.state.floatbank8_b_air 16820 non-null float64 primary_cleaner.state.floatbank8_b_level 16833 non-null float64 primary_cleaner.state.floatbank8_c_air 16822 non-null float64 primary_cleaner.state.floatbank8_c_level 16833 non-null float64 primary_cleaner.state.floatbank8_d_air 16821 non-null float64 primary_cleaner.state.floatbank8_d_level 16833 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.au_pb_ratio 15618 non-null float64 rougher.input.feed_ag 16778 non-null float64 rougher.input.feed_pb 16632 non-null float64 rougher.input.feed_rate 16347 non-null float64 rougher.input.feed_size 16443 non-null float64 rougher.input.feed_sol 16568 non-null float64 rougher.input.feed_au 16777 non-null float64 rougher.input.floatbank10_sulfate 15816 non-null float64 rougher.input.floatbank10_xanthate 16514 non-null float64 rougher.input.floatbank11_sulfate 16237 non-null float64 rougher.input.floatbank11_xanthate 14956 non-null float64 rougher.output.concentrate_ag 16778 non-null float64 rougher.output.concentrate_pb 16778 non-null float64 rougher.output.concentrate_sol 16698 non-null float64 rougher.output.concentrate_au 16778 non-null float64 rougher.output.recovery 14287 non-null float64 rougher.output.tail_ag 14610 non-null float64 rougher.output.tail_pb 16778 non-null float64 rougher.output.tail_sol 14611 non-null float64 rougher.output.tail_au 14611 non-null float64 rougher.state.floatbank10_a_air 16807 non-null float64 rougher.state.floatbank10_a_level 16807 non-null float64 rougher.state.floatbank10_b_air 16807 non-null float64 rougher.state.floatbank10_b_level 16807 non-null float64 rougher.state.floatbank10_c_air 16807 non-null float64 rougher.state.floatbank10_c_level 16814 non-null float64 rougher.state.floatbank10_d_air 16802 non-null float64 rougher.state.floatbank10_d_level 16809 non-null float64 rougher.state.floatbank10_e_air 16257 non-null float64 rougher.state.floatbank10_e_level 16809 non-null float64 rougher.state.floatbank10_f_air 16802 non-null float64 rougher.state.floatbank10_f_level 16802 non-null float64 secondary_cleaner.output.tail_ag 16776 non-null float64 secondary_cleaner.output.tail_pb 16764 non-null float64 secondary_cleaner.output.tail_sol 14874 non-null float64 secondary_cleaner.output.tail_au 16778 non-null float64 secondary_cleaner.state.floatbank2_a_air 16497 non-null float64 secondary_cleaner.state.floatbank2_a_level 16751 non-null float64 secondary_cleaner.state.floatbank2_b_air 16705 non-null float64 secondary_cleaner.state.floatbank2_b_level 16748 non-null float64 secondary_cleaner.state.floatbank3_a_air 16763 non-null float64 secondary_cleaner.state.floatbank3_a_level 16747 non-null float64 secondary_cleaner.state.floatbank3_b_air 16752 non-null float64 secondary_cleaner.state.floatbank3_b_level 16750 non-null float64 secondary_cleaner.state.floatbank4_a_air 16731 non-null float64 secondary_cleaner.state.floatbank4_a_level 16747 non-null float64 secondary_cleaner.state.floatbank4_b_air 16768 non-null float64 secondary_cleaner.state.floatbank4_b_level 16767 non-null float64 secondary_cleaner.state.floatbank5_a_air 16775 non-null float64 secondary_cleaner.state.floatbank5_a_level 16775 non-null float64 secondary_cleaner.state.floatbank5_b_air 16775 non-null float64 secondary_cleaner.state.floatbank5_b_level 16776 non-null float64 secondary_cleaner.state.floatbank6_a_air 16757 non-null float64 secondary_cleaner.state.floatbank6_a_level 16775 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB . None . **************************************************************************************************** Describe: . final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au primary_cleaner.input.sulfate ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 16788.000000 | 16788.000000 | 16490.000000 | 16789.000000 | 15339.000000 | 16794.000000 | 16677.000000 | 16715.000000 | 16794.000000 | 15553.000000 | ... | 16731.000000 | 16747.000000 | 16768.000000 | 16767.000000 | 16775.000000 | 16775.000000 | 16775.000000 | 16776.000000 | 16757.000000 | 16775.000000 | . mean | 4.716907 | 9.113559 | 8.301123 | 39.467217 | 67.213166 | 8.757048 | 2.360327 | 9.303932 | 2.687512 | 129.479789 | ... | 19.101874 | -494.164481 | 14.778164 | -476.600082 | 15.779488 | -500.230146 | 12.377241 | -498.956257 | 18.429208 | -521.801826 | . std | 2.096718 | 3.389495 | 3.825760 | 13.917227 | 11.960446 | 3.634103 | 1.215576 | 4.263208 | 1.272757 | 45.386931 | ... | 6.883163 | 84.803334 | 5.999149 | 89.381172 | 6.834703 | 76.983542 | 6.219989 | 82.146207 | 6.958294 | 77.170888 | . min | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000003 | ... | 0.000000 | -799.920713 | 0.000000 | -800.021781 | -0.423260 | -799.741097 | 0.427084 | -800.258209 | 0.024270 | -810.473526 | . 25% | 3.971262 | 8.825748 | 6.939185 | 42.055722 | 62.625685 | 7.610544 | 1.641604 | 7.870275 | 2.172953 | 103.064021 | ... | 14.508299 | -500.837689 | 10.741388 | -500.269182 | 10.977713 | -500.530594 | 8.925586 | -500.147603 | 13.977626 | -501.080595 | . 50% | 4.869346 | 10.065316 | 8.557228 | 44.498874 | 67.644601 | 9.220393 | 2.453690 | 10.021968 | 2.781132 | 131.783108 | ... | 19.986958 | -499.778379 | 14.943933 | -499.593286 | 15.998340 | -499.784231 | 11.092839 | -499.933330 | 18.034960 | -500.109898 | . 75% | 5.821176 | 11.054809 | 10.289741 | 45.976222 | 72.824595 | 10.971110 | 3.192404 | 11.648573 | 3.416936 | 159.539839 | ... | 24.983961 | -494.648754 | 20.023751 | -400.137948 | 20.000701 | -496.531781 | 15.979467 | -498.418000 | 24.984992 | -499.565540 | . max | 16.001945 | 17.031899 | 18.124851 | 53.611374 | 100.000000 | 19.552149 | 6.086532 | 22.317730 | 9.789625 | 251.999948 | ... | 60.000000 | -127.692333 | 28.003828 | -71.472472 | 63.116298 | -275.073125 | 39.846228 | -120.190931 | 54.876806 | -39.784927 | . 8 rows × 86 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . final.output.concentrate_ag | 72 | 0.43 | . final.output.concentrate_pb | 72 | 0.43 | . final.output.concentrate_sol | 370 | 2.19 | . final.output.concentrate_au | 71 | 0.42 | . ... | ... | ... | . secondary_cleaner.state.floatbank5_a_level | 85 | 0.50 | . secondary_cleaner.state.floatbank5_b_air | 85 | 0.50 | . secondary_cleaner.state.floatbank5_b_level | 84 | 0.50 | . secondary_cleaner.state.floatbank6_a_air | 103 | 0.61 | . secondary_cleaner.state.floatbank6_a_level | 85 | 0.50 | . 87 rows × 2 columns . **************************************************************************************************** Shape: . (16860, 87) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . get_information(df_test) . Head: . date primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-09-01 00:59:59 | 210.800909 | 14.993118 | 8.080000 | 1.005021 | 1398.981301 | -500.225577 | 1399.144926 | -499.919735 | 1400.102998 | ... | 12.023554 | -497.795834 | 8.016656 | -501.289139 | 7.946562 | -432.317850 | 4.872511 | -500.037437 | 26.705889 | -499.709414 | . 1 | 2016-09-01 01:59:59 | 215.392455 | 14.987471 | 8.080000 | 0.990469 | 1398.777912 | -500.057435 | 1398.055362 | -499.778182 | 1396.151033 | ... | 12.058140 | -498.695773 | 8.130979 | -499.634209 | 7.958270 | -525.839648 | 4.878850 | -500.162375 | 25.019940 | -499.819438 | . 2 | 2016-09-01 02:59:59 | 215.259946 | 12.884934 | 7.786667 | 0.996043 | 1398.493666 | -500.868360 | 1398.860436 | -499.764529 | 1398.075709 | ... | 11.962366 | -498.767484 | 8.096893 | -500.827423 | 8.071056 | -500.801673 | 4.905125 | -499.828510 | 24.994862 | -500.622559 | . 3 | 2016-09-01 03:59:59 | 215.336236 | 12.006805 | 7.640000 | 0.863514 | 1399.618111 | -498.863574 | 1397.440120 | -499.211024 | 1400.129303 | ... | 12.033091 | -498.350935 | 8.074946 | -499.474407 | 7.897085 | -500.868509 | 4.931400 | -499.963623 | 24.948919 | -498.709987 | . 4 | 2016-09-01 04:59:59 | 199.099327 | 10.682530 | 7.530000 | 0.805575 | 1401.268123 | -500.808305 | 1398.128818 | -499.504543 | 1402.172226 | ... | 12.025367 | -500.786497 | 8.054678 | -500.397500 | 8.107890 | -509.526725 | 4.957674 | -500.360026 | 25.003331 | -500.856333 | . 5 rows × 53 columns . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5554 non-null float64 primary_cleaner.input.depressant 5572 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5690 non-null float64 primary_cleaner.state.floatbank8_a_air 5840 non-null float64 primary_cleaner.state.floatbank8_a_level 5840 non-null float64 primary_cleaner.state.floatbank8_b_air 5840 non-null float64 primary_cleaner.state.floatbank8_b_level 5840 non-null float64 primary_cleaner.state.floatbank8_c_air 5840 non-null float64 primary_cleaner.state.floatbank8_c_level 5840 non-null float64 primary_cleaner.state.floatbank8_d_air 5840 non-null float64 primary_cleaner.state.floatbank8_d_level 5840 non-null float64 rougher.input.feed_ag 5840 non-null float64 rougher.input.feed_pb 5840 non-null float64 rougher.input.feed_rate 5816 non-null float64 rougher.input.feed_size 5834 non-null float64 rougher.input.feed_sol 5789 non-null float64 rougher.input.feed_au 5840 non-null float64 rougher.input.floatbank10_sulfate 5599 non-null float64 rougher.input.floatbank10_xanthate 5733 non-null float64 rougher.input.floatbank11_sulfate 5801 non-null float64 rougher.input.floatbank11_xanthate 5503 non-null float64 rougher.state.floatbank10_a_air 5839 non-null float64 rougher.state.floatbank10_a_level 5840 non-null float64 rougher.state.floatbank10_b_air 5839 non-null float64 rougher.state.floatbank10_b_level 5840 non-null float64 rougher.state.floatbank10_c_air 5839 non-null float64 rougher.state.floatbank10_c_level 5840 non-null float64 rougher.state.floatbank10_d_air 5839 non-null float64 rougher.state.floatbank10_d_level 5840 non-null float64 rougher.state.floatbank10_e_air 5839 non-null float64 rougher.state.floatbank10_e_level 5840 non-null float64 rougher.state.floatbank10_f_air 5839 non-null float64 rougher.state.floatbank10_f_level 5840 non-null float64 secondary_cleaner.state.floatbank2_a_air 5836 non-null float64 secondary_cleaner.state.floatbank2_a_level 5840 non-null float64 secondary_cleaner.state.floatbank2_b_air 5833 non-null float64 secondary_cleaner.state.floatbank2_b_level 5840 non-null float64 secondary_cleaner.state.floatbank3_a_air 5822 non-null float64 secondary_cleaner.state.floatbank3_a_level 5840 non-null float64 secondary_cleaner.state.floatbank3_b_air 5840 non-null float64 secondary_cleaner.state.floatbank3_b_level 5840 non-null float64 secondary_cleaner.state.floatbank4_a_air 5840 non-null float64 secondary_cleaner.state.floatbank4_a_level 5840 non-null float64 secondary_cleaner.state.floatbank4_b_air 5840 non-null float64 secondary_cleaner.state.floatbank4_b_level 5840 non-null float64 secondary_cleaner.state.floatbank5_a_air 5840 non-null float64 secondary_cleaner.state.floatbank5_a_level 5840 non-null float64 secondary_cleaner.state.floatbank5_b_air 5840 non-null float64 secondary_cleaner.state.floatbank5_b_level 5840 non-null float64 secondary_cleaner.state.floatbank6_a_air 5840 non-null float64 secondary_cleaner.state.floatbank6_a_level 5840 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB . None . **************************************************************************************************** Describe: . primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air primary_cleaner.state.floatbank8_c_level ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . count | 5554.000000 | 5572.000000 | 5856.000000 | 5690.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | ... | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | 5840.000000 | . mean | 170.515243 | 8.482873 | 7.264651 | 1.321420 | 1481.990241 | -509.057796 | 1486.908670 | -511.743956 | 1468.495216 | -509.741212 | ... | 15.636031 | -516.266074 | 13.145702 | -476.338907 | 12.308967 | -512.208126 | 9.470986 | -505.017827 | 16.678722 | -512.351694 | . std | 49.608602 | 3.353105 | 0.611526 | 0.693246 | 310.453166 | 61.339256 | 313.224286 | 67.139074 | 309.980748 | 62.671873 | ... | 4.660835 | 62.756748 | 4.304086 | 105.549424 | 3.762827 | 58.864651 | 3.312471 | 68.785898 | 5.404514 | 69.919839 | . min | 0.000103 | 0.000031 | 5.650000 | 0.000003 | 0.000000 | -799.773788 | 0.000000 | -800.029078 | 0.000000 | -799.995127 | ... | 0.000000 | -799.798523 | 0.000000 | -800.836914 | -0.223393 | -799.661076 | 0.528083 | -800.220337 | -0.079426 | -809.859706 | . 25% | 143.340022 | 6.411500 | 6.885625 | 0.888769 | 1497.190681 | -500.455211 | 1497.150234 | -500.936639 | 1437.050321 | -501.300441 | ... | 12.057838 | -501.054741 | 11.880119 | -500.419113 | 10.123459 | -500.879383 | 7.991208 | -500.223089 | 13.012422 | -500.833821 | . 50% | 176.103893 | 8.023252 | 7.259333 | 1.183362 | 1554.659783 | -499.997402 | 1553.268084 | -500.066588 | 1546.160672 | -500.079537 | ... | 17.001867 | -500.160145 | 14.952102 | -499.644328 | 12.062877 | -500.047621 | 9.980774 | -500.001338 | 16.007242 | -500.041085 | . 75% | 207.240761 | 10.017725 | 7.650000 | 1.763797 | 1601.681656 | -499.575313 | 1601.784707 | -499.323361 | 1600.785573 | -499.009545 | ... | 18.030985 | -499.441529 | 15.940011 | -401.523664 | 15.017881 | -499.297033 | 11.992176 | -499.722835 | 21.009076 | -499.395621 | . max | 274.409626 | 40.024582 | 15.500000 | 5.433169 | 2212.432090 | -57.195404 | 1975.147923 | -142.527229 | 1715.053773 | -150.937035 | ... | 30.051797 | -401.565212 | 31.269706 | -6.506986 | 25.258848 | -244.483566 | 14.090194 | -126.463446 | 26.705889 | -29.093593 | . 8 rows × 52 columns . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . date | 0 | 0.00 | . primary_cleaner.input.sulfate | 302 | 5.16 | . primary_cleaner.input.depressant | 284 | 4.85 | . primary_cleaner.input.feed_size | 0 | 0.00 | . primary_cleaner.input.xanthate | 166 | 2.83 | . primary_cleaner.state.floatbank8_a_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_a_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_b_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_b_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_c_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_c_level | 16 | 0.27 | . primary_cleaner.state.floatbank8_d_air | 16 | 0.27 | . primary_cleaner.state.floatbank8_d_level | 16 | 0.27 | . rougher.input.feed_ag | 16 | 0.27 | . rougher.input.feed_pb | 16 | 0.27 | . rougher.input.feed_rate | 40 | 0.68 | . rougher.input.feed_size | 22 | 0.38 | . rougher.input.feed_sol | 67 | 1.14 | . rougher.input.feed_au | 16 | 0.27 | . rougher.input.floatbank10_sulfate | 257 | 4.39 | . rougher.input.floatbank10_xanthate | 123 | 2.10 | . rougher.input.floatbank11_sulfate | 55 | 0.94 | . rougher.input.floatbank11_xanthate | 353 | 6.03 | . rougher.state.floatbank10_a_air | 17 | 0.29 | . rougher.state.floatbank10_a_level | 16 | 0.27 | . rougher.state.floatbank10_b_air | 17 | 0.29 | . rougher.state.floatbank10_b_level | 16 | 0.27 | . rougher.state.floatbank10_c_air | 17 | 0.29 | . rougher.state.floatbank10_c_level | 16 | 0.27 | . rougher.state.floatbank10_d_air | 17 | 0.29 | . rougher.state.floatbank10_d_level | 16 | 0.27 | . rougher.state.floatbank10_e_air | 17 | 0.29 | . rougher.state.floatbank10_e_level | 16 | 0.27 | . rougher.state.floatbank10_f_air | 17 | 0.29 | . rougher.state.floatbank10_f_level | 16 | 0.27 | . secondary_cleaner.state.floatbank2_a_air | 20 | 0.34 | . secondary_cleaner.state.floatbank2_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank2_b_air | 23 | 0.39 | . secondary_cleaner.state.floatbank2_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank3_a_air | 34 | 0.58 | . secondary_cleaner.state.floatbank3_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank3_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank3_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank4_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank4_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank4_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank4_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank5_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank5_a_level | 16 | 0.27 | . secondary_cleaner.state.floatbank5_b_air | 16 | 0.27 | . secondary_cleaner.state.floatbank5_b_level | 16 | 0.27 | . secondary_cleaner.state.floatbank6_a_air | 16 | 0.27 | . secondary_cleaner.state.floatbank6_a_level | 16 | 0.27 | . **************************************************************************************************** Shape: . (5856, 53) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . df (soruce data) has 86 columns in total which is the same with the training dataset. Though df has 22716 entries while the training dataset has 16860 entries. The test dataset has 52 columns total and 5856 total entries. There are many missing values and null values that will not be easily replaced or can be filled in. . Check that recovery is calculated correctly . Using the training set, calculate recovery for the rougher.output.recovery feature. Find the MAE between the calculated values and the feature values . def rough_recovery(row): concentrate = row[&#39;rougher.output.concentrate_au&#39;] feed = row[&#39;rougher.input.feed_au&#39;] tail = row[&#39;rougher.output.tail_au&#39;] try: recovery = ((concentrate * (feed - tail)) / (feed * (concentrate - tail))) return recovery * 100 except ZeroDivisionError: return 0 . au_calc_recovery = df_train.apply(rough_recovery, axis=1) display(au_calc_recovery) . 0 87.107763 1 86.843261 2 86.842308 3 87.226430 4 86.688794 ... 16855 89.574376 16856 87.724007 16857 88.890579 16858 89.858126 16859 89.514960 Length: 16860, dtype: float64 . y_true, y_pred = df_train[&#39;rougher.output.recovery&#39;], au_calc_recovery au_recovery = pd.concat([y_true, y_pred], axis=1, sort=False) au_recovery.columns = [&#39;rougher.output.recovery&#39;, &#39;au_calc_recovery&#39;] au_recovery.head(5) . rougher.output.recovery au_calc_recovery . 0 | 87.107763 | 87.107763 | . 1 | 86.843261 | 86.843261 | . 2 | 86.842308 | 86.842308 | . 3 | 87.226430 | 87.226430 | . 4 | 86.688794 | 86.688794 | . def calc_MAE(y_true, y_pred): try: return mean_absolute_error(y_true , y_pred) except ValueError: return 0 . au_recovery[&#39;MAE&#39;] = calc_MAE(au_recovery[&#39;rougher.output.recovery&#39;], au_recovery[&#39;au_calc_recovery&#39;]) display(au_recovery.info()) display(au_recovery) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 3 columns): rougher.output.recovery 14287 non-null float64 au_calc_recovery 14610 non-null float64 MAE 16860 non-null int64 dtypes: float64(2), int64(1) memory usage: 395.3 KB . None . rougher.output.recovery au_calc_recovery MAE . 0 | 87.107763 | 87.107763 | 0 | . 1 | 86.843261 | 86.843261 | 0 | . 2 | 86.842308 | 86.842308 | 0 | . 3 | 87.226430 | 87.226430 | 0 | . 4 | 86.688794 | 86.688794 | 0 | . ... | ... | ... | ... | . 16855 | 89.574376 | 89.574376 | 0 | . 16856 | 87.724007 | 87.724007 | 0 | . 16857 | 88.890579 | 88.890579 | 0 | . 16858 | 89.858126 | 89.858126 | 0 | . 16859 | 89.514960 | 89.514960 | 0 | . 16860 rows × 3 columns . au_recovery[&#39;MAE&#39;].plot.hist(grid=True) plt.title(&quot;Histogram for Variance of MAE calculation&quot;) . Text(0.5, 1.0, &#39;Histogram for Variance of MAE calculation&#39;) . print(&#39;Maximum value on au_recovery MAE calculation:&#39;, au_recovery[&#39;MAE&#39;].max()) print(&#39;Minimum value on au_recovery MAE calculation:&#39;, au_recovery[&#39;MAE&#39;].min()) . Maximum value on au_recovery MAE calculation: 0 Minimum value on au_recovery MAE calculation: 0 . There were missing values for both the calculated recovery and also for the rougher.output.recovery feature. 0 was replaced for those rows where a MAE calculation could not be made. It was discovered that 0 was the only outcome for MAE calculation between the two which indicates that there is no error between the calculated values and the actual values. . Analyze the features not available in the test set . cols_not_test = set(df).difference(set(df_test)) print(&#39;Number of columns missing from test dataset:&#39;, len(cols_not_test)) display(cols_not_test) . Number of columns missing from test dataset: 34 . {&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_au&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.recovery&#39;, &#39;final.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;final.output.tail_pb&#39;, &#39;final.output.tail_sol&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;primary_cleaner.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.recovery&#39;, &#39;rougher.output.tail_ag&#39;, &#39;rougher.output.tail_au&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;secondary_cleaner.output.tail_sol&#39;} . There are 34 columns missing from the test dataset and all the dtype for these columns is float which are output values or calculations. It&#39;s good to note that the test dataset is missing the two target columns (final.output.recovery and rougher.output.recovery). . Preprocessing Data . We can use our formula to calculate the rougher.output.recovery since earlier we&#39;ve determined that the MAE shows 0 bewteen the calculation values and the actual values. This will help fill in the missing gaps in rougher.output.recovery column. . df[&#39;rougher.output.recovery&#39;] = df.apply(rough_recovery, axis=1) df_train[&#39;rougher.output.recovery&#39;] = df_train.apply(rough_recovery, axis=1) . It might be a good idea to fill missing values and null values in the datasets with fillna(method=‘ffill’) since the data ordered by time. . df.fillna(method=&#39;ffill&#39;, inplace=True) display(df.info()) df_test.fillna(method=&#39;ffill&#39;, inplace=True) display(df_test.info()) df_train.fillna(method=&#39;ffill&#39;, inplace=True) df_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22716 non-null float64 final.output.concentrate_pb 22716 non-null float64 final.output.concentrate_sol 22716 non-null float64 final.output.concentrate_au 22716 non-null float64 final.output.recovery 22716 non-null float64 final.output.tail_ag 22716 non-null float64 final.output.tail_pb 22716 non-null float64 final.output.tail_sol 22716 non-null float64 final.output.tail_au 22716 non-null float64 primary_cleaner.input.sulfate 22716 non-null float64 primary_cleaner.input.depressant 22716 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 22716 non-null float64 primary_cleaner.output.concentrate_ag 22716 non-null float64 primary_cleaner.output.concentrate_pb 22716 non-null float64 primary_cleaner.output.concentrate_sol 22716 non-null float64 primary_cleaner.output.concentrate_au 22716 non-null float64 primary_cleaner.output.tail_ag 22716 non-null float64 primary_cleaner.output.tail_pb 22716 non-null float64 primary_cleaner.output.tail_sol 22716 non-null float64 primary_cleaner.output.tail_au 22716 non-null float64 primary_cleaner.state.floatbank8_a_air 22716 non-null float64 primary_cleaner.state.floatbank8_a_level 22716 non-null float64 primary_cleaner.state.floatbank8_b_air 22716 non-null float64 primary_cleaner.state.floatbank8_b_level 22716 non-null float64 primary_cleaner.state.floatbank8_c_air 22716 non-null float64 primary_cleaner.state.floatbank8_c_level 22716 non-null float64 primary_cleaner.state.floatbank8_d_air 22716 non-null float64 primary_cleaner.state.floatbank8_d_level 22716 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22716 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22716 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22716 non-null float64 rougher.calculation.au_pb_ratio 22716 non-null float64 rougher.input.feed_ag 22716 non-null float64 rougher.input.feed_pb 22716 non-null float64 rougher.input.feed_rate 22716 non-null float64 rougher.input.feed_size 22716 non-null float64 rougher.input.feed_sol 22716 non-null float64 rougher.input.feed_au 22716 non-null float64 rougher.input.floatbank10_sulfate 22716 non-null float64 rougher.input.floatbank10_xanthate 22716 non-null float64 rougher.input.floatbank11_sulfate 22716 non-null float64 rougher.input.floatbank11_xanthate 22716 non-null float64 rougher.output.concentrate_ag 22716 non-null float64 rougher.output.concentrate_pb 22716 non-null float64 rougher.output.concentrate_sol 22716 non-null float64 rougher.output.concentrate_au 22716 non-null float64 rougher.output.recovery 22716 non-null float64 rougher.output.tail_ag 22716 non-null float64 rougher.output.tail_pb 22716 non-null float64 rougher.output.tail_sol 22716 non-null float64 rougher.output.tail_au 22716 non-null float64 rougher.state.floatbank10_a_air 22716 non-null float64 rougher.state.floatbank10_a_level 22716 non-null float64 rougher.state.floatbank10_b_air 22716 non-null float64 rougher.state.floatbank10_b_level 22716 non-null float64 rougher.state.floatbank10_c_air 22716 non-null float64 rougher.state.floatbank10_c_level 22716 non-null float64 rougher.state.floatbank10_d_air 22716 non-null float64 rougher.state.floatbank10_d_level 22716 non-null float64 rougher.state.floatbank10_e_air 22716 non-null float64 rougher.state.floatbank10_e_level 22716 non-null float64 rougher.state.floatbank10_f_air 22716 non-null float64 rougher.state.floatbank10_f_level 22716 non-null float64 secondary_cleaner.output.tail_ag 22716 non-null float64 secondary_cleaner.output.tail_pb 22716 non-null float64 secondary_cleaner.output.tail_sol 22716 non-null float64 secondary_cleaner.output.tail_au 22716 non-null float64 secondary_cleaner.state.floatbank2_a_air 22716 non-null float64 secondary_cleaner.state.floatbank2_a_level 22716 non-null float64 secondary_cleaner.state.floatbank2_b_air 22716 non-null float64 secondary_cleaner.state.floatbank2_b_level 22716 non-null float64 secondary_cleaner.state.floatbank3_a_air 22716 non-null float64 secondary_cleaner.state.floatbank3_a_level 22716 non-null float64 secondary_cleaner.state.floatbank3_b_air 22716 non-null float64 secondary_cleaner.state.floatbank3_b_level 22716 non-null float64 secondary_cleaner.state.floatbank4_a_air 22716 non-null float64 secondary_cleaner.state.floatbank4_a_level 22716 non-null float64 secondary_cleaner.state.floatbank4_b_air 22716 non-null float64 secondary_cleaner.state.floatbank4_b_level 22716 non-null float64 secondary_cleaner.state.floatbank5_a_air 22716 non-null float64 secondary_cleaner.state.floatbank5_a_level 22716 non-null float64 secondary_cleaner.state.floatbank5_b_air 22716 non-null float64 secondary_cleaner.state.floatbank5_b_level 22716 non-null float64 secondary_cleaner.state.floatbank6_a_air 22716 non-null float64 secondary_cleaner.state.floatbank6_a_level 22716 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5856 non-null float64 primary_cleaner.input.depressant 5856 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5856 non-null float64 primary_cleaner.state.floatbank8_a_air 5856 non-null float64 primary_cleaner.state.floatbank8_a_level 5856 non-null float64 primary_cleaner.state.floatbank8_b_air 5856 non-null float64 primary_cleaner.state.floatbank8_b_level 5856 non-null float64 primary_cleaner.state.floatbank8_c_air 5856 non-null float64 primary_cleaner.state.floatbank8_c_level 5856 non-null float64 primary_cleaner.state.floatbank8_d_air 5856 non-null float64 primary_cleaner.state.floatbank8_d_level 5856 non-null float64 rougher.input.feed_ag 5856 non-null float64 rougher.input.feed_pb 5856 non-null float64 rougher.input.feed_rate 5856 non-null float64 rougher.input.feed_size 5856 non-null float64 rougher.input.feed_sol 5856 non-null float64 rougher.input.feed_au 5856 non-null float64 rougher.input.floatbank10_sulfate 5856 non-null float64 rougher.input.floatbank10_xanthate 5856 non-null float64 rougher.input.floatbank11_sulfate 5856 non-null float64 rougher.input.floatbank11_xanthate 5856 non-null float64 rougher.state.floatbank10_a_air 5856 non-null float64 rougher.state.floatbank10_a_level 5856 non-null float64 rougher.state.floatbank10_b_air 5856 non-null float64 rougher.state.floatbank10_b_level 5856 non-null float64 rougher.state.floatbank10_c_air 5856 non-null float64 rougher.state.floatbank10_c_level 5856 non-null float64 rougher.state.floatbank10_d_air 5856 non-null float64 rougher.state.floatbank10_d_level 5856 non-null float64 rougher.state.floatbank10_e_air 5856 non-null float64 rougher.state.floatbank10_e_level 5856 non-null float64 rougher.state.floatbank10_f_air 5856 non-null float64 rougher.state.floatbank10_f_level 5856 non-null float64 secondary_cleaner.state.floatbank2_a_air 5856 non-null float64 secondary_cleaner.state.floatbank2_a_level 5856 non-null float64 secondary_cleaner.state.floatbank2_b_air 5856 non-null float64 secondary_cleaner.state.floatbank2_b_level 5856 non-null float64 secondary_cleaner.state.floatbank3_a_air 5856 non-null float64 secondary_cleaner.state.floatbank3_a_level 5856 non-null float64 secondary_cleaner.state.floatbank3_b_air 5856 non-null float64 secondary_cleaner.state.floatbank3_b_level 5856 non-null float64 secondary_cleaner.state.floatbank4_a_air 5856 non-null float64 secondary_cleaner.state.floatbank4_a_level 5856 non-null float64 secondary_cleaner.state.floatbank4_b_air 5856 non-null float64 secondary_cleaner.state.floatbank4_b_level 5856 non-null float64 secondary_cleaner.state.floatbank5_a_air 5856 non-null float64 secondary_cleaner.state.floatbank5_a_level 5856 non-null float64 secondary_cleaner.state.floatbank5_b_air 5856 non-null float64 secondary_cleaner.state.floatbank5_b_level 5856 non-null float64 secondary_cleaner.state.floatbank6_a_air 5856 non-null float64 secondary_cleaner.state.floatbank6_a_level 5856 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16860 non-null float64 final.output.concentrate_pb 16860 non-null float64 final.output.concentrate_sol 16860 non-null float64 final.output.concentrate_au 16860 non-null float64 final.output.recovery 16860 non-null float64 final.output.tail_ag 16860 non-null float64 final.output.tail_pb 16860 non-null float64 final.output.tail_sol 16860 non-null float64 final.output.tail_au 16860 non-null float64 primary_cleaner.input.sulfate 16860 non-null float64 primary_cleaner.input.depressant 16860 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 16860 non-null float64 primary_cleaner.output.concentrate_ag 16860 non-null float64 primary_cleaner.output.concentrate_pb 16860 non-null float64 primary_cleaner.output.concentrate_sol 16860 non-null float64 primary_cleaner.output.concentrate_au 16860 non-null float64 primary_cleaner.output.tail_ag 16860 non-null float64 primary_cleaner.output.tail_pb 16860 non-null float64 primary_cleaner.output.tail_sol 16860 non-null float64 primary_cleaner.output.tail_au 16860 non-null float64 primary_cleaner.state.floatbank8_a_air 16860 non-null float64 primary_cleaner.state.floatbank8_a_level 16860 non-null float64 primary_cleaner.state.floatbank8_b_air 16860 non-null float64 primary_cleaner.state.floatbank8_b_level 16860 non-null float64 primary_cleaner.state.floatbank8_c_air 16860 non-null float64 primary_cleaner.state.floatbank8_c_level 16860 non-null float64 primary_cleaner.state.floatbank8_d_air 16860 non-null float64 primary_cleaner.state.floatbank8_d_level 16860 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16860 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16860 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16860 non-null float64 rougher.calculation.au_pb_ratio 16860 non-null float64 rougher.input.feed_ag 16860 non-null float64 rougher.input.feed_pb 16860 non-null float64 rougher.input.feed_rate 16860 non-null float64 rougher.input.feed_size 16860 non-null float64 rougher.input.feed_sol 16860 non-null float64 rougher.input.feed_au 16860 non-null float64 rougher.input.floatbank10_sulfate 16860 non-null float64 rougher.input.floatbank10_xanthate 16860 non-null float64 rougher.input.floatbank11_sulfate 16860 non-null float64 rougher.input.floatbank11_xanthate 16860 non-null float64 rougher.output.concentrate_ag 16860 non-null float64 rougher.output.concentrate_pb 16860 non-null float64 rougher.output.concentrate_sol 16860 non-null float64 rougher.output.concentrate_au 16860 non-null float64 rougher.output.recovery 16860 non-null float64 rougher.output.tail_ag 16860 non-null float64 rougher.output.tail_pb 16860 non-null float64 rougher.output.tail_sol 16860 non-null float64 rougher.output.tail_au 16860 non-null float64 rougher.state.floatbank10_a_air 16860 non-null float64 rougher.state.floatbank10_a_level 16860 non-null float64 rougher.state.floatbank10_b_air 16860 non-null float64 rougher.state.floatbank10_b_level 16860 non-null float64 rougher.state.floatbank10_c_air 16860 non-null float64 rougher.state.floatbank10_c_level 16860 non-null float64 rougher.state.floatbank10_d_air 16860 non-null float64 rougher.state.floatbank10_d_level 16860 non-null float64 rougher.state.floatbank10_e_air 16860 non-null float64 rougher.state.floatbank10_e_level 16860 non-null float64 rougher.state.floatbank10_f_air 16860 non-null float64 rougher.state.floatbank10_f_level 16860 non-null float64 secondary_cleaner.output.tail_ag 16860 non-null float64 secondary_cleaner.output.tail_pb 16860 non-null float64 secondary_cleaner.output.tail_sol 16860 non-null float64 secondary_cleaner.output.tail_au 16860 non-null float64 secondary_cleaner.state.floatbank2_a_air 16860 non-null float64 secondary_cleaner.state.floatbank2_a_level 16860 non-null float64 secondary_cleaner.state.floatbank2_b_air 16860 non-null float64 secondary_cleaner.state.floatbank2_b_level 16860 non-null float64 secondary_cleaner.state.floatbank3_a_air 16860 non-null float64 secondary_cleaner.state.floatbank3_a_level 16860 non-null float64 secondary_cleaner.state.floatbank3_b_air 16860 non-null float64 secondary_cleaner.state.floatbank3_b_level 16860 non-null float64 secondary_cleaner.state.floatbank4_a_air 16860 non-null float64 secondary_cleaner.state.floatbank4_a_level 16860 non-null float64 secondary_cleaner.state.floatbank4_b_air 16860 non-null float64 secondary_cleaner.state.floatbank4_b_level 16860 non-null float64 secondary_cleaner.state.floatbank5_a_air 16860 non-null float64 secondary_cleaner.state.floatbank5_a_level 16860 non-null float64 secondary_cleaner.state.floatbank5_b_air 16860 non-null float64 secondary_cleaner.state.floatbank5_b_level 16860 non-null float64 secondary_cleaner.state.floatbank6_a_air 16860 non-null float64 secondary_cleaner.state.floatbank6_a_level 16860 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB . . We were able to fill in some missing values with our calculation of the rougher.output.recover for both training and test datasets as well as fill in the rest of the missing and null values using the fillna(method=‘ffill’) since the data ordered by time (it fills in null values with the previous data from the instance). . Visualize . Concentrations of metals (Au, Ag, Pb) at each different purification stage . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;final [Au]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Au]&#39;) plt.hist(df[&#39;rougher.output.concentrate_au&#39;], bins=100, alpha=0.5, label=&#39;rougher [Au]&#39;) plt.hist(df[&#39;rougher.input.feed_au&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Au]&#39;) plt.xlabel(&#39;Concentrations of Au at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Au&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 55) . . (0, 55) . Au (gold) concentrations get more concentrated with each stage of purification which is to be expected as the impurities are removed with each stage and the target being gold recovery that concentrations of Au would be greatest at the final stage. . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;final [Ag]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Ag]&#39;) plt.hist(df[&#39;rougher.output.concentrate_ag&#39;], bins=100, alpha=0.5, label=&#39;rougher [Ag]&#39;) plt.hist(df[&#39;rougher.input.feed_ag&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Ag]&#39;) plt.xlabel(&#39;Concentrations of Ag at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Ag&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 22) . . (0, 22) . Concentrations for Ag (silver) decrease with each stage of purification which is to be expected as Ag is considered a impurity in the target gold recovery. . plt.figure(figsize=(10,5)) plt.hist(df[&#39;final.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;final [Pb]&#39;) plt.hist(df[&#39;primary_cleaner.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;primary_cleaner [Pb]&#39;) plt.hist(df[&#39;rougher.output.concentrate_pb&#39;], bins=100, alpha=0.5, label=&#39;rougher [Pb]&#39;) plt.hist(df[&#39;rougher.input.feed_pb&#39;], bins=100, alpha=0.5, label=&#39;rougher input feed [Pb]&#39;) plt.xlabel(&#39;Concentrations of Pb at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for Concentrations of Pb&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(0, 17) . . (0, 17) . Interesting with Pb concentrations (lead) seems to have slightly increase with each stage of purification. This might be due to how the process of gold recovery is done where the solute might be not be able to filter out lead as easily as other impurities. Though the concentration really is not nearly as concentrated in comparison to gold. . There appears to be outliers where concentraions of metals are 0. . Compare the feed particle size distributions in the training set and in the test set . If the distributions vary significantly, the model evaluation will be incorrect. . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_au&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_au&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Au&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Au from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_au&#39;, &#39;test rougher.input.feed_au&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_ag&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_ag&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Ag&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Ag from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_ag&#39;, &#39;test rougher.input.feed_ag&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_pb&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_pb&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of Pb&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of Pb from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_pb&#39;, &#39;test rougher.input.feed_pb&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_sol&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_sol&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed of sol&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed of sol from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_sol&#39;, &#39;test rougher.input.feed_sol&#39;],loc=&#39;upper right&#39;) plt.grid(True) . . plt.figure(figsize=(10,5)) df_train[&#39;rougher.input.feed_size&#39;].plot.kde(bw_method=0.3) df_test[&#39;rougher.input.feed_size&#39;].plot.kde(bw_method=0.3) plt.xlabel(&#39;Rougher input feed size&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&quot;Density plot for rougher input feed size from both train and test datasets&quot;) plt.legend([&#39;train rougher.input.feed_size&#39;, &#39;test rougher.input.feed_size&#39;],loc=&#39;upper right&#39;) plt.grid(True) plt.xlim(-10, 175) . . (-10, 175) . From the plots we can see that the particle size feed for each Au, Ag, Pb, sol, and feed size from train and test datasets are fairly similar. . Total concentrations of all substances at different stages: raw feed, rougher concentrate, and final concentrate . #collapse-hide def total_rough_feed(df): au = df[&#39;rougher.input.feed_au&#39;] ag = df[&#39;rougher.input.feed_ag&#39;] pb = df[&#39;rougher.input.feed_pb&#39;] sol = df[&#39;rougher.input.feed_sol&#39;] total = au + ag + pb + sol return total def total_rough_conc(df): au = df[&#39;rougher.output.concentrate_au&#39;] ag = df[&#39;rougher.output.concentrate_ag&#39;] pb = df[&#39;rougher.output.concentrate_pb&#39;] sol = df[&#39;rougher.output.concentrate_sol&#39;] total = au + ag + pb + sol return total def total_final_conc(df): au = df[&#39;final.output.concentrate_au&#39;] ag = df[&#39;final.output.concentrate_ag&#39;] pb = df[&#39;final.output.concentrate_pb&#39;] sol = df[&#39;final.output.concentrate_sol&#39;] total = au + ag + pb + sol return total . . df[&#39;total_rough_feed&#39;] = total_rough_feed(df) df[&#39;total_rough_conc&#39;] = total_rough_conc(df) df[&#39;total_final_conc&#39;] = total_final_conc(df) df[[&#39;date&#39;,&#39;total_rough_feed&#39;,&#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]] . date total_rough_feed total_rough_conc total_final_conc . 0 | 2016-01-15 00:00:00 | 51.680034 | 66.424950 | 63.644396 | . 1 | 2016-01-15 01:00:00 | 50.659114 | 67.012710 | 63.957723 | . 2 | 2016-01-15 02:00:00 | 50.609929 | 66.103793 | 64.311180 | . 3 | 2016-01-15 03:00:00 | 51.061546 | 65.752751 | 63.573449 | . 4 | 2016-01-15 04:00:00 | 47.859163 | 65.908382 | 64.004667 | . ... | ... | ... | ... | ... | . 22711 | 2018-08-18 06:59:59 | 53.415050 | 70.781325 | 68.098589 | . 22712 | 2018-08-18 07:59:59 | 53.696482 | 70.539603 | 68.274362 | . 22713 | 2018-08-18 08:59:59 | 54.589604 | 55.376330 | 68.226068 | . 22714 | 2018-08-18 09:59:59 | 54.027355 | 69.201689 | 68.200449 | . 22715 | 2018-08-18 10:59:59 | 53.535054 | 69.544003 | 68.353154 | . 22716 rows × 4 columns . plt.figure(figsize=(10,5)) plt.hist(df[&#39;total_rough_feed&#39;], bins=100, alpha=0.5, label=&#39;total_rough_feed&#39;) plt.hist(df[&#39;total_rough_conc&#39;], bins=100, alpha=0.5, label=&#39;total rough []&#39;) plt.hist(df[&#39;total_final_conc&#39;], bins=100, alpha=0.5, label=&#39;total final []&#39;) plt.xlabel(&#39;Total concentrations of elements at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for total concentrations of elements&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) . . From the histogram of total concentrations of all elements the obvious abnormal values are the ones that linger around 0. It&#39;s best to drop these values moving forward. It is ideal to remove these values from both test and train datasets because if we only remove from train, the model will be shocked with such examples on test in comparison. . df = df[df[&#39;total_rough_feed&#39;] &gt; 1] df = df[df[&#39;total_rough_conc&#39;] &gt; 1] df = df[df[&#39;total_final_conc&#39;] &gt; 1] . # Merging columns &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; from df to aid in removing abnormal values df_train_clean = pd.merge(df_train, df[[&#39;date&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_train_clean = df_train_clean.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) df_train_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14612 entries, 0 to 16859 Data columns (total 90 columns): date 14612 non-null object final.output.concentrate_ag 14612 non-null float64 final.output.concentrate_pb 14612 non-null float64 final.output.concentrate_sol 14612 non-null float64 final.output.concentrate_au 14612 non-null float64 final.output.recovery 14612 non-null float64 final.output.tail_ag 14612 non-null float64 final.output.tail_pb 14612 non-null float64 final.output.tail_sol 14612 non-null float64 final.output.tail_au 14612 non-null float64 primary_cleaner.input.sulfate 14612 non-null float64 primary_cleaner.input.depressant 14612 non-null float64 primary_cleaner.input.feed_size 14612 non-null float64 primary_cleaner.input.xanthate 14612 non-null float64 primary_cleaner.output.concentrate_ag 14612 non-null float64 primary_cleaner.output.concentrate_pb 14612 non-null float64 primary_cleaner.output.concentrate_sol 14612 non-null float64 primary_cleaner.output.concentrate_au 14612 non-null float64 primary_cleaner.output.tail_ag 14612 non-null float64 primary_cleaner.output.tail_pb 14612 non-null float64 primary_cleaner.output.tail_sol 14612 non-null float64 primary_cleaner.output.tail_au 14612 non-null float64 primary_cleaner.state.floatbank8_a_air 14612 non-null float64 primary_cleaner.state.floatbank8_a_level 14612 non-null float64 primary_cleaner.state.floatbank8_b_air 14612 non-null float64 primary_cleaner.state.floatbank8_b_level 14612 non-null float64 primary_cleaner.state.floatbank8_c_air 14612 non-null float64 primary_cleaner.state.floatbank8_c_level 14612 non-null float64 primary_cleaner.state.floatbank8_d_air 14612 non-null float64 primary_cleaner.state.floatbank8_d_level 14612 non-null float64 rougher.calculation.sulfate_to_au_concentrate 14612 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 14612 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 14612 non-null float64 rougher.calculation.au_pb_ratio 14612 non-null float64 rougher.input.feed_ag 14612 non-null float64 rougher.input.feed_pb 14612 non-null float64 rougher.input.feed_rate 14612 non-null float64 rougher.input.feed_size 14612 non-null float64 rougher.input.feed_sol 14612 non-null float64 rougher.input.feed_au 14612 non-null float64 rougher.input.floatbank10_sulfate 14612 non-null float64 rougher.input.floatbank10_xanthate 14612 non-null float64 rougher.input.floatbank11_sulfate 14612 non-null float64 rougher.input.floatbank11_xanthate 14612 non-null float64 rougher.output.concentrate_ag 14612 non-null float64 rougher.output.concentrate_pb 14612 non-null float64 rougher.output.concentrate_sol 14612 non-null float64 rougher.output.concentrate_au 14612 non-null float64 rougher.output.recovery 14612 non-null float64 rougher.output.tail_ag 14612 non-null float64 rougher.output.tail_pb 14612 non-null float64 rougher.output.tail_sol 14612 non-null float64 rougher.output.tail_au 14612 non-null float64 rougher.state.floatbank10_a_air 14612 non-null float64 rougher.state.floatbank10_a_level 14612 non-null float64 rougher.state.floatbank10_b_air 14612 non-null float64 rougher.state.floatbank10_b_level 14612 non-null float64 rougher.state.floatbank10_c_air 14612 non-null float64 rougher.state.floatbank10_c_level 14612 non-null float64 rougher.state.floatbank10_d_air 14612 non-null float64 rougher.state.floatbank10_d_level 14612 non-null float64 rougher.state.floatbank10_e_air 14612 non-null float64 rougher.state.floatbank10_e_level 14612 non-null float64 rougher.state.floatbank10_f_air 14612 non-null float64 rougher.state.floatbank10_f_level 14612 non-null float64 secondary_cleaner.output.tail_ag 14612 non-null float64 secondary_cleaner.output.tail_pb 14612 non-null float64 secondary_cleaner.output.tail_sol 14612 non-null float64 secondary_cleaner.output.tail_au 14612 non-null float64 secondary_cleaner.state.floatbank2_a_air 14612 non-null float64 secondary_cleaner.state.floatbank2_a_level 14612 non-null float64 secondary_cleaner.state.floatbank2_b_air 14612 non-null float64 secondary_cleaner.state.floatbank2_b_level 14612 non-null float64 secondary_cleaner.state.floatbank3_a_air 14612 non-null float64 secondary_cleaner.state.floatbank3_a_level 14612 non-null float64 secondary_cleaner.state.floatbank3_b_air 14612 non-null float64 secondary_cleaner.state.floatbank3_b_level 14612 non-null float64 secondary_cleaner.state.floatbank4_a_air 14612 non-null float64 secondary_cleaner.state.floatbank4_a_level 14612 non-null float64 secondary_cleaner.state.floatbank4_b_air 14612 non-null float64 secondary_cleaner.state.floatbank4_b_level 14612 non-null float64 secondary_cleaner.state.floatbank5_a_air 14612 non-null float64 secondary_cleaner.state.floatbank5_a_level 14612 non-null float64 secondary_cleaner.state.floatbank5_b_air 14612 non-null float64 secondary_cleaner.state.floatbank5_b_level 14612 non-null float64 secondary_cleaner.state.floatbank6_a_air 14612 non-null float64 secondary_cleaner.state.floatbank6_a_level 14612 non-null float64 total_rough_feed 14612 non-null float64 total_rough_conc 14612 non-null float64 total_final_conc 14612 non-null float64 dtypes: float64(89), object(1) memory usage: 10.1+ MB . . # Merging columns &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; from df to aid in removing abnormal values df_test_clean = pd.merge(df_test, df[[&#39;date&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_test_clean = df_test_clean.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) df_test_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5290 entries, 0 to 5855 Data columns (total 56 columns): date 5290 non-null object primary_cleaner.input.sulfate 5290 non-null float64 primary_cleaner.input.depressant 5290 non-null float64 primary_cleaner.input.feed_size 5290 non-null float64 primary_cleaner.input.xanthate 5290 non-null float64 primary_cleaner.state.floatbank8_a_air 5290 non-null float64 primary_cleaner.state.floatbank8_a_level 5290 non-null float64 primary_cleaner.state.floatbank8_b_air 5290 non-null float64 primary_cleaner.state.floatbank8_b_level 5290 non-null float64 primary_cleaner.state.floatbank8_c_air 5290 non-null float64 primary_cleaner.state.floatbank8_c_level 5290 non-null float64 primary_cleaner.state.floatbank8_d_air 5290 non-null float64 primary_cleaner.state.floatbank8_d_level 5290 non-null float64 rougher.input.feed_ag 5290 non-null float64 rougher.input.feed_pb 5290 non-null float64 rougher.input.feed_rate 5290 non-null float64 rougher.input.feed_size 5290 non-null float64 rougher.input.feed_sol 5290 non-null float64 rougher.input.feed_au 5290 non-null float64 rougher.input.floatbank10_sulfate 5290 non-null float64 rougher.input.floatbank10_xanthate 5290 non-null float64 rougher.input.floatbank11_sulfate 5290 non-null float64 rougher.input.floatbank11_xanthate 5290 non-null float64 rougher.state.floatbank10_a_air 5290 non-null float64 rougher.state.floatbank10_a_level 5290 non-null float64 rougher.state.floatbank10_b_air 5290 non-null float64 rougher.state.floatbank10_b_level 5290 non-null float64 rougher.state.floatbank10_c_air 5290 non-null float64 rougher.state.floatbank10_c_level 5290 non-null float64 rougher.state.floatbank10_d_air 5290 non-null float64 rougher.state.floatbank10_d_level 5290 non-null float64 rougher.state.floatbank10_e_air 5290 non-null float64 rougher.state.floatbank10_e_level 5290 non-null float64 rougher.state.floatbank10_f_air 5290 non-null float64 rougher.state.floatbank10_f_level 5290 non-null float64 secondary_cleaner.state.floatbank2_a_air 5290 non-null float64 secondary_cleaner.state.floatbank2_a_level 5290 non-null float64 secondary_cleaner.state.floatbank2_b_air 5290 non-null float64 secondary_cleaner.state.floatbank2_b_level 5290 non-null float64 secondary_cleaner.state.floatbank3_a_air 5290 non-null float64 secondary_cleaner.state.floatbank3_a_level 5290 non-null float64 secondary_cleaner.state.floatbank3_b_air 5290 non-null float64 secondary_cleaner.state.floatbank3_b_level 5290 non-null float64 secondary_cleaner.state.floatbank4_a_air 5290 non-null float64 secondary_cleaner.state.floatbank4_a_level 5290 non-null float64 secondary_cleaner.state.floatbank4_b_air 5290 non-null float64 secondary_cleaner.state.floatbank4_b_level 5290 non-null float64 secondary_cleaner.state.floatbank5_a_air 5290 non-null float64 secondary_cleaner.state.floatbank5_a_level 5290 non-null float64 secondary_cleaner.state.floatbank5_b_air 5290 non-null float64 secondary_cleaner.state.floatbank5_b_level 5290 non-null float64 secondary_cleaner.state.floatbank6_a_air 5290 non-null float64 secondary_cleaner.state.floatbank6_a_level 5290 non-null float64 total_rough_feed 5290 non-null float64 total_rough_conc 5290 non-null float64 total_final_conc 5290 non-null float64 dtypes: float64(55), object(1) memory usage: 2.3+ MB . . # Removing 0 values from &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns columns = [&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;] df_clean = df.replace(0, np.nan).dropna(axis=0, how=&#39;any&#39;, subset=columns).fillna(0) display(df.info()) df_clean.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 19902 entries, 0 to 22715 Data columns (total 90 columns): date 19902 non-null object final.output.concentrate_ag 19902 non-null float64 final.output.concentrate_pb 19902 non-null float64 final.output.concentrate_sol 19902 non-null float64 final.output.concentrate_au 19902 non-null float64 final.output.recovery 19902 non-null float64 final.output.tail_ag 19902 non-null float64 final.output.tail_pb 19902 non-null float64 final.output.tail_sol 19902 non-null float64 final.output.tail_au 19902 non-null float64 primary_cleaner.input.sulfate 19902 non-null float64 primary_cleaner.input.depressant 19902 non-null float64 primary_cleaner.input.feed_size 19902 non-null float64 primary_cleaner.input.xanthate 19902 non-null float64 primary_cleaner.output.concentrate_ag 19902 non-null float64 primary_cleaner.output.concentrate_pb 19902 non-null float64 primary_cleaner.output.concentrate_sol 19902 non-null float64 primary_cleaner.output.concentrate_au 19902 non-null float64 primary_cleaner.output.tail_ag 19902 non-null float64 primary_cleaner.output.tail_pb 19902 non-null float64 primary_cleaner.output.tail_sol 19902 non-null float64 primary_cleaner.output.tail_au 19902 non-null float64 primary_cleaner.state.floatbank8_a_air 19902 non-null float64 primary_cleaner.state.floatbank8_a_level 19902 non-null float64 primary_cleaner.state.floatbank8_b_air 19902 non-null float64 primary_cleaner.state.floatbank8_b_level 19902 non-null float64 primary_cleaner.state.floatbank8_c_air 19902 non-null float64 primary_cleaner.state.floatbank8_c_level 19902 non-null float64 primary_cleaner.state.floatbank8_d_air 19902 non-null float64 primary_cleaner.state.floatbank8_d_level 19902 non-null float64 rougher.calculation.sulfate_to_au_concentrate 19902 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.au_pb_ratio 19902 non-null float64 rougher.input.feed_ag 19902 non-null float64 rougher.input.feed_pb 19902 non-null float64 rougher.input.feed_rate 19902 non-null float64 rougher.input.feed_size 19902 non-null float64 rougher.input.feed_sol 19902 non-null float64 rougher.input.feed_au 19902 non-null float64 rougher.input.floatbank10_sulfate 19902 non-null float64 rougher.input.floatbank10_xanthate 19902 non-null float64 rougher.input.floatbank11_sulfate 19902 non-null float64 rougher.input.floatbank11_xanthate 19902 non-null float64 rougher.output.concentrate_ag 19902 non-null float64 rougher.output.concentrate_pb 19902 non-null float64 rougher.output.concentrate_sol 19902 non-null float64 rougher.output.concentrate_au 19902 non-null float64 rougher.output.recovery 19902 non-null float64 rougher.output.tail_ag 19902 non-null float64 rougher.output.tail_pb 19902 non-null float64 rougher.output.tail_sol 19902 non-null float64 rougher.output.tail_au 19902 non-null float64 rougher.state.floatbank10_a_air 19902 non-null float64 rougher.state.floatbank10_a_level 19902 non-null float64 rougher.state.floatbank10_b_air 19902 non-null float64 rougher.state.floatbank10_b_level 19902 non-null float64 rougher.state.floatbank10_c_air 19902 non-null float64 rougher.state.floatbank10_c_level 19902 non-null float64 rougher.state.floatbank10_d_air 19902 non-null float64 rougher.state.floatbank10_d_level 19902 non-null float64 rougher.state.floatbank10_e_air 19902 non-null float64 rougher.state.floatbank10_e_level 19902 non-null float64 rougher.state.floatbank10_f_air 19902 non-null float64 rougher.state.floatbank10_f_level 19902 non-null float64 secondary_cleaner.output.tail_ag 19902 non-null float64 secondary_cleaner.output.tail_pb 19902 non-null float64 secondary_cleaner.output.tail_sol 19902 non-null float64 secondary_cleaner.output.tail_au 19902 non-null float64 secondary_cleaner.state.floatbank2_a_air 19902 non-null float64 secondary_cleaner.state.floatbank2_a_level 19902 non-null float64 secondary_cleaner.state.floatbank2_b_air 19902 non-null float64 secondary_cleaner.state.floatbank2_b_level 19902 non-null float64 secondary_cleaner.state.floatbank3_a_air 19902 non-null float64 secondary_cleaner.state.floatbank3_a_level 19902 non-null float64 secondary_cleaner.state.floatbank3_b_air 19902 non-null float64 secondary_cleaner.state.floatbank3_b_level 19902 non-null float64 secondary_cleaner.state.floatbank4_a_air 19902 non-null float64 secondary_cleaner.state.floatbank4_a_level 19902 non-null float64 secondary_cleaner.state.floatbank4_b_air 19902 non-null float64 secondary_cleaner.state.floatbank4_b_level 19902 non-null float64 secondary_cleaner.state.floatbank5_a_air 19902 non-null float64 secondary_cleaner.state.floatbank5_a_level 19902 non-null float64 secondary_cleaner.state.floatbank5_b_air 19902 non-null float64 secondary_cleaner.state.floatbank5_b_level 19902 non-null float64 secondary_cleaner.state.floatbank6_a_air 19902 non-null float64 secondary_cleaner.state.floatbank6_a_level 19902 non-null float64 total_rough_feed 19902 non-null float64 total_rough_conc 19902 non-null float64 total_final_conc 19902 non-null float64 dtypes: float64(89), object(1) memory usage: 13.8+ MB . None . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 19902 entries, 0 to 22715 Data columns (total 90 columns): date 19902 non-null object final.output.concentrate_ag 19902 non-null float64 final.output.concentrate_pb 19902 non-null float64 final.output.concentrate_sol 19902 non-null float64 final.output.concentrate_au 19902 non-null float64 final.output.recovery 19902 non-null float64 final.output.tail_ag 19902 non-null float64 final.output.tail_pb 19902 non-null float64 final.output.tail_sol 19902 non-null float64 final.output.tail_au 19902 non-null float64 primary_cleaner.input.sulfate 19902 non-null float64 primary_cleaner.input.depressant 19902 non-null float64 primary_cleaner.input.feed_size 19902 non-null float64 primary_cleaner.input.xanthate 19902 non-null float64 primary_cleaner.output.concentrate_ag 19902 non-null float64 primary_cleaner.output.concentrate_pb 19902 non-null float64 primary_cleaner.output.concentrate_sol 19902 non-null float64 primary_cleaner.output.concentrate_au 19902 non-null float64 primary_cleaner.output.tail_ag 19902 non-null float64 primary_cleaner.output.tail_pb 19902 non-null float64 primary_cleaner.output.tail_sol 19902 non-null float64 primary_cleaner.output.tail_au 19902 non-null float64 primary_cleaner.state.floatbank8_a_air 19902 non-null float64 primary_cleaner.state.floatbank8_a_level 19902 non-null float64 primary_cleaner.state.floatbank8_b_air 19902 non-null float64 primary_cleaner.state.floatbank8_b_level 19902 non-null float64 primary_cleaner.state.floatbank8_c_air 19902 non-null float64 primary_cleaner.state.floatbank8_c_level 19902 non-null float64 primary_cleaner.state.floatbank8_d_air 19902 non-null float64 primary_cleaner.state.floatbank8_d_level 19902 non-null float64 rougher.calculation.sulfate_to_au_concentrate 19902 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 19902 non-null float64 rougher.calculation.au_pb_ratio 19902 non-null float64 rougher.input.feed_ag 19902 non-null float64 rougher.input.feed_pb 19902 non-null float64 rougher.input.feed_rate 19902 non-null float64 rougher.input.feed_size 19902 non-null float64 rougher.input.feed_sol 19902 non-null float64 rougher.input.feed_au 19902 non-null float64 rougher.input.floatbank10_sulfate 19902 non-null float64 rougher.input.floatbank10_xanthate 19902 non-null float64 rougher.input.floatbank11_sulfate 19902 non-null float64 rougher.input.floatbank11_xanthate 19902 non-null float64 rougher.output.concentrate_ag 19902 non-null float64 rougher.output.concentrate_pb 19902 non-null float64 rougher.output.concentrate_sol 19902 non-null float64 rougher.output.concentrate_au 19902 non-null float64 rougher.output.recovery 19902 non-null float64 rougher.output.tail_ag 19902 non-null float64 rougher.output.tail_pb 19902 non-null float64 rougher.output.tail_sol 19902 non-null float64 rougher.output.tail_au 19902 non-null float64 rougher.state.floatbank10_a_air 19902 non-null float64 rougher.state.floatbank10_a_level 19902 non-null float64 rougher.state.floatbank10_b_air 19902 non-null float64 rougher.state.floatbank10_b_level 19902 non-null float64 rougher.state.floatbank10_c_air 19902 non-null float64 rougher.state.floatbank10_c_level 19902 non-null float64 rougher.state.floatbank10_d_air 19902 non-null float64 rougher.state.floatbank10_d_level 19902 non-null float64 rougher.state.floatbank10_e_air 19902 non-null float64 rougher.state.floatbank10_e_level 19902 non-null float64 rougher.state.floatbank10_f_air 19902 non-null float64 rougher.state.floatbank10_f_level 19902 non-null float64 secondary_cleaner.output.tail_ag 19902 non-null float64 secondary_cleaner.output.tail_pb 19902 non-null float64 secondary_cleaner.output.tail_sol 19902 non-null float64 secondary_cleaner.output.tail_au 19902 non-null float64 secondary_cleaner.state.floatbank2_a_air 19902 non-null float64 secondary_cleaner.state.floatbank2_a_level 19902 non-null float64 secondary_cleaner.state.floatbank2_b_air 19902 non-null float64 secondary_cleaner.state.floatbank2_b_level 19902 non-null float64 secondary_cleaner.state.floatbank3_a_air 19902 non-null float64 secondary_cleaner.state.floatbank3_a_level 19902 non-null float64 secondary_cleaner.state.floatbank3_b_air 19902 non-null float64 secondary_cleaner.state.floatbank3_b_level 19902 non-null float64 secondary_cleaner.state.floatbank4_a_air 19902 non-null float64 secondary_cleaner.state.floatbank4_a_level 19902 non-null float64 secondary_cleaner.state.floatbank4_b_air 19902 non-null float64 secondary_cleaner.state.floatbank4_b_level 19902 non-null float64 secondary_cleaner.state.floatbank5_a_air 19902 non-null float64 secondary_cleaner.state.floatbank5_a_level 19902 non-null float64 secondary_cleaner.state.floatbank5_b_air 19902 non-null float64 secondary_cleaner.state.floatbank5_b_level 19902 non-null float64 secondary_cleaner.state.floatbank6_a_air 19902 non-null float64 secondary_cleaner.state.floatbank6_a_level 19902 non-null float64 total_rough_feed 19902 non-null float64 total_rough_conc 19902 non-null float64 total_final_conc 19902 non-null float64 dtypes: float64(89), object(1) memory usage: 13.8+ MB . . plt.figure(figsize=(10,5)) plt.hist(df_clean[&#39;total_rough_feed&#39;], bins=100, alpha=0.5, label=&#39;total_rough_feed&#39;) plt.hist(df_clean[&#39;total_rough_conc&#39;], bins=100, alpha=0.5, label=&#39;total rough []&#39;) plt.hist(df_clean[&#39;total_final_conc&#39;], bins=100, alpha=0.5, label=&#39;total final []&#39;) plt.xlabel(&#39;Total concentrations of elements at various stages of purification&#39;) plt.ylabel(&#39;Frequency&#39;) plt.title(&quot;Histogram for total concentrations of elements&quot;) plt.legend(loc=&#39;upper right&#39;) plt.grid(True) . . We were able to successfully drop most of abnormal values from our distrubtion from the full dataset, train dataset, and test dataset. . Model Building . Evaluation Metrics . Write a function to calculate the final sMAPE value . def calc_smape(target, prediction): try: smape = 100/len(target) * np.abs(np.sum(2 * np.abs(prediction - target) / (np.abs(target) + np.abs(prediction)))) return smape except ZeroDivisionError: return 0 def calc_f_smape(target, prediction): r_smape = calc_smape(target[0], prediction[0]) f_smape = calc_smape(target[1], prediction[1]) final_smape = (0.25 * r_smape) + (0.75 * f_smape) return final_smape . smape_scorer = make_scorer(calc_f_smape, greater_is_better=False) smape_scorer . make_scorer(calc_f_smape, greater_is_better=False) . Parameter greater_is_better=False is used so further algorithms could understand that we want to minimize the score. . Prepare data subsets . # Dropping columns missing in original test dataset from train dataset and any other unnecessary columns df_train_complete = df_train_clean.drop([&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_au&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;final.output.tail_pb&#39;, &#39;final.output.tail_sol&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;primary_cleaner.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.tail_ag&#39;, &#39;rougher.output.tail_au&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;secondary_cleaner.output.tail_sol&#39;, &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;, &#39;date&#39;], axis=1) df_train_complete.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14612 entries, 0 to 16859 Data columns (total 54 columns): final.output.recovery 14612 non-null float64 primary_cleaner.input.sulfate 14612 non-null float64 primary_cleaner.input.depressant 14612 non-null float64 primary_cleaner.input.feed_size 14612 non-null float64 primary_cleaner.input.xanthate 14612 non-null float64 primary_cleaner.state.floatbank8_a_air 14612 non-null float64 primary_cleaner.state.floatbank8_a_level 14612 non-null float64 primary_cleaner.state.floatbank8_b_air 14612 non-null float64 primary_cleaner.state.floatbank8_b_level 14612 non-null float64 primary_cleaner.state.floatbank8_c_air 14612 non-null float64 primary_cleaner.state.floatbank8_c_level 14612 non-null float64 primary_cleaner.state.floatbank8_d_air 14612 non-null float64 primary_cleaner.state.floatbank8_d_level 14612 non-null float64 rougher.input.feed_ag 14612 non-null float64 rougher.input.feed_pb 14612 non-null float64 rougher.input.feed_rate 14612 non-null float64 rougher.input.feed_size 14612 non-null float64 rougher.input.feed_sol 14612 non-null float64 rougher.input.feed_au 14612 non-null float64 rougher.input.floatbank10_sulfate 14612 non-null float64 rougher.input.floatbank10_xanthate 14612 non-null float64 rougher.input.floatbank11_sulfate 14612 non-null float64 rougher.input.floatbank11_xanthate 14612 non-null float64 rougher.output.recovery 14612 non-null float64 rougher.state.floatbank10_a_air 14612 non-null float64 rougher.state.floatbank10_a_level 14612 non-null float64 rougher.state.floatbank10_b_air 14612 non-null float64 rougher.state.floatbank10_b_level 14612 non-null float64 rougher.state.floatbank10_c_air 14612 non-null float64 rougher.state.floatbank10_c_level 14612 non-null float64 rougher.state.floatbank10_d_air 14612 non-null float64 rougher.state.floatbank10_d_level 14612 non-null float64 rougher.state.floatbank10_e_air 14612 non-null float64 rougher.state.floatbank10_e_level 14612 non-null float64 rougher.state.floatbank10_f_air 14612 non-null float64 rougher.state.floatbank10_f_level 14612 non-null float64 secondary_cleaner.state.floatbank2_a_air 14612 non-null float64 secondary_cleaner.state.floatbank2_a_level 14612 non-null float64 secondary_cleaner.state.floatbank2_b_air 14612 non-null float64 secondary_cleaner.state.floatbank2_b_level 14612 non-null float64 secondary_cleaner.state.floatbank3_a_air 14612 non-null float64 secondary_cleaner.state.floatbank3_a_level 14612 non-null float64 secondary_cleaner.state.floatbank3_b_air 14612 non-null float64 secondary_cleaner.state.floatbank3_b_level 14612 non-null float64 secondary_cleaner.state.floatbank4_a_air 14612 non-null float64 secondary_cleaner.state.floatbank4_a_level 14612 non-null float64 secondary_cleaner.state.floatbank4_b_air 14612 non-null float64 secondary_cleaner.state.floatbank4_b_level 14612 non-null float64 secondary_cleaner.state.floatbank5_a_air 14612 non-null float64 secondary_cleaner.state.floatbank5_a_level 14612 non-null float64 secondary_cleaner.state.floatbank5_b_air 14612 non-null float64 secondary_cleaner.state.floatbank5_b_level 14612 non-null float64 secondary_cleaner.state.floatbank6_a_air 14612 non-null float64 secondary_cleaner.state.floatbank6_a_level 14612 non-null float64 dtypes: float64(54) memory usage: 6.1 MB . We removed the columns that were not present in the test dataset from training. These additional columns can confuse the model if they are useless (since they were not present in the original test dataset). If several features were added with random values to any model it will likely reduce the quality. That’s why it’s important to use only proper features. This is why we are prepping the train dataset for the model training. . # Getting columns final.output.recovery and rougher.output.recovery for test dataset and removing &#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39; columns df_test_complete = pd.merge(df_test_clean, df[[&#39;date&#39;, &#39;final.output.recovery&#39;, &#39;rougher.output.recovery&#39;]],on=&#39;date&#39;, how=&#39;left&#39;) df_test_complete = df_test_complete.drop([&#39;total_rough_feed&#39;, &#39;total_rough_conc&#39;, &#39;total_final_conc&#39;, &#39;date&#39;], axis=1) df_test_complete.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 5290 entries, 0 to 5289 Data columns (total 54 columns): primary_cleaner.input.sulfate 5290 non-null float64 primary_cleaner.input.depressant 5290 non-null float64 primary_cleaner.input.feed_size 5290 non-null float64 primary_cleaner.input.xanthate 5290 non-null float64 primary_cleaner.state.floatbank8_a_air 5290 non-null float64 primary_cleaner.state.floatbank8_a_level 5290 non-null float64 primary_cleaner.state.floatbank8_b_air 5290 non-null float64 primary_cleaner.state.floatbank8_b_level 5290 non-null float64 primary_cleaner.state.floatbank8_c_air 5290 non-null float64 primary_cleaner.state.floatbank8_c_level 5290 non-null float64 primary_cleaner.state.floatbank8_d_air 5290 non-null float64 primary_cleaner.state.floatbank8_d_level 5290 non-null float64 rougher.input.feed_ag 5290 non-null float64 rougher.input.feed_pb 5290 non-null float64 rougher.input.feed_rate 5290 non-null float64 rougher.input.feed_size 5290 non-null float64 rougher.input.feed_sol 5290 non-null float64 rougher.input.feed_au 5290 non-null float64 rougher.input.floatbank10_sulfate 5290 non-null float64 rougher.input.floatbank10_xanthate 5290 non-null float64 rougher.input.floatbank11_sulfate 5290 non-null float64 rougher.input.floatbank11_xanthate 5290 non-null float64 rougher.state.floatbank10_a_air 5290 non-null float64 rougher.state.floatbank10_a_level 5290 non-null float64 rougher.state.floatbank10_b_air 5290 non-null float64 rougher.state.floatbank10_b_level 5290 non-null float64 rougher.state.floatbank10_c_air 5290 non-null float64 rougher.state.floatbank10_c_level 5290 non-null float64 rougher.state.floatbank10_d_air 5290 non-null float64 rougher.state.floatbank10_d_level 5290 non-null float64 rougher.state.floatbank10_e_air 5290 non-null float64 rougher.state.floatbank10_e_level 5290 non-null float64 rougher.state.floatbank10_f_air 5290 non-null float64 rougher.state.floatbank10_f_level 5290 non-null float64 secondary_cleaner.state.floatbank2_a_air 5290 non-null float64 secondary_cleaner.state.floatbank2_a_level 5290 non-null float64 secondary_cleaner.state.floatbank2_b_air 5290 non-null float64 secondary_cleaner.state.floatbank2_b_level 5290 non-null float64 secondary_cleaner.state.floatbank3_a_air 5290 non-null float64 secondary_cleaner.state.floatbank3_a_level 5290 non-null float64 secondary_cleaner.state.floatbank3_b_air 5290 non-null float64 secondary_cleaner.state.floatbank3_b_level 5290 non-null float64 secondary_cleaner.state.floatbank4_a_air 5290 non-null float64 secondary_cleaner.state.floatbank4_a_level 5290 non-null float64 secondary_cleaner.state.floatbank4_b_air 5290 non-null float64 secondary_cleaner.state.floatbank4_b_level 5290 non-null float64 secondary_cleaner.state.floatbank5_a_air 5290 non-null float64 secondary_cleaner.state.floatbank5_a_level 5290 non-null float64 secondary_cleaner.state.floatbank5_b_air 5290 non-null float64 secondary_cleaner.state.floatbank5_b_level 5290 non-null float64 secondary_cleaner.state.floatbank6_a_air 5290 non-null float64 secondary_cleaner.state.floatbank6_a_level 5290 non-null float64 final.output.recovery 5290 non-null float64 rougher.output.recovery 5290 non-null float64 dtypes: float64(54) memory usage: 2.2 MB . . We added in the missing target columns to the test dataset from the original df. . x_train = df_train_complete.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) y_train = df_train_complete[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] # Get features/target for test sub dataset x_test = df_test_complete.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) y_test = df_test_complete[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] . Train different models . Evaluate them using cross-validation. Pick the best model and test it using the test sample. . def get_cv_scores(classifier, x_train, y_train): kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True) cross_val_smape = np.abs(cross_val_score(classifier, x_train, y_train.to_numpy(), scoring=smape_scorer, cv=kfold)) print(&#39;Mean Cross-Validation sMAPE Score:&#39;, cross_val_smape.mean()) print(&#39;Min Cross-Validation sMAPE Score:&#39;, cross_val_smape.min()) print(&#39;Max Cross-Validation sMAPE Score:&#39;, cross_val_smape.max()) . LinearRegression . lr = MultiOutputRegressor(LinearRegression()) print(&#39;Linear Regression model:&#39;) get_cv_scores(lr, x_train, y_train) . Linear Regression model: Mean Cross-Validation sMAPE Score: 34.341736599192494 Min Cross-Validation sMAPE Score: 7.541454024526066 Max Cross-Validation sMAPE Score: 47.99015041175999 . DecisionTree . dt = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE)) print(&#39;Decision Tree model:&#39;) get_cv_scores(dt, x_train, y_train) . Decision Tree model: Mean Cross-Validation sMAPE Score: 2.1941193666185566 Min Cross-Validation sMAPE Score: 0.6229571145328955 Max Cross-Validation sMAPE Score: 3.9114996607788086 . RandomForest . rf = MultiOutputRegressor(RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=100)) print(&#39;Random Forest model:&#39;) get_cv_scores(rf, x_train, y_train) . Random Forest model: Mean Cross-Validation sMAPE Score: 2.3734912002748216 Min Cross-Validation sMAPE Score: 1.2144453600511473 Max Cross-Validation sMAPE Score: 4.28980393715649 . Conclusion . Cross-Validation of 5 fold sMAPE Scores Mean Min Max . Linear Regression | 34.34 | 7.54 | 47.99 | . Decision Tree | 2.19 | 0.623 | 3.91 | . Random Forest | 2.37 | 1.21 | 4.29 | . Looking that the sMAPE scores from the cross-validation of 5 fold, Decision Tree Regression model has the best lowest sMAPE scores across the mean, min, and max. . Parameter tuning . param_grid = { &#39;estimator__max_depth&#39;: range(1, 15), &#39;estimator__min_samples_leaf&#39;: range(1, 15) } # Create a based model model = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE)) # Instantiate the grid search model grid_search = GridSearchCV(estimator= model, param_grid= param_grid, scoring= smape_scorer, cv= 3, n_jobs= -1, verbose= 2) . # Fit the grid search to the data grid_search.fit(x_train, y_train.to_numpy()) best_param = grid_search.best_params_ print(&quot; n The best score across ALL searched params: n&quot;, grid_search.best_score_) print(&quot; n The best parameters across ALL searched params: n&quot;, grid_search.best_params_) . Fitting 3 folds for each of 196 candidates, totalling 588 fits [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... . [Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=-1)]: Done 1 out of 1 | elapsed: 0.1s remaining: 0.0s . [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=1, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=2, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=3, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=4, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=5, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=6, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=7, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=8, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=1, estimator__min_samples_leaf=9, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=10, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=11, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=12, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=13, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=1, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=1, estimator__min_samples_leaf=14, total= 0.1s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=1, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=2, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=3, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=4, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=5, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=6, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=7, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=8, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=2, estimator__min_samples_leaf=9, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=10, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=11, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=12, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=13, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=2, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=2, estimator__min_samples_leaf=14, total= 0.2s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=1, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=2, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=3, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=4, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=5, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=6, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=7, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=8, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=3, estimator__min_samples_leaf=9, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=10, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=11, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=12, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=13, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=3, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=3, estimator__min_samples_leaf=14, total= 0.3s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=1, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=2, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=3, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=4, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=5, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=6, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=7, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=8, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=4, estimator__min_samples_leaf=9, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=10, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=11, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=12, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=13, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=4, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=4, estimator__min_samples_leaf=14, total= 0.4s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=4, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=8, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=5, estimator__min_samples_leaf=9, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=10, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=12, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=5, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=5, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=1, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=5, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=7, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=6, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=13, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=6, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=6, estimator__min_samples_leaf=14, total= 0.5s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=1, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=2, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=3, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=4, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=5, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=6, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=7, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=8, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=7, estimator__min_samples_leaf=9, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=10, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=11, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=12, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=13, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=7, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=7, estimator__min_samples_leaf=14, total= 0.6s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=1, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=2, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=3, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=4, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=5, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=6, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=7, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=8, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=8, estimator__min_samples_leaf=9, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=10, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=11, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=12, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=13, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=8, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=8, estimator__min_samples_leaf=14, total= 0.7s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=9, estimator__min_samples_leaf=1 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=2 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=3 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=4 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=5 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=6 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=7 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=8 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=9 ........... [CV] estimator__max_depth=9, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=10 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=11 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=12 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=13 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=9, estimator__min_samples_leaf=14 .......... [CV] estimator__max_depth=9, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=1, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=2, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=3, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=4, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=5, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=6, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=7, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=8, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=10, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=11, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=12, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=13, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=10, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=10, estimator__min_samples_leaf=14, total= 0.8s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=1, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=2, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=3, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=4, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=5, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=6, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=7, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=8, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=11, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=11, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=1, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=9, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=11, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=12, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=13, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=12, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=12, estimator__min_samples_leaf=14, total= 0.9s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=2, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=13, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=3, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=4, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=5, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=6, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=7, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=8, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=13, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=13, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=1 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=1, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=2 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=2, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=3 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=3, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=4 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=4, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=5 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=5, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=6 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=6, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=7 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=7, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=8 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=8, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=9 .......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=9, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=10 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=10, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.1s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=11 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=11, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=12 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=12, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=13 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=13, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s [CV] estimator__max_depth=14, estimator__min_samples_leaf=14 ......... [CV] estimator__max_depth=14, estimator__min_samples_leaf=14, total= 1.0s . [Parallel(n_jobs=-1)]: Done 588 out of 588 | elapsed: 6.2min finished . The best score across ALL searched params: -2.457959649299341 The best parameters across ALL searched params: {&#39;estimator__max_depth&#39;: 2, &#39;estimator__min_samples_leaf&#39;: 13} . . Model Selection . model = MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE, max_depth=2, min_samples_leaf=13)) model.fit(x_train, y_train.to_numpy()) prediction = model.predict(x_test) . smape = calc_smape(y_test.to_numpy(), prediction) print(&#39;Final sMAPE score: &#39;, smape) . Final sMAPE score: 14.464377261082916 . Overall Conclusion . To fill the null values in the original datasets, we used fillna(method=&#39;ffill&#39;) since the data is ordered by time. MAE calculation values were also used to replace the original values in the datasets. We dropped the abnormal values for concentrations that were equal to and/or close to 0. And then we trained Linear Regression, Decision Tree Regressor, and Random Forest Regressor using the train dataset with a cross-validation of 5 fold to obtain sMAPE scores. Decision Tree had the best lowest sMAPE scores overall and was selected undergo hyper parameterization with GridSearchCV. Below sums up the final Decision Tree model and it&#39;s parameters: . Decision Tree Regression . Parameters: | random_state= 42, max_depth=2, min_samples_leaf=13 | . Final sMAPE score | 14.4644 | . Testing the final model using the test dataset gave us the final sMAPE score of 14.46% which is higher than our cross-validation mean score which used the only used train dataset. .",
            "url": "https://cmdang-mochi.github.io/ds-projects/machine%20learning/python/pandas/numpy/mathplotlib/scikit-learn/2020/12/16/gold_recovery.html",
            "relUrl": "/machine%20learning/python/pandas/numpy/mathplotlib/scikit-learn/2020/12/16/gold_recovery.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Profit Risks for New Oil Wells",
            "content": "Project Description . Analyze potential profit and risks for an oil company. . Create a model that will analyze different proposed locations for a new oil well that will maximize profits while minimizing risk. . Goal is to find the best place for a new oil well using provided data from three different regions. . Steps to choose the location: . Collect the oil well parameters in the selected region: oil quality and volume of reserves; | Build a model for predicting the volume of reserves in the new wells; | Pick the oil wells with the highest estimated values; | Pick the region with the highest total profit for the selected oil wells. | . There is data on oil samples from three regions. Parameters of each oil well in the region are already known. Build a model that will help to pick the region with the highest profit margin. Analyze potential profit and risks using the Bootstrapping technique. . Data description . Geological exploration data for the three regions are stored in files: . id — unique oil well identifier | f0, f1, f2 — three features of points (their specific meaning is unimportant, but the features themselves are significant) | product — volume of reserves in the oil well (thousand barrels). | . Conditions: . Only linear regression is suitable for model training (the rest are not sufficiently predictable). | When exploring the region, a study of 500 points is carried with picking the best 200 points for the profit calculation. | The budget for development of 200 oil wells is 100 USD million. | One barrel of raw materials brings 4.5 USD of revenue The revenue from one unit of product is 4,500 dollars (volume of reserves is in thousand barrels). | After the risk evaluation, keep only the regions with the risk of losses lower than 2.5%. From the ones that fit the criteria, the region with the highest average profit should be selected. | . The data is synthetic: contract details and well characteristics are not disclosed. . Import Libraries . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from numpy.random import RandomState from tabulate import tabulate from scipy import stats as st . Exploratory data analysis . Load data . geo_0 = pd.read_csv(&#39;/content/datasets/geo_data_0.csv&#39;) geo_1 = pd.read_csv(&#39;/content/datasets/geo_data_1.csv&#39;) geo_2 = pd.read_csv(&#39;/content/datasets/geo_data_2.csv&#39;) . def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . # Opening dataset for geo_0 get_information(geo_0) . Head: . id f0 f1 f2 product . 0 txEyH | 0.705745 | -0.497823 | 1.221170 | 105.280062 | . 1 2acmU | 1.334711 | -0.340164 | 4.365080 | 73.037750 | . 2 409Wp | 1.022732 | 0.151990 | 1.419926 | 85.265647 | . 3 iJLyR | -0.032172 | 0.139033 | 2.978566 | 168.620776 | . 4 Xdl7t | 1.988431 | 0.155413 | 4.751769 | 154.036647 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 0.500419 | 0.250143 | 2.502647 | 92.500000 | . std 0.871832 | 0.504433 | 3.248248 | 44.288691 | . min -1.408605 | -0.848218 | -12.088328 | 0.000000 | . 25% -0.072580 | -0.200881 | 0.287748 | 56.497507 | . 50% 0.502360 | 0.250252 | 2.515969 | 91.849972 | . 75% 1.073581 | 0.700646 | 4.715088 | 128.564089 | . max 2.362331 | 1.343769 | 16.003790 | 185.364347 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_0 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . # Opening dataset for geo_1 get_information(geo_1) . Head: . id f0 f1 f2 product . 0 kBEdx | -15.001348 | -8.276000 | -0.005876 | 3.179103 | . 1 62mP7 | 14.272088 | -3.475083 | 0.999183 | 26.953261 | . 2 vyE1P | 6.263187 | -5.948386 | 5.001160 | 134.766305 | . 3 KcrkZ | -13.081196 | -11.506057 | 4.999415 | 137.945408 | . 4 AHL4O | 12.702195 | -8.147433 | 5.004363 | 134.766305 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 1.141296 | -4.796579 | 2.494541 | 68.825000 | . std 8.965932 | 5.119872 | 1.703572 | 45.944423 | . min -31.609576 | -26.358598 | -0.018144 | 0.000000 | . 25% -6.298551 | -8.267985 | 1.000021 | 26.953261 | . 50% 1.153055 | -4.813172 | 2.011479 | 57.085625 | . 75% 8.621015 | -1.332816 | 3.999904 | 107.813044 | . max 29.421755 | 18.734063 | 5.019721 | 137.945408 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_1 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . # Opening dataset for geo_2 get_information(geo_2) . Head: . id f0 f1 f2 product . 0 fwXo0 | -1.146987 | 0.963328 | -0.828965 | 27.758673 | . 1 WJtFt | 0.262778 | 0.269839 | -2.530187 | 56.069697 | . 2 ovLUW | 0.194587 | 0.289035 | -5.586433 | 62.871910 | . 3 q6cA6 | 2.236060 | -0.553760 | 0.930038 | 114.572842 | . 4 WPMUX | -0.515993 | 1.716266 | 5.899011 | 149.600746 | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 100000 entries, 0 to 99999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 100000 non-null object 1 f0 100000 non-null float64 2 f1 100000 non-null float64 3 f2 100000 non-null float64 4 product 100000 non-null float64 dtypes: float64(4), object(1) memory usage: 3.8+ MB . None . **************************************************************************************************** Describe: . f0 f1 f2 product . count 100000.000000 | 100000.000000 | 100000.000000 | 100000.000000 | . mean 0.002023 | -0.002081 | 2.495128 | 95.000000 | . std 1.732045 | 1.730417 | 3.473445 | 44.749921 | . min -8.760004 | -7.084020 | -11.970335 | 0.000000 | . 25% -1.162288 | -1.174820 | 0.130359 | 59.450441 | . 50% 0.009424 | -0.009482 | 2.484236 | 94.925613 | . 75% 1.158535 | 1.163678 | 4.858794 | 130.595027 | . max 7.238262 | 7.844801 | 16.739402 | 190.029838 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . id 0 | 0.0 | . f0 0 | 0.0 | . f1 0 | 0.0 | . f2 0 | 0.0 | . product 0 | 0.0 | . **************************************************************************************************** Shape: . (100000, 5) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Dataset for geo_2 show to have 100000 entries with no null/missing values and datatypes seem to be correct. . From all the datasets, we might be able to drop the id column as it does not provide the models much information. . Preprocessing data . geo_0_data = geo_0.drop([&#39;id&#39;], axis=1) display(geo_0_data.head()) geo_1_data = geo_1.drop([&#39;id&#39;], axis=1) display(geo_1_data.head()) geo_2_data = geo_2.drop([&#39;id&#39;], axis=1) display(geo_2_data.head()) . f0 f1 f2 product . 0 0.705745 | -0.497823 | 1.221170 | 105.280062 | . 1 1.334711 | -0.340164 | 4.365080 | 73.037750 | . 2 1.022732 | 0.151990 | 1.419926 | 85.265647 | . 3 -0.032172 | 0.139033 | 2.978566 | 168.620776 | . 4 1.988431 | 0.155413 | 4.751769 | 154.036647 | . f0 f1 f2 product . 0 -15.001348 | -8.276000 | -0.005876 | 3.179103 | . 1 14.272088 | -3.475083 | 0.999183 | 26.953261 | . 2 6.263187 | -5.948386 | 5.001160 | 134.766305 | . 3 -13.081196 | -11.506057 | 4.999415 | 137.945408 | . 4 12.702195 | -8.147433 | 5.004363 | 134.766305 | . f0 f1 f2 product . 0 -1.146987 | 0.963328 | -0.828965 | 27.758673 | . 1 0.262778 | 0.269839 | -2.530187 | 56.069697 | . 2 0.194587 | 0.289035 | -5.586433 | 62.871910 | . 3 2.236060 | -0.553760 | 0.930038 | 114.572842 | . 4 -0.515993 | 1.716266 | 5.899011 | 149.600746 | . The datasets geo_0, geo_1, geo_2 all have 100,000 entries with no missing values and have datatypes that seem to be correct. We were able to drop the id column as this would not provide much information for our model. . Split Data . RANDOM_STATE = 12345 #Random_State def get_train_valid(df): df_train, df_valid = train_test_split(df, test_size=0.25, random_state=RANDOM_STATE) # Splits data up to 75% train and 25% test return df_train, df_valid . geo0_target = geo_0_data[&#39;product&#39;] geo0_features = geo_0_data.drop([&#39;product&#39;], axis=1) geo0_x_train, geo0_x_valid = get_train_valid(geo0_features) geo0_y_train, geo0_y_valid = get_train_valid(geo0_target) assert geo0_x_train.shape[0] == geo0_y_train.shape[0] assert geo0_x_valid.shape[0] == geo0_y_valid.shape[0] print(&#39;geo_0 datasets: n&#39;) print(&#39;Train:&#39;, geo0_x_train.shape, &#39; Target Train:&#39;, geo0_y_train.shape) print(&#39;Validation:&#39;, geo0_x_valid.shape, &#39; Target Validation:&#39;, geo0_y_valid.shape) . geo_0 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . geo1_target = geo_1_data[&#39;product&#39;] geo1_features = geo_1_data.drop([&#39;product&#39;], axis=1) geo1_x_train, geo1_x_valid = get_train_valid(geo1_features) geo1_y_train, geo1_y_valid = get_train_valid(geo1_target) assert geo1_x_train.shape[0] == geo1_y_train.shape[0] assert geo1_x_valid.shape[0] == geo1_y_valid.shape[0] print(&#39;geo_1 datasets: n&#39;) print(&#39;Train:&#39;, geo1_x_train.shape, &#39; Target Train:&#39;, geo1_y_train.shape) print(&#39;Validation:&#39;, geo1_x_valid.shape, &#39; Target Validation:&#39;, geo1_y_valid.shape) . geo_1 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . geo2_target = geo_2_data[&#39;product&#39;] geo2_features = geo_2_data.drop([&#39;product&#39;], axis=1) geo2_x_train, geo2_x_valid = get_train_valid(geo2_features) geo2_y_train, geo2_y_valid = get_train_valid(geo2_target) assert geo2_x_train.shape[0] == geo2_y_train.shape[0] assert geo2_x_valid.shape[0] == geo2_y_valid.shape[0] print(&#39;geo_2 datasets: n&#39;) print(&#39;Train:&#39;, geo2_x_train.shape, &#39; Target Train:&#39;, geo2_y_train.shape) print(&#39;Validation:&#39;, geo2_x_valid.shape, &#39; Target Validation:&#39;, geo2_y_valid.shape) . geo_2 datasets: Train: (75000, 3) Target Train: (75000,) Validation: (25000, 3) Target Validation: (25000,) . Needs fixing: We don&#39;t need valid part in the task because we don&#39;t tune parameters. Create only train(75%) and test(25%). Updated to only have validation(25%) and train(75%) datasets, since we do not need the test dataset for tunning parameters on our models . The dataset for the three different regions were successfully split into train and validation datasets with a 75:25 ratio. There is no testing sub-dataset as the models will not be hyper-tuned. . Model building . def LinReg_sanity_check(x_train, x_valid, y_train, y_valid): model = LinearRegression() model.fit(x_train, y_train) predictions = model.predict(x_valid) print(&#39;Model validation/prediction datasets scores&#39;) print(&#39;Accuracy:&#39;, model.score(x_valid, y_valid)) print(&#39;R2:&#39;, r2_score(y_valid, predictions)) print(&#39;RMSE:&#39;, mean_squared_error(y_valid, predictions, squared=False)) . geo_0 region . geo0_model = LinearRegression() geo0_model.fit(geo0_x_train, geo0_y_train) geo0_predictions = geo0_model.predict(geo0_x_valid) print(&#39;Sanity check for geo_0 model:&#39;) LinReg_sanity_check(geo0_x_train, geo0_x_valid, geo0_y_train, geo0_y_valid) print(&#39; nAverage volume of predicted reserves in geo_0 (thousand barrels):&#39;, geo0_predictions.mean()) . Sanity check for geo_0 model: Model validation/prediction datasets scores Accuracy: 0.27994321524487786 R2: 0.27994321524487786 RMSE: 37.5794217150813 Average volume of predicted reserves in geo_0 (thousand barrels): 92.59256778438038 . For the data in geo_0, the accuracy, R2 and RMSE scores are pretty terrble. The accuracy and R2 score are very low and the RMSE score is very big. . geo_1 region . geo1_model = LinearRegression() geo1_model.fit(geo1_x_train, geo1_y_train) geo1_predictions = geo1_model.predict(geo1_x_valid) print(&#39;Sanity check for geo_1 model: n&#39;) LinReg_sanity_check(geo1_x_train, geo1_x_valid, geo1_y_train, geo1_y_valid) print(&#39; nAverage volume of predicted reserves in geo_1 (thousand barrels):&#39;, geo1_predictions.mean()) . Sanity check for geo_1 model: Model validation/prediction datasets scores Accuracy: 0.9996233978805126 R2: 0.9996233978805127 RMSE: 0.893099286775616 Average volume of predicted reserves in geo_1 (thousand barrels): 68.728546895446 . The scores for data in geo_1 are actually pretty good with accuracy and R2 score pretty close to 1 and RMSE score being very low. . geo_2 region . geo2_model = LinearRegression() geo2_model.fit(geo2_x_train, geo2_y_train) geo2_predictions = geo2_model.predict(geo2_x_valid) print(&#39;Sanity check for geo_2 model: n&#39;) LinReg_sanity_check(geo2_x_train, geo2_x_valid, geo2_y_train, geo2_y_valid) print(&#39; nAverage volume of predicted reserves in geo_2 (thousand barrels):&#39;, geo2_predictions.mean()) . Sanity check for geo_2 model: Model validation/prediction datasets scores Accuracy: 0.20524758386040443 R2: 0.20524758386040443 RMSE: 40.02970873393434 Average volume of predicted reserves in geo_2 (thousand barrels): 94.96504596800489 . Similar to the model for geo_0, the scores here for geo_2 model are not that great with a very low accuracy and R2 score and high RMSE score. . Conclusion . The table below shows each models&#39; results using the valdiation datasets to make predictions: . . geo_0 geo_1 geo_2 . Accuracy Score | 0.2799 | 0.9996 | 0.2052 | . R2 Score | 0.2799 | 0.9996 | 0.2052 | . RMSE Score | 37.579 | 0.8903 | 40.030 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . . R2 score shows the relative measure of fit, while RMSE is an absolute measure of fit. Lower values of RMSE indicate a better fit. . The model for geo_1 has the best scores for accuracy, R2, and RMSE. With accuracy and R2 being close to 1 and with a very small RMSE it seems like the model can predict pretty well using the dataset provided for geo_1. However, predicted average volume of reserves in ge0_1 is the lowest of the 3 regions being at 68.968 thousands barrels. . The models for geo_0 and geo_2 performed pretty similar for the datasets given for these two regions. The accuracy and R2 score are very small ranging around 0.20 - 0.30 and have RMSE score around 40. In comparison, though these two have similar scores in accuracy, R2 and RMSE, the model for geo_0 did slightly better with a slightly higher accuracy and R2 score and a bit lower RMSE score compared to the model for geo_2. . The model for geo_2 predicted largest average volume of reserves with 94.955 thousands barrels. With geo_0 coming in second with the largest predicted average volume of reserves at 92.708 thousands barrels. geo_1 came in last with the smallest predicted average volume at 68.968 thousand barrels. . Having such a large RMSE score for the models for geo_0 and geo_2 shows that perhaps a linear regression model might not be ideal for these two regions. . Profit calculation . BUDGET = 100000000 # budget for development of 200 oil wells is 100 USD million POINT_PER_BUDGET = 200 # number of wells in the budget PRODUCT_PRICE = 4500 # revenue from one unit of product is 4,500 dollars (volume of reserves is in thousand barrels) #Calculation for the volume of reserves sufficient for developing a new well without losses volume_no_loss = (BUDGET/POINT_PER_BUDGET) / PRODUCT_PRICE print(&#39;Volume of reserves sufficient for developing a new well without losses:&#39;, volume_no_loss, &#39;(thousand barrels)&#39;) def average_volume(df): return df[&#39;product&#39;].mean() print(&#39; nAverage volume of reserves in each region:&#39;) print(&#39;* geo_0: &#39;, average_volume(geo_0), &#39;(thousand barrels)&#39;) print(&#39;* geo_1: &#39;, average_volume(geo_1), &#39;(thousand barrels)&#39;) print(&#39;* geo_2: &#39;, average_volume(geo_2), &#39;(thousand barrels)&#39;) . Volume of reserves sufficient for developing a new well without losses: 111.11111111111111 (thousand barrels) Average volume of reserves in each region: * geo_0: 92.49999999999976 (thousand barrels) * geo_1: 68.82500000002561 (thousand barrels) * geo_2: 95.00000000000041 (thousand barrels) . Conclusion . The minimum volume of reserves need to develop a new well without losses is 111.11 (thousand barrels). . Tables showing average volume in each region: . geo_0 geo_1 geo_2 . Average volume of reserves (thousand barrels) | 92.499 | 68.825 | 95.00 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . Just by looking at the average volume of reserves in reach region (actual and predicted volumes), it doesn&#39;t seem that a developing a new well would be profitable. We would have to look at the top producing wells (actual and predicted) in each region to see if the calculation differs. . Profit from a set of selected oil wells and model predictions . def revenue(y_valid, predictions, count): predictions = pd.Series(predictions) y_valid = pd.Series(y_valid.values) predict_sorted = predictions.sort_values(ascending=False) selected_wells = y_valid[predict_sorted.index][:count] return PRODUCT_PRICE * selected_wells.sum() def profit(revenue): profit = revenue - BUDGET return profit . rev_geo0 = revenue(geo0_y_valid, geo0_predictions, 200) print(&#39;Predicted profit for region geo_0 with top best 200 wells:&#39;, profit(rev_geo0).round(2)) . Predicted profit for region geo_0 with top best 200 wells: 33208260.43 . rev_geo1 = revenue(geo1_y_valid, geo1_predictions, 200) print(&#39;Predicted profit for region geo_1 with top best 200 wells:&#39;, profit(rev_geo1).round(2)) . Predicted profit for region geo_1 with top best 200 wells: 24150866.97 . rev_geo2 = revenue(geo2_y_valid, geo2_predictions, 200) print(&#39;Predicted profit for region geo_2 with top best 200 wells:&#39;, profit(rev_geo2).round(2)) . Predicted profit for region geo_2 with top best 200 wells: 27103499.64 . Conclusion . From the calculations in this section, it&#39;s best to develop new wells in region geo_0. The calculation for predicted profit is highest in this region at 33,208,260.43 USD. geo_1 had the lowest predicted profit at 24,150,866.97 USD and geo_2 came out in the middle at 27,103,499.64 USD. . Calculate risks and profit for each region . def profit_distribution(y_valid, predictions): y_valid = pd.Series(y_valid.values) state = np.random.RandomState(12345) values = [] for i in range(1000): target_subsample = y_valid.sample(n=500, replace=True, random_state=state) predict_subsample = predictions[target_subsample.index] rev = revenue(target_subsample, predict_subsample, 200) values.append(rev) values = pd.Series(values) values = values.sort_values() profit_values = profit(values) return profit_values #Calculation for confidence interval at 95% def confidence_interval(profit_values): confidence_interval = st.t.interval( 0.95, len(profit_values)-1, profit_values.mean(), profit_values.sem()) return print(&quot;95% confidence interval:&quot;, confidence_interval) #Value at Risk (Risk of Losses) | Confidence Level calculation def risk_of_loss(profit_values): &quot;&quot;&quot; Value at risk (VaR) is a measure of the risk of loss for investments. It estimates how much a set of investments might lose (with a given probability), given normal market conditions, in a set time period such as a day &quot;&quot;&quot; upper = profit_values.quantile(0.975) lower = profit_values.quantile(0.025) return print(&#39; n&#39;, tabulate([[&#39;2.5%&#39;, lower], [&#39;97.5%&#39;, upper]], floatfmt=&#39;.2f&#39;, headers=[&#39;Confidence Level&#39;, &#39;Value at Risk&#39;])) #Sum of loss (negative profit) within our 1000 sample def loss(profit_values): loss = sum(i for i in profit_values if i &lt; 0) return loss #Get number of instances where profit sample is negative def count_loss(profit_values): num_loss = profit_values.lt(0).sum().sum() return num_loss #Sum of gain (positive profit) within our 1000 sample def gain(profit_values): gain = sum(i for i in profit_values if i &gt; 0) return gain #Get number of instances where profit sample is positive def count_gain(profit_values): num_gain = profit_values.gt(0).sum().sum() return num_gain #Calculation for profit/loss ratio def proft_loss_ratio(profit_values): total_gain = gain(profit_values) total_loss = loss(profit_values) total_num_gain = count_gain(profit_values) total_num_loss = count_loss(profit_values) ratio = ((total_gain/total_num_gain) / (total_loss/total_num_loss)) return ratio #Loss probability calculation def loss_prob(profit_values): &quot;&quot;&quot; Loss probability is percentage of negative values in profit array &quot;&quot;&quot; total_num_loss = count_loss(profit_values) #Count of negative values in profit array prob = total_num_loss / 1000 #Sample size of 1000 for profit array return prob . geo0_profit_values = profit_distribution(geo0_y_valid, geo0_predictions) print(&#39;Average profit for region geo_0:&#39;, round(geo0_profit_values.mean(), 2)) confidence_interval(geo0_profit_values) risk_of_loss(geo0_profit_values) #Loss probability print(&#39; nLoss probability for geo_0:&#39;, loss_prob(geo0_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo0_profit_values))) . Average profit for region geo_0: 3961649.85 95% confidence interval: (3796203.151479729, 4127096.544567701) Confidence Level Value at Risk 2.5% -1112155.46 97.5% 9097669.42 Loss probability for geo_0: 0.069 or 6.90% . geo1_profit_values = profit_distribution(geo1_y_valid, geo1_predictions) print(&#39;Average profit for region geo_1:&#39;, round(geo1_profit_values.mean(), 2)) confidence_interval(geo1_profit_values) risk_of_loss(geo1_profit_values) #Loss probability print(&#39; nLoss probability for geo_1:&#39;, loss_prob(geo1_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo1_profit_values))) . Average profit for region geo_1: 4560451.06 95% confidence interval: (4431472.486639012, 4689429.629094217) Confidence Level Value at Risk 2.5% 338205.09 97.5% 8522894.54 Loss probability for geo_1: 0.015 or 1.50% . geo2_profit_values = profit_distribution(geo2_y_valid, geo2_predictions) print(&#39;Average profit for region geo_2:&#39;, round(geo2_profit_values.mean(), 2)) confidence_interval(geo2_profit_values) risk_of_loss(geo2_profit_values) #Loss probability print(&#39; nLoss probability for geo_2:&#39;, loss_prob(geo2_profit_values), &#39;or&#39;, &#39;{:.2%}&#39;.format(loss_prob(geo2_profit_values))) . Average profit for region geo_2: 4044038.67 95% confidence interval: (3874457.974712804, 4213619.356654332) Confidence Level Value at Risk 2.5% -1633504.13 97.5% 9503595.75 Loss probability for geo_2: 0.076 or 7.60% . Conclusion . Based on the calculations in this section, the best region get develop new oil wells would be region geo_1. Out of the 1000 samples that was obtained in each region, geo_1 had the smallest probablity of loss at 1.50%. The calculated average profit for region geo_1 out of the 1000 sample slice came in at 4,560,451.06 USD which was the highest average profit out of the three regions. The table below summarizes the findings in this secion: . geo_0 geo_1 geo_2 . Average profit (USD) | 3,961,649.85 | 4,560,451.06 | 4,044,038.67 | . 95% confidence interval | (3796203.15, 4127096.54) | (4431472.49, 4689429.63) | (3874457.97, 4213619.36) | . Risk of loss at 2.5% confidence level | -1112155.46 | 338205.09 | -1633504.13 | . Risk of loss at 97.5% confidence level | 9097669.42 | 8522894.54 | 9503595.75 | . Loss probability (%) | 6.90% | 1.50% | 7.60% | . Overall Conclusion . In conlusion, with the calculations made for the provided datasets it seems as though region geo_1 is the best region develop new wells. The model for geo_1 had the best scores. The predicted profit for geo_1 came in second out of the three regions which isn&#39;t bad. And overall, geo_1 when sampled 1000 wells had the lowest loss probablity and had highest average profit. . The table below summarize the sections we calculated: . geo_0 geo_1 geo_2 . Accuracy Score | 0.2799 | 0.9996 | 0.2052 | . R2 Score | 0.2799 | 0.9996 | 0.2052 | . RMSE Score | 37.579 | 0.8903 | 40.030 | . Average volume of reserves (thousand barrels) | 92.499 | 68.825 | 95.00 | . Average predicted volume reserves (thousand barrels) | 92.592 | 68.728 | 94.965 | . Predicted profit (USD) | 33,208,260.43 | 24,150,866.97 | 27,103,499.64 | . Average profit (USD) | 3,961,649.85 | 4,560,451.06 | 4,044,038.67 | . 95% confidence interval | (3796203.15, 4127096.54) | (4431472.49, 4689429.63) | (3874457.97, 4213619.36) | . Risk of loss at 2.5% confidence level | -1112155.46 | 338205.09 | -1633504.13 | . Risk of loss at 97.5% confidence level | 9097669.42 | 8522894.54 | 9503595.75 | . Loss probability (%) | 6.90% | 1.50% | 7.60% | .",
            "url": "https://cmdang-mochi.github.io/ds-projects/jupyter/pandas.%20numpy/scikit-learn/tabulate/scipy/2020/11/22/profit_risks_for_new_oil_wells.html",
            "relUrl": "/jupyter/pandas.%20numpy/scikit-learn/tabulate/scipy/2020/11/22/profit_risks_for_new_oil_wells.html",
            "date": " • Nov 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Video Game Success Patterns/Trends",
            "content": "Project description . User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. You need to identify patterns that determine whether a game succeeds or not. This will allow you to spot potential big winners and plan advertising campaigns. . In front of you is data going back to 2016. Let’s imagine that it’s December 2016 and you’re planning a campaign for 2017. . (The important thing is to get experience working with data. It doesn&#39;t really matter whether you&#39;re forecasting 2017 sales based on data from 2016 or 2027 sales based on data from 2016.) . The dataset contains the abbreviation ESRB. The Entertainment Software Rating Board evaluates a game&#39;s content and assigns an age rating such as Teen or Mature. . Data description . Data is from open sources 2016 - historical game sales data: user and expert ratings, country sales, genres and platforms, ESRB ratings . Name - the name of the game | Platform - platform | Year_of_Release - release year | Genre - game genre | NA_sales - sales in North America (millions of copies sold) | EU_sales - sales in Europe (millions of copies sold) | JP_sales - sales in Japan (millions of copies sold) | Other_sales - sales in other countries (millions of copies sold) | Critic_Score - critic scores (maximum 100) | User_Score - user rating (maximum 10) | Rating - rating by the ESRB (Entertainment Software Rating Board), which determines the rating of the game and the appropriate age category | . Import Libraries . # Import in libraries to use in project import pandas as pd import numpy as np import matplotlib.pyplot as plt import re from scipy import stats as st import seaborn as sns import matplotlib.ticker as ticker import seaborn.apionly as sns . /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The examples.directory rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2. In the future, examples will be found relative to the &#39;datapath&#39; directory. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/_collections_abc.py:841: MatplotlibDeprecationWarning: The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3. self[key] = other[key] /opt/conda/lib/python3.7/site-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version. warnings.warn(msg, UserWarning) . . Load Data . def get_information(df): &quot;&quot;&quot; Prints general info about the dataframe to get an idea of what it looks like&quot;&quot;&quot; print(&#39;Head: n&#39;) display(df.head()) print(&#39;*&#39;*100, &#39; n&#39;) # Prints a break to seperate print data print(&#39;Info: n&#39;) display(df.info()) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Describe: n&#39;) display(df.describe()) display(df.describe(include=&#39;object&#39;)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Columns with nulls: n&#39;) display(get_null_df(df,4)) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Shape: n&#39;) display(df.shape) print(&#39;*&#39;*100, &#39; n&#39;) print(&#39;Duplicated: n&#39;) print(&#39;Number of duplicated rows: {}&#39;.format(df.duplicated().sum())) def get_null_df(df, num): &quot;&quot;&quot;Gets percentage of null values per column per dataframe&quot;&quot;&quot; df_nulls = pd.DataFrame(df.isna().sum(), columns=[&#39;missing_values&#39;]) df_nulls[&#39;percent_of_nulls&#39;] = round(df_nulls[&#39;missing_values&#39;] / df.shape[0], num) *100 return df_nulls def get_null(df): &quot;&quot;&quot;Gets percentage of null values in dataframe&quot;&quot;&quot; count = 0 df = df.copy() s = (df.isna().sum() / df.shape[0]) for column, percent in zip(s.index, s.values): num_of_nulls = df[column].isna().sum() if num_of_nulls == 0: continue else: count += 1 print(&#39;Columns {} has {:.{}%} percent of Nulls, and {} number of nulls&#39;.format(column, percent, num, num_of_nulls)) if count !=0: print(&#39;Number of columns with NA: {}&#39;.format(count)) else: print(&#39; nNo NA columns found&#39;) . df_games = pd.read_csv(&#39;/datasets/games.csv&#39;) # this is correct path get_information(df_games) . Head: . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 | Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 | Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 | Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 | Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 | Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . **************************************************************************************************** Info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): Name 16713 non-null object Platform 16715 non-null object Year_of_Release 16446 non-null float64 Genre 16713 non-null object NA_sales 16715 non-null float64 EU_sales 16715 non-null float64 JP_sales 16715 non-null float64 Other_sales 16715 non-null float64 Critic_Score 8137 non-null float64 User_Score 10014 non-null object Rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . None . **************************************************************************************************** Describe: . Year_of_Release NA_sales EU_sales JP_sales Other_sales Critic_Score . count | 16446.000000 | 16715.000000 | 16715.000000 | 16715.000000 | 16715.000000 | 8137.000000 | . mean | 2006.484616 | 0.263377 | 0.145060 | 0.077617 | 0.047342 | 68.967679 | . std | 5.877050 | 0.813604 | 0.503339 | 0.308853 | 0.186731 | 13.938165 | . min | 1980.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | . 25% | 2003.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 60.000000 | . 50% | 2007.000000 | 0.080000 | 0.020000 | 0.000000 | 0.010000 | 71.000000 | . 75% | 2010.000000 | 0.240000 | 0.110000 | 0.040000 | 0.030000 | 79.000000 | . max | 2016.000000 | 41.360000 | 28.960000 | 10.220000 | 10.570000 | 98.000000 | . Name Platform Genre User_Score Rating . count | 16713 | 16715 | 16713 | 10014 | 9949 | . unique | 11559 | 31 | 12 | 96 | 8 | . top | Need for Speed: Most Wanted | PS2 | Action | tbd | E | . freq | 12 | 2161 | 3369 | 2424 | 3990 | . **************************************************************************************************** Columns with nulls: . missing_values percent_of_nulls . Name | 2 | 0.01 | . Platform | 0 | 0.00 | . Year_of_Release | 269 | 1.61 | . Genre | 2 | 0.01 | . NA_sales | 0 | 0.00 | . EU_sales | 0 | 0.00 | . JP_sales | 0 | 0.00 | . Other_sales | 0 | 0.00 | . Critic_Score | 8578 | 51.32 | . User_Score | 6701 | 40.09 | . Rating | 6766 | 40.48 | . **************************************************************************************************** Shape: . (16715, 11) . **************************************************************************************************** Duplicated: Number of duplicated rows: 0 . . Columns Critic_Score, User_Score, and Rating have a lot of null values, Critic_Score with about 51% null values, and User_Score and Rating having about 40% null values. These columns seem to have the most missing values. Year_of_Release have about 269 values missing but it is a small percentage compared the first three mentioned. The dataset has 16715 entries in total. . Pre-process Data . Replace the column names (make them lowercase) . df_games.columns . Index([&#39;Name&#39;, &#39;Platform&#39;, &#39;Year_of_Release&#39;, &#39;Genre&#39;, &#39;NA_sales&#39;, &#39;EU_sales&#39;, &#39;JP_sales&#39;, &#39;Other_sales&#39;, &#39;Critic_Score&#39;, &#39;User_Score&#39;, &#39;Rating&#39;], dtype=&#39;object&#39;) . df_games.columns = [columns.lower().replace(&#39; &#39;, &#39;_&#39;) for columns in df_games.columns] df_games.columns . Index([&#39;name&#39;, &#39;platform&#39;, &#39;year_of_release&#39;, &#39;genre&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;, &#39;critic_score&#39;, &#39;user_score&#39;, &#39;rating&#39;], dtype=&#39;object&#39;) . Convert data types and deal with missing values . df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . We have to keep in mind about specifying explicit values, as this may cause errors in real business tasks. Depending on the data and how many values are missing we might have to just drop the missing values or fill then in with our best judgement. . name and genre are missing the 2 values and I believe we can leave these as N. . | year_of_release should be an int instead of a float and we need to replace some of the missing values in that column. . | critic_score has a lot of missing values. . | user_score should be a float or int but again we have to deal with the missing values in this column or figure out how to deal with the TBD values. . | rating just like critic_score has a ton of missing values. . | . def convert_to_type(df, column, type_value): &quot;&quot;&quot; Convert to columns to certain type values&quot;&quot;&quot; for col in column: df[col] = df[col].astype(type_value) . df_games[&#39;name&#39;].fillna(np.nan, inplace=True) df_games[&#39;genre&#39;].fillna(np.nan, inplace=True) convert_to_type(df_games, [&#39;name&#39;], str) convert_to_type(df_games, [&#39;genre&#39;], str) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . name and genre missing values have been filled in with NAN (only 2 values missing). . # Grouping the data by platform and seeing the mean and median for the year_of_release df_platform = df_games.groupby([&#39;platform&#39;]).agg({&#39;year_of_release&#39;:[&#39;mean&#39;, &#39;median&#39;]}) display(df_platform) . year_of_release . mean median . platform . 2600 | 1982.137931 | 1982.0 | . 3DO | 1994.666667 | 1995.0 | . 3DS | 2013.126953 | 2013.0 | . DC | 1999.942308 | 2000.0 | . DS | 2008.185290 | 2008.0 | . GB | 1995.958763 | 1997.0 | . GBA | 2003.210851 | 2003.0 | . GC | 2003.400369 | 2003.0 | . GEN | 1993.034483 | 1993.0 | . GG | 1992.000000 | 1992.0 | . N64 | 1998.531646 | 1999.0 | . NES | 1987.153061 | 1986.5 | . NG | 1994.500000 | 1994.5 | . PC | 2008.914316 | 2010.0 | . PCFX | 1996.000000 | 1996.0 | . PS | 1998.005882 | 1998.0 | . PS2 | 2004.583921 | 2005.0 | . PS3 | 2010.840735 | 2011.0 | . PS4 | 2015.145408 | 2015.0 | . PSP | 2008.731769 | 2009.0 | . PSV | 2014.132867 | 2014.0 | . SAT | 1996.028902 | 1996.0 | . SCD | 1993.833333 | 1994.0 | . SNES | 1993.845188 | 1994.0 | . TG16 | 1995.000000 | 1995.0 | . WS | 2000.000000 | 2000.0 | . Wii | 2008.966563 | 2009.0 | . WiiU | 2013.659864 | 2013.0 | . X360 | 2009.880682 | 2010.0 | . XB | 2003.636364 | 2004.0 | . XOne | 2014.951417 | 2015.0 | . df_platform_median = df_games.groupby([&#39;platform&#39;]).agg({&#39;year_of_release&#39;:&#39;median&#39;}).reset_index() # Fill in NaN values in year_of_release with median year_of_release per platform df_games.year_of_release = df_games.year_of_release.fillna(df_games.platform.map(df_platform_median.set_index(&#39;platform&#39;).year_of_release)) convert_to_type(df_games, [&#39;year_of_release&#39;], int) df_games[&#39;year_of_release&#39;] . 0 2006 1 1985 2 2008 3 2009 4 1996 ... 16710 2016 16711 2006 16712 2016 16713 2003 16714 2016 Name: year_of_release, Length: 16715, dtype: int64 . year_of_release missing values was filled in with median year_of_release per platform. This way at least the filled in missing values are a bit more tailored towards the game in regards to the platform. We were successful in converting the data type from float to int as well. . df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(5), int64(1), object(5) memory usage: 1.4+ MB . df_critic_score = df_games.groupby([&#39;genre&#39;]).agg({&#39;critic_score&#39;:[&#39;mean&#39;, &#39;median&#39;, pd.Series.mode]}) display(df_critic_score) print(&quot;Mean critic_score: &quot;, df_games[&#39;critic_score&#39;].mean()) print(&quot;Median critic_score&quot;, df_games[&#39;critic_score&#39;].median()) . critic_score . mean median mode . genre . Action | 66.629101 | 68.0 | 71 | . Adventure | 65.331269 | 66.0 | 66 | . Fighting | 69.217604 | 72.0 | 74 | . Misc | 66.619503 | 69.0 | 73 | . Platform | 68.058350 | 69.0 | 71 | . Puzzle | 67.424107 | 70.0 | 75 | . Racing | 67.963612 | 69.0 | 82 | . Role-Playing | 72.652646 | 74.0 | 77 | . Shooter | 70.181144 | 73.0 | [72.0, 74.0, 78.0, 81.0] | . Simulation | 68.619318 | 70.0 | 65 | . Sports | 71.968174 | 75.0 | 80 | . Strategy | 72.086093 | 73.0 | [69.0, 75.0] | . nan | NaN | NaN | [] | . Mean critic_score: 68.96767850559173 Median critic_score 71.0 . I feel like it&#39;s best to fill in with median critic_score per genre and for the NaN we can use the median overall critic_score. I am not sure if it makes much of a difference here if we used mean or median but looking at the mode for critic_score it seems like it would be best to use median as the mean seems a bit lower than expected comparing to the mode (which shows what is the most reoccuring critic_score in that genre). . df_critic_score_median = df_games.groupby([&#39;genre&#39;]).agg({&#39;critic_score&#39;:&#39;median&#39;}).reset_index() df_games.critic_score = df_games.critic_score.fillna(df_games.genre.map(df_critic_score_median.set_index(&#39;genre&#39;).critic_score)) df_games[&#39;critic_score&#39;].fillna(np.nan, inplace=True) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(5), int64(1), object(5) memory usage: 1.4+ MB . # Replacing TBD with NaN and converting to float data type df_games[&#39;user_score&#39;].replace([&#39;tbd&#39;], np.nan, inplace=True) df_games[&#39;user_score&#39;].fillna(np.nan, inplace=True) convert_to_type(df_games, [&#39;user_score&#39;], float) df_user_score = df_games.groupby([&#39;genre&#39;]).agg({&#39;user_score&#39;:[&#39;mean&#39;, &#39;median&#39;, pd.Series.mode]}) display(df_user_score) print(&quot;Mean user_score: &quot;, df_games[&#39;user_score&#39;].mean()) print(&quot;Median user_score&quot;, df_games[&#39;user_score&#39;].median()) . user_score . mean median mode . genre . Action | 7.054044 | 7.4 | 8 | . Adventure | 7.133000 | 7.6 | 8.2 | . Fighting | 7.302506 | 7.6 | [7.9, 8.5] | . Misc | 6.819362 | 7.1 | 7.8 | . Platform | 7.301402 | 7.7 | 8.6 | . Puzzle | 7.175000 | 7.5 | 7.5 | . Racing | 7.036193 | 7.4 | 8.2 | . Role-Playing | 7.619515 | 7.8 | 8.2 | . Shooter | 7.041883 | 7.4 | [7.8, 8.2] | . Simulation | 7.134593 | 7.5 | [7.3, 8.0, 8.2, 8.8] | . Sports | 6.961197 | 7.4 | 7.8 | . Strategy | 7.295177 | 7.8 | 8.3 | . nan | NaN | NaN | [] | . Mean user_score: 7.125046113306982 Median user_score 7.5 . The data for user_score has a similar pattern with mean, median, and mode as the critic_score. With the same logic, I think it&#39;s best to replace the missing values for user_score with median user_score per genre. . df_user_score_median = df_games.groupby([&#39;genre&#39;]).agg({&#39;user_score&#39;:&#39;median&#39;}).reset_index() df_games.user_score = df_games.user_score.fillna(df_games.genre.map(df_user_score_median.set_index(&#39;genre&#39;).user_score)) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 16713 non-null float64 rating 9949 non-null object dtypes: float64(6), int64(1), object(4) memory usage: 1.4+ MB . df_games[&#39;rating&#39;].fillna(np.nan, inplace=True) df_rating = df_games.groupby([&#39;genre&#39;]).agg({&#39;rating&#39;: pd.Series.mode}).reset_index() df_rating . genre rating . 0 | Action | T | . 1 | Adventure | E | . 2 | Fighting | T | . 3 | Misc | E | . 4 | Platform | E | . 5 | Puzzle | E | . 6 | Racing | E | . 7 | Role-Playing | T | . 8 | Shooter | M | . 9 | Simulation | E | . 10 | Sports | E | . 11 | Strategy | T | . 12 | nan | [] | . Since rating is categorical we can&#39;t just use mean and median as easily. Mode might be a good filler here. We can used the mode for rating per genre to fill in the missing values. . df_games.rating = df_games.rating.fillna(df_games.genre.map(df_rating.set_index(&#39;genre&#39;).rating)) convert_to_type(df_games, [&#39;rating&#39;], str) df_games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null int64 genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16713 non-null float64 user_score 16713 non-null float64 rating 16715 non-null object dtypes: float64(6), int64(1), object(4) memory usage: 1.4+ MB . Calculate the total sales (the sum of sales in all regions) for each game and put these values in a separate column . df_games[&#39;total_sales&#39;] = df_games[[&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]].sum(axis=1) df_games . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | 69.0 | 7.7 | E | 40.24 | . 2 | Mario Kart Wii | Wii | 2008 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | 74.0 | 7.8 | T | 31.38 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 16710 | Samurai Warriors: Sanada Maru | PS3 | 2016 | Action | 0.00 | 0.00 | 0.01 | 0.00 | 68.0 | 7.4 | T | 0.01 | . 16711 | LMA Manager 2007 | X360 | 2006 | Sports | 0.00 | 0.01 | 0.00 | 0.00 | 75.0 | 7.4 | E | 0.01 | . 16712 | Haitaka no Psychedelica | PSV | 2016 | Adventure | 0.00 | 0.00 | 0.01 | 0.00 | 66.0 | 7.6 | E | 0.01 | . 16713 | Spirits &amp; Spells | GBA | 2003 | Platform | 0.01 | 0.00 | 0.00 | 0.00 | 69.0 | 7.7 | E | 0.01 | . 16714 | Winning Post 8 2016 | PSV | 2016 | Simulation | 0.00 | 0.00 | 0.01 | 0.00 | 70.0 | 7.5 | E | 0.01 | . 16715 rows × 12 columns . Name of columns were converted to lowercase. | name and genre missing values were placed by NaN (only 2 missing values) | year_of_release missing values were filled with median year_of_release per platform and then converted data type to int. | critic_score and user_score were filled with median values per genre. user_score&#39;s TBD had to be changed to np.nan value and then filled. user_score also underwent a data type change to float. There are 2 missing values left as NaN and these seem to connect to the 2 missing name and genre. I believe having these values remain NaN is okay for later analysis. | rating missing values were filled with the mode value per genre. | total_sales is the sum of sales in all the regions of that game. | . Exploratory Data Analysis . Number of games released in different years . def get_annotation(ax): &quot;&quot;&quot;Get annotation for graphs on top of bar graphs&quot;&quot;&quot; for p in ax.patches: ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.05)) . num_of_games_per_year = df_games.groupby([&#39;year_of_release&#39;])[&#39;name&#39;].agg(&#39;count&#39;) num_of_games_per_year . year_of_release 1980 9 1981 46 1982 53 1983 17 1984 14 1985 14 1986 21 1987 16 1988 15 1989 17 1990 16 1991 41 1992 43 1993 62 1994 121 1995 219 1996 263 1997 290 1998 386 1999 341 2000 350 2001 482 2002 829 2003 800 2004 783 2005 973 2006 1006 2007 1197 2008 1457 2009 1476 2010 1302 2011 1161 2012 653 2013 552 2014 582 2015 606 2016 502 Name: name, dtype: int64 . get_annotation(num_of_games_per_year.plot(kind=&#39;bar&#39;, figsize=(14,6))) plt.ylabel(&#39;number_of_games&#39;) plt.title(&quot;Number of games released per year&quot;) plt.grid(True) . In the 1980s to 1990s, there is a small amount of games released. Then there is a surge of games released passed 1994 where it seems to have an exponential growth of titles realeased. I have a feeling that this is due to release of home game consoles/platforms making more games more readily available. We can take a look at the consoles in each year to see if this is true. Though it&#39;s interesting as there is a number of games tapers off in 2011 and remains steady around 500-600 range of titles. . df_games[&#39;platform&#39;].unique() . array([&#39;Wii&#39;, &#39;NES&#39;, &#39;GB&#39;, &#39;DS&#39;, &#39;X360&#39;, &#39;PS3&#39;, &#39;PS2&#39;, &#39;SNES&#39;, &#39;GBA&#39;, &#39;PS4&#39;, &#39;3DS&#39;, &#39;N64&#39;, &#39;PS&#39;, &#39;XB&#39;, &#39;PC&#39;, &#39;2600&#39;, &#39;PSP&#39;, &#39;XOne&#39;, &#39;WiiU&#39;, &#39;GC&#39;, &#39;GEN&#39;, &#39;DC&#39;, &#39;PSV&#39;, &#39;SAT&#39;, &#39;SCD&#39;, &#39;WS&#39;, &#39;NG&#39;, &#39;TG16&#39;, &#39;3DO&#39;, &#39;GG&#39;, &#39;PCFX&#39;], dtype=object) . platform_per_year = df_games.groupby([&#39;year_of_release&#39;])[&#39;platform&#39;].nunique() platform_per_year . year_of_release 1980 1 1981 1 1982 1 1983 2 1984 2 1985 4 1986 2 1987 2 1988 4 1989 3 1990 4 1991 4 1992 6 1993 5 1994 10 1995 8 1996 8 1997 6 1998 7 1999 8 2000 9 2001 10 2002 8 2003 6 2004 7 2005 8 2006 10 2007 11 2008 9 2009 7 2010 7 2011 9 2012 9 2013 11 2014 10 2015 10 2016 9 Name: platform, dtype: int64 . get_annotation(platform_per_year.plot(kind=&#39;bar&#39;, figsize=(14,6))) plt.ylabel(&#39;number_of_platforms&#39;) plt.title(&quot;Number of platforms per year with game releases&quot;) plt.grid(True) . As we saw from above, there is a height in number of platforms in 1994. It&#39;s interesting to note that perhaps platforms past this point have end-of-life around 5-6 years as there seems to be a peak for number of plaforms with games released around that time frame. Though, interesting like we saw previous with the number of games released at around 2011-2012, the numbers past this point seem to stablize compared to previous pattern observed. . Change in sales per platform . df_games.groupby(&#39;platform&#39;)[&#39;total_sales&#39;].agg(sum).sort_values(ascending=False) . platform PS2 1255.77 X360 971.42 PS3 939.65 Wii 907.51 DS 806.12 PS 730.86 GBA 317.85 PS4 314.14 PSP 294.05 PC 259.52 3DS 259.00 XB 257.74 GB 255.46 NES 251.05 N64 218.68 SNES 200.04 GC 198.93 XOne 159.32 2600 96.98 WiiU 82.19 PSV 54.07 SAT 33.59 GEN 30.77 DC 15.95 SCD 1.86 NG 1.44 WS 1.42 TG16 0.16 3DO 0.10 GG 0.04 PCFX 0.03 Name: total_sales, dtype: float64 . Top selling platforms include: PS2, X360, PS3, Wii, DS, and PS. We can do a quick graph comparison of all platforms game sales over time just to have a quick overview and mental note of the comparison, but it would be wise just to focus on analysis of these top selling platforms. . def plot_bar(df, x, y, column=&#39;&#39;, value=&#39;&#39;, func=np.sum): if column != &#39;&#39; and value !=&#39;&#39;: filter_df = df[df[column] == value] plot_df = filter_df.pivot_table(index= x, values= y, aggfunc= func) values_to_plot = plot_df[y].values else: plot_df = df.pivot_table(index= x, values= y, aggfunc= func) values_to_plot = plot_df[y].values title = str(value) + &#39; - &#39; + str(y) + &#39; vs. &#39; + x ax = plot_df.plot(kind= &#39;bar&#39;, figsize=(14,6), rot=45, title=title, legend=False, grid=True) for p in ax.patches: ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.05)) . # Bar graph for all platforms versus the total sales of games per year for platform in df_games[&#39;platform&#39;].unique(): plot_bar(df_games, &#39;year_of_release&#39;, &#39;total_sales&#39;, &#39;platform&#39;, platform) plt.ylabel(&#39;total_sales&#39;) . /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). fig = self.plt.figure(figsize=self.figsize) . . Wii sales title released started in 2006 but then declined around 2011 and really died down passed that point. . | NES title sales started around 1983 though the sales took off in 1984 spiked in 1985 and then again died down and spiked again in 1988 but died down after that. . | GB title sales started in 1988 but really took off the next year in 1989 but sales seemed to declined after this point (might be due to no new game releases). Sales start increase in 1992 but again there is a decline until 1996 where this is another surge. However, it declines again and then increases for the interval from 1998 to 1999 and declines and stops at 2001. It should be noted that there is no distinction on our data for GB versus GB Color (since both are the same plaform but one was a later release with color resolution). This might explain the long life of sales for this platform. . | DS game sales starts to take off in 2004 and peaks in 2005 staying fairly constant until 2010 when sales start to decline and it really dies off in 2013. . | X360 game sales start in 2005 but really start climbing in 2006 and peaks around 2010 and games sales after this have gradual decline after this point. . | PS3 game sales start at 2006 and climb and peak at 2011 and slowly declines with sales really dropping after 2014. . | PS2 game sales start in 2000 then stay pretty high in the period between 2001 to 2005. After this point, game sales for the PS2 start really declining in 2006 and stop after 2011. . | SNES game sales start in 1990 and staky fairly constant and peak in 1993. Game sales for SNES start to decline in 1996 and really drop off after this and ends in 1999. . | GBA game sales start in 2000 but really take off in 2001. It stays pretty constant until 2005 where it really starts to decline and sales stop at 2007. . | PS4 game sales start in 2013 but takes off in 2014 and is still pretty constant with our data, however, we can&#39;t see past 2016 to really see when sales will start to decline for this the PS4. . | 3DS game sales start off strong in 2011 and there is a slight decline after 2015. . | N64 game sales start in 1996 (which make sense with the decline of SNES in 1996) and take off peaking at 1999. There is decline after 2000 and sales stop after 2002. . | PS game sales start in 1994 and start a gradual climb and peak in 1998 and slowly declines after this point with a sales really stopping in 2003. . | . . Quick analysis from the graphs consoles seem to have peak and decline around 4-5 years. General exceptions for these are hand-held consoles (GB, GBA, DS, PSV) and PC and earlier consoles. Platforms/consoles have sales past 4-5 year point might have game titles released on both newer and older platforms. Though from the data, the top 4 selling games releases throughout the years were on the following platforms: PS2, X360, PS3, Wii, DS, and PS . Determining the actual period with leading platforms . Though we know the platforms with the leading game sales of all time from our data consists of: PS2, X360, PS3, Wii, DS, and PS, these platforms wouldn&#39;t really show up for our prognosis for 2017 since these are considered end-of-life already. Though looking at their patterns we can try to forcast the sales for the currently/lastest leading platforms. . Knowing that the lastest platforms seemed to be released around 2013, we should take a look around this period and use the data from the former leading platforms to make a prognosis for 2017. . df_period = df_games[(df_games[&#39;year_of_release&#39;] &gt;= 2013) &amp; (df_games[&#39;year_of_release&#39;].notnull())] df_period.pivot_table(index=&#39;platform&#39;, values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;).sort_values(by=&#39;total_sales&#39;, ascending=False) . total_sales . platform . PS4 | 314.14 | . PS3 | 181.43 | . XOne | 159.32 | . 3DS | 144.44 | . X360 | 136.80 | . WiiU | 64.63 | . PC | 39.43 | . PSV | 33.25 | . Wii | 13.66 | . PSP | 3.50 | . DS | 1.54 | . Starting in from 2013 until 2016, it seems like the platform with leading in game sales is PS4. PS3, though coming in second on this list, we need to keep in mind that the sales for this should be in decline do the newer PS4 release. XOne should follow the same pattern with X360 falling behind in sales with the release of the new platform. Same with 3DS and the DS, WiiU and Wii, PSV and PSP. We should see a decline in sales for the older platforms in 2017. PC is the only platform that would not have newer releases for the platform, though the sales for PC do vary, this might be because of number of title releases for the PC or so other underlying cause. . # Newer platforms df_ps4 = df_games.query(&#39;platform == &quot;PS4&quot; &amp; year_of_release &gt;= 2013&#39;) df_xone = df_games.query(&#39;platform == &quot;XOne&quot; &amp; year_of_release &gt;= 2013&#39;) df_3ds = df_games.query(&#39;platform == &quot;3DS&quot; &amp; year_of_release &gt;= 2013&#39;) df_wiiu = df_games.query(&#39;platform == &quot;WiiU&quot; &amp; year_of_release &gt;= 2013&#39;) df_psv = df_games.query(&#39;platform == &quot;PSV&quot; &amp; year_of_release &gt;= 2013&#39;) # Older platforms df_ps3 = df_games.query(&#39;platform == &quot;PS3&quot; &amp; year_of_release &gt;= 2013&#39;) df_x360 = df_games.query(&#39;platform == &quot;X360&quot; &amp; year_of_release &gt;= 2013&#39;) df_ds = df_games.query(&#39;platform == &quot;DS&quot; &amp; year_of_release &gt;= 2013&#39;) df_wii = df_games.query(&#39;platform == &quot;Wii&quot; &amp; year_of_release &gt;= 2013&#39;) df_psp = df_games.query(&#39;platform == &quot;PSP&quot; &amp; year_of_release &gt;= 2013&#39;) . plot_bar(df_ps4,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PS4 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_ps3,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PS3 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . PS4 seemed to have been introduced in 2013 where PS3 seems to be in a height of sales but as expected the sales for PS3 decrease over time and the sames should decrease even more or be close to non-existent in 2017. PS4 sale should be on the raise but since it seems like 2016 data is not complete it&#39;s hard to determine if PS4 sales will spike in 2017. . plot_bar(df_xone,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;XOne - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_x360,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;X360 - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what we see with PS4 vs PS3, XOne and X360 follow the same pattern. With sales for X360 high in 2013 but with the introduction of XOne in 2013 there is a raise in sales for XOne and decline for X360. By 2017, X360 should be similar to PS3 sales. Sales for XOne in 2017 should increase too for 2017 or have as similar raise to that of PS4. . plot_bar(df_3ds,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;3DS - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_ds,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;DS - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . It seems as though the hand-held platforms here have a different pattern. The 3DS seems to have peaked sales in 2013 but is on the decline versus the DS which seems like was on it&#39;s decline and stopped in 2013 with no data after. for 3DS sales in 2017, it seems like it might decrease a bit more. . plot_bar(df_wiiu,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;WiiU - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_wii,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;Wii - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what was seen with the PS4 vs PS3 and XOne vs X360, WiiU and Wii are following the same pattern with the decline of Wii starting in 2013 and the raise of WiiU. Though it seems like WiiU is declining and this might be do to lack of new title releases for the platform. Though for Wii in 2017, it seems like the platform will cease sales. . plot_bar(df_psv,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PSV - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) plot_bar(df_psp,&#39;year_of_release&#39;, &#39;total_sales&#39;) plt.title(&#39;PSV - total_sales vs year_of_release&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . Similar to what we have seen with newer platforms being introduced, the PSP sales decreased after 2013 and ceased in after 2015. PSV has a slight decline as well but this might be do to lack of new title realeses for the platform. Interesting enough, this is also a hand-held platform and from what was seen above with 3DS vs DS it seems like the older platform sales for these really do come to a quick stop in sales after introduction of a new platform versus the other platforms. . . The leading platforms that we would see sales in 2017 are: PS4, XOne, 3DS, WiiU, PSV, and PC. These platforms are the lastest and most current platforms to date. So long as new game titles are released we should see the sales stay stable or grow. With anticipation of new platforms, ever 5-6 years or so we need to keep in mind that these platforms were newly introduced around 2013 so close to 2017 we might see a decrease in sales as well as we are past the point of peaking sales. Thought the hand-held consoles (PSV and 3DS) it&#39;s hard to determine their end-of-life as they vary slightly compared to the other platforms. PC sales will stay mostly consistent as there is no threat of new platforms. . The platforms that have reached their end-of-life (shrinking in sales) are: PS3, X360, and Wii. As we saw from above, the introduction of new platforms brought the decline to these older platforms. For the hand-held platforms (PSP and DS) sales declined and came to a halt much more sudden than the other platforms. Since these older platforms still have little sales, we might see a halt of sales in 2017. . Distribution of global sales by platform . plt.figure(figsize=(30,10)) ax = sns.boxplot(x=df_games[&#39;total_sales&#39;], y=df_games[&#39;platform&#39;]) ax.yaxis.grid(True) # Show the horizontal gridlines ax.xaxis.grid(True) # Show the vertical gridlines ax.xaxis.set_major_locator(ticker.MultipleLocator(.1)) ax.xaxis.set_major_formatter(ticker.ScalarFormatter()) ax.set(xlim=(0, 4)) . [(0, 4)] . There is a difference in range for all the game sales per platform across the globe.It seems like the median for NES game sales is the highest at about 1.38 million USD compared to the other platforms. GB game median game sales comes second next to the NES at about 1.16 million USD. There are some game sales that are significantly outside quartile on the right. These games must be best sellers to have a big right skew outside the quartile. . display(df_games.sort_values(by=&#39;total_sales&#39;, ascending=False).head(10)) . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | 69.0 | 7.7 | E | 40.24 | . 2 | Mario Kart Wii | Wii | 2008 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | 74.0 | 7.8 | T | 31.38 | . 5 | Tetris | GB | 1989 | Puzzle | 23.20 | 2.26 | 4.22 | 0.58 | 70.0 | 7.5 | E | 30.26 | . 6 | New Super Mario Bros. | DS | 2006 | Platform | 11.28 | 9.14 | 6.50 | 2.88 | 89.0 | 8.5 | E | 29.80 | . 7 | Wii Play | Wii | 2006 | Misc | 13.96 | 9.18 | 2.93 | 2.84 | 58.0 | 6.6 | E | 28.91 | . 8 | New Super Mario Bros. Wii | Wii | 2009 | Platform | 14.44 | 6.94 | 4.70 | 2.24 | 87.0 | 8.4 | E | 28.32 | . 9 | Duck Hunt | NES | 1984 | Shooter | 26.93 | 0.63 | 0.28 | 0.47 | 73.0 | 7.4 | M | 28.31 | . Wii Sports seems to be the top selling game overall from our data at 82.54 million USD on the Wii platform. Super Mario Bros comes in second at 40.24 million USD on the NES. . pd.pivot_table(df_games, values=&#39;total_sales&#39;, index=&#39;platform&#39;, aggfunc={&#39;sum&#39; , &#39;median&#39;, &#39;mean&#39;}).sort_values(by=&#39;mean&#39;, ascending=False) . mean median sum . platform . GB | 2.606735 | 1.165 | 255.46 | . NES | 2.561735 | 1.375 | 251.05 | . GEN | 1.061034 | 0.150 | 30.77 | . SNES | 0.836987 | 0.320 | 200.04 | . PS4 | 0.801378 | 0.200 | 314.14 | . X360 | 0.769746 | 0.280 | 971.42 | . 2600 | 0.729173 | 0.460 | 96.98 | . PS3 | 0.705973 | 0.270 | 939.65 | . Wii | 0.687508 | 0.190 | 907.51 | . N64 | 0.685517 | 0.270 | 218.68 | . XOne | 0.645020 | 0.220 | 159.32 | . PS | 0.610576 | 0.260 | 730.86 | . PS2 | 0.581106 | 0.230 | 1255.77 | . WiiU | 0.559116 | 0.220 | 82.19 | . 3DS | 0.498077 | 0.120 | 259.00 | . GBA | 0.386679 | 0.160 | 317.85 | . DS | 0.374765 | 0.110 | 806.12 | . GC | 0.357788 | 0.150 | 198.93 | . XB | 0.312791 | 0.150 | 257.74 | . SCD | 0.310000 | 0.065 | 1.86 | . DC | 0.306731 | 0.135 | 15.95 | . PC | 0.266448 | 0.050 | 259.52 | . PSP | 0.243218 | 0.090 | 294.05 | . WS | 0.236667 | 0.215 | 1.42 | . SAT | 0.194162 | 0.120 | 33.59 | . PSV | 0.125744 | 0.055 | 54.07 | . NG | 0.120000 | 0.100 | 1.44 | . TG16 | 0.080000 | 0.080 | 0.16 | . GG | 0.040000 | 0.040 | 0.04 | . 3DO | 0.033333 | 0.020 | 0.10 | . PCFX | 0.030000 | 0.030 | 0.03 | . GB platform has the highest mean value at 2.6 million USD and like we saw above with the boxplot the second highest median value at 1.65 million USD. NES comes in second with the highest mean value of 2.56 million USD and the highest overall median at 1.37 million USD. Though, the highest selling game sales is found in PS2 platform at 1255.77 million USD but the mean value for this is 0.58 million USD and median is 0.23 million USD. The distribution for these values are pretty interesting in terms of sum of top selling versus the mean and median values. . . Overall, the median values are lower compared to the mean values for game sales per platform. . impact of user and critic feedback on sales . wii = df_games.query(&#39;platform == &quot;Wii&quot;&#39;) print(&#39;***critic_score median:&#39;, wii[&#39;critic_score&#39;].median()) display(wii[&#39;critic_score&#39;].describe()) print(&#39; n***user_score median:&#39;, wii[&#39;user_score&#39;].median()) display(wii[&#39;user_score&#39;].describe()) . ***critic_score median: 69.0 . count 1320.000000 mean 67.103030 std 10.823695 min 19.000000 25% 66.000000 50% 69.000000 75% 73.000000 max 97.000000 Name: critic_score, dtype: float64 . ***user_score median: 7.4 . count 1320.000000 mean 7.107197 std 1.183492 min 0.200000 25% 7.100000 50% 7.400000 75% 7.600000 max 9.300000 Name: user_score, dtype: float64 . The critic_score and user_score are close in mean and median values if we scale the critic_score to match user_score. Overall, the user_score seems to have a higher scoring compared to the critic_score. . wii[&#39;critic_score_scale&#39;] = wii[&#39;critic_score&#39;] / 10.0 ax = wii.plot.scatter(x=&#39;user_score&#39;, y=&#39;total_sales&#39;, label=&#39;user_score&#39;, figsize=(20,8), alpha=0.4) wii.plot.scatter(x=&#39;critic_score_scale&#39;, y=&#39;total_sales&#39;, color=&#39;DarkGreen&#39;, label=&#39;critic_score&#39;, ax=ax, alpha=0.4) plt.legend() plt.grid(True) plt.xlabel(&#39;scoring&#39;) plt.title(&quot;critic_score and user_score on the Wii per total sales&quot;) . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Text(0.5, 1.0, &#39;critic_score and user_score on the Wii per total sales&#39;) . Looking at the scatter plot it doesn&#39;t seem like there is a linear correlation relationship between scoring and sales. Though, we do see a huge rating around 7-8 scoring range where there is a spike in total_sales but this seems like it&#39;s only for a certain game title as sales don&#39;t remain this high as we move higher on the scoring. From the scatter plot, there is a slight increase in total_sales with increase of rating/scoring with both critic and user, which would make sense as popular game titles I imagine would have higher ratings/scoring from both critics and users. . print(&#39;critic_score correlation calculation:&#39;, &#39;{:.1%}&#39;.format(wii[&#39;critic_score&#39;].corr(wii[&#39;total_sales&#39;]))) print(&#39;user_score correlation calculation:&#39;, &#39;{:.1%}&#39;.format(wii[&#39;user_score&#39;].corr(wii[&#39;total_sales&#39;]))) . critic_score correlation calculation: 11.1% user_score correlation calculation: 6.2% . There doesn&#39;t seem to be a significant correlation between scoring/rating and sales from the linear correlation calculation. Though critic_score seems to have a higher influence than user_score. . . Overall, critic_score and user_score doesn&#39;t seem to have a linear correlation with total_sales. Though from our scatter plot, it does seem like there is a relationship with scoring and total sales as it seeems like there is a higher density of total sales with higher scoring/rating. critic_score seems to have a higher influence in total_sales compared to user_score. . Compare the sales of the same games on other platforms . cross_platform = set(df_period.groupby([&#39;name&#39;]).filter(lambda x: x.shape[0] &gt; 1)[&#39;name&#39;]) #List of unique games that are cross-platform df_popular_games = df_period.query(&#39;name in @cross_platform&#39;).groupby([&#39;name&#39;, &#39;platform&#39;]).agg({&#39;total_sales&#39;: &#39;sum&#39;}) df_popular_games.sort_values(by=&#39;total_sales&#39;, ascending=False, inplace=True) df_popular_games.query(&#39;total_sales &gt; 6&#39;).head(10) . total_sales . name platform . Grand Theft Auto V | PS3 | 21.05 | . X360 | 16.27 | . Call of Duty: Black Ops 3 | PS4 | 14.63 | . Grand Theft Auto V | PS4 | 12.62 | . Call of Duty: Ghosts | X360 | 10.24 | . PS3 | 9.36 | . Minecraft | X360 | 9.18 | . FIFA 16 | PS4 | 8.58 | . Star Wars Battlefront (2015) | PS4 | 7.98 | . Call of Duty: Advanced Warfare | PS4 | 7.66 | . Across platforms starting from 2013, PS4 seems to be the most popular platfrom and second to that is X360. Most popular being Grand Theft Auto V on PS3, X360, and PS4. All other games seem to fall for PS4 for popularity. . Distribution of games by genre . genres = df_period.groupby(&#39;genre&#39;).agg({&#39;total_sales&#39;: &#39;sum&#39;}).sort_values(by=&#39;total_sales&#39;, ascending=False) plot_bar(genres,&#39;genre&#39;, &#39;total_sales&#39;) plt.title(&#39;total_sales vs genre&#39;) plt.ylabel(&#39;total_sales&#39;) . Text(0, 0.5, &#39;total_sales&#39;) . From the start of 2013, the most popular genre of game sold overall is Action. Second would be Shooter and then Sport and Role-Playing come close to thrid and fourth. . for year in [2013 + i for i in range(4)]: plot_bar(df_period, &#39;genre&#39;, &#39;total_sales&#39;, column=&#39;year_of_release&#39;, value=year, func=&#39;count&#39;) plt.ylabel(&#39;total_sales&#39;) . 2013 the most popular genre was Action followed by Role-Playing with Adventure and Shooter close in sales. . | 2014 similar to what was seen in 2013, Action was the top selling genre with Role-Playing falling second and Adventure being in the top 3. Sports came in at fourth place. . | 2015 was by far the top selling genre with count sold being over 200 which is much different from what we saw previously. . | 2016 has a similar pattern to 2013 and 2014 with Action being top selling. . | . . Action is the top selling genre overall. All other genre seem to stay moderately the same with sales though Shooter seems to fluctuate along with Adventure throughout the changing years. This might be due to number of releases per year per genre. Puzzle remains the lowest selling genre overall. . Statistical Data Analysis . Create a user profile for each region . regions = [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;] for region in regions: print(region.title()) display(df_period.pivot_table(index=&#39;platform&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) display(df_period.pivot_table(index=&#39;genre&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) display(df_period.pivot_table(index=&#39;rating&#39;, values=region, aggfunc=&#39;sum&#39;).sort_values(by=region, ascending=False).head()) print(&#39;******&#39;*10, &#39; n&#39;) . Na_Sales . na_sales . platform . PS4 | 108.74 | . XOne | 93.12 | . X360 | 81.66 | . PS3 | 63.50 | . 3DS | 39.04 | . na_sales . genre . Action | 126.23 | . Shooter | 109.78 | . Sports | 65.27 | . Role-Playing | 46.51 | . Misc | 27.49 | . na_sales . rating . M | 184.77 | . E | 102.49 | . T | 96.90 | . E10+ | 54.50 | . ************************************************************ Eu_Sales . eu_sales . platform . PS4 | 141.09 | . PS3 | 67.81 | . XOne | 51.59 | . X360 | 42.52 | . 3DS | 31.17 | . eu_sales . genre . Action | 118.32 | . Shooter | 87.88 | . Sports | 60.52 | . Role-Playing | 37.02 | . Racing | 20.19 | . eu_sales . rating . M | 162.21 | . E | 108.39 | . T | 79.01 | . E10+ | 42.88 | . ************************************************************ Jp_Sales . jp_sales . platform . 3DS | 67.86 | . PS3 | 23.35 | . PSV | 18.66 | . PS4 | 15.96 | . WiiU | 10.88 | . jp_sales . genre . Role-Playing | 51.14 | . Action | 40.49 | . Misc | 9.20 | . Fighting | 7.65 | . Shooter | 6.61 | . jp_sales . rating . T | 86.72 | . E | 33.37 | . M | 14.92 | . E10+ | 5.89 | . ************************************************************ Other_Sales . other_sales . platform . PS4 | 48.35 | . PS3 | 26.77 | . XOne | 14.27 | . X360 | 12.11 | . 3DS | 6.37 | . other_sales . genre . Action | 37.23 | . Shooter | 28.78 | . Sports | 19.45 | . Role-Playing | 11.51 | . Misc | 6.09 | . other_sales . rating . M | 52.82 | . E | 28.97 | . T | 25.69 | . E10+ | 12.61 | . ************************************************************ . NA region: . For the period starting from 2013, PS4 is the top selling platform with XOne, X360 and PS3 trailing behind and 3DS being at the bottom. | Action being to top genre, and Shooter, Sports, and Role-Playing falling behind. | M rating is top rating with E and T behind that. | . EU region: . For the period starting from 2013, PS4 is the top selling platform with PS3, XOne, X360 and 3DS falling behind in that order. | Top selling genres are similar order to NA with Action being the best genre, Shooter, Sports and Role-Playing falling behind. Different from NA though is Racing genre rather than Misc genre compared to NA. | Same pattern as we saw with NA the rating comes with M being top selling and E and T falling behind that. | . JP region: . Different from what we see with NA and EU the top selling platform in JP is 3DS. PS3, PSV, PS4, and WiiU falling behind the 3DS which is much different from NA and EU region. The hand-held platforms are much more dominate in JP and no XOne or X360 found in sales. | Another difference we see here compared to NA and EU region with genre, Role-Playing is the top selling genre with Action, Misc, Fighting, and Shooter on the bottom. | Another difference from NA and EU region is with the rating in JP, T is the top selling genre and E and M being behind that. | . Other region: . Similar to what NA and EU region, PS4 is the top selling platform. PS3, XOne, X360, and 3DS trailing behind. | Again similar to NA and EU region, Action is the top selling genre. Following behind is Shooter, Sports, Role-Playing, and Misc. | Same pattern we see with NA rating, M is the top selling rating of games and E and T follow behind M. | . . Interesting that most of the regions have similar patterns for top selling platforms, genre, and rating except for JP. JP doesn&#39;t even have the XBox series platform in the top selling. Also, having 3DS and PSV as top selling, it seems like hand-held platforms are more popular in JP than other regions. Genre is different in JP as well compared to other regions and again this might be do to 3DS being the top selling. . Hypotheses testing . Null hypothesis vs Alternative hypothesis and Significance level . Null hypothesis is believed to a be a true statement. The alternative hypothesis is formulated to disprove the null hypothesis with at least one statisical representation example that makes the null hypothesis false. Since the null hypothesis would be false it has to be rejected. . Alternative hypothesis is usually the mathematical opposite of the null hypothesis. . We choose a significance level of 0.05 which indicates a 5% risk of concluding that a difference exists when there is no actual difference. Anything higher seems like it would be risky. . Testing if average user ratings of the Xbox One and PC platforms are the same. . Null hypothesis: . The average user rating of Xbox One and PC platforms do not differ. | . Alternative hypothesis: . * The average user rating of Xbox One and PC platforms do&#160;differ. . The alternative hypothesis will be tested to disprove the null hypothesis. . What to test: . Compare the averages/means user rating from both XOne and PC, if they are off by an alpha (that we specify) this means we could reject the null hypothesis. | . xone_user_rating = df_games.query(&#39;platform == &quot;XOne&quot;&#39;)[&#39;user_score&#39;] pc_user_rating = df_games.query(&#39;platform == &quot;PC&quot;&#39;)[&#39;user_score&#39;] print(&quot;Average user rating for XOne: &quot;, xone_user_rating.mean(), &quot; nAverage user rating for PC: &quot;, pc_user_rating.mean()) . Average user rating for XOne: 6.7615384615384615 Average user rating for PC: 7.16170431211499 . alpha = 0.05 #critical statistical significance results = st.ttest_ind( xone_user_rating, pc_user_rating) print(&#39;p-value: &#39;, results.pvalue, &#39; nalpha: &#39;, alpha) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 3.5341366694379986e-05 alpha: 0.05 We reject the null hypothesis . Mathematically, it was proven that the average user rating from XOne and PC do differ. The t-test confirms that it appears that these ratings are differ. . With means of: . Average user rating for XOne: 6.761538461538461 | Average user rating for PC: 7.161704312115005 | . Testing if average user ratings for the Action and Sports genres are different. . Null hypothesis: . The average user ratings for the Action and Sports genres do not differ. | . Alternative hypothesis: . * The average user ratings for the Action and Sports genres do&#160;differ. . The alternative hypothesis will be tested to disprove the null hypothesis. . What to test: . Compare the averages/means user rating from both Action and Sports genres, if they are off by an alpha (that we specify) this means we could reject the null hypothesis. | . action_user_rating = df_games.query(&#39;genre == &quot;Action&quot;&#39;)[&#39;user_score&#39;] sports_user_rating = df_games.query(&#39;genre == &quot;Sports&quot;&#39;)[&#39;user_score&#39;] print(&quot;Average user rating for Action genre: &quot;, action_user_rating.mean(), &quot; nAverage user rating for Sports genre: &quot;, sports_user_rating.mean()) . Average user rating for Action genre: 7.21208073612348 Average user rating for Sports genre: 7.193867120954002 . alpha = 0.05 #critical statistical significance results = st.ttest_ind( action_user_rating, sports_user_rating) print(&#39;p-value: &#39;, results.pvalue, &#39; nalpha: &#39;, alpha) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.5342570517131008 alpha: 0.05 We can&#39;t reject the null hypothesis . Mathematically, it was proven that the average user rating from Action genre and Sports do not differ, since we are not able to reject the null hypothesis. The t-test confirms that it appears that these ratings are similar. . With means of: . Average user rating for Action genre: 7.212080736123694 | Average user rating for Sports genre: 7.193867120953876 | . The average/means are pretty similar between Action and Sports genres. . . With the t-test and an alpha of 0.05, we were able to prove that the average user rating from XOne and PC do differ and also with seperate t-test we concluded that the average user rating from Action genre and Sports do not differ. . Conclusion . critic_score and user_score had lots of missing values which may have caused influences in the analysis with how we decided to fill in those values. . | Using similar genre with the game titles, we were able to generate a relevant rating for the game titles that had missing values. . | Overall, PS2 had the highest selling games titles for it&#39;s platform with the data provided. . | The typical lifespan of a platform ranges from 5-6 years, as this seems to be where new platforms are announced and introduced to the market. Then it takes about 2-3 years for the older platform to become irrelevant after the introduction to the newer platform. . | Action is the top selling genre of games overall, though it gets interesting when we take a closer look at the regions where it seems to differ in some regions. . | NA Region: the top selling platform is PS4, top selling genre is Action, and top selling rating of games is M. . | EU Region: the top selling platform is PS4, top selling genre is Action, and top selling rating of games is M. . | JP Region: the top selling platform is 3DS, top selling genre is Role-Playing, and top selling rating of games is T followed behind by E. . | . . It&#39;s interesting overall to see that hand-held consoles are more popular in Japan versus other places in the world and also that the XBox series is not targeted towards those in Japan at all but is everywhere else in the world. Also, with the rating being M being top selling in NA and EU it does seem like shooter games are more popular in these regions vs what is seen in JP. This seems like there is a cultural influence with game sells and their target audience. .",
            "url": "https://cmdang-mochi.github.io/ds-projects/jupyter/exploratory%20analysis/statistical%20analysis/pandas/numpy/mathplotlib/scripy/seaborn/2020/10/23/successful_video_games_sales_trends.html",
            "relUrl": "/jupyter/exploratory%20analysis/statistical%20analysis/pandas/numpy/mathplotlib/scripy/seaborn/2020/10/23/successful_video_games_sales_trends.html",
            "date": " • Oct 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome! . This website displays my project portfolio Connie Dang 1. . check out my Linkedin page. &#8617; . |",
          "url": "https://cmdang-mochi.github.io/ds-projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cmdang-mochi.github.io/ds-projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}